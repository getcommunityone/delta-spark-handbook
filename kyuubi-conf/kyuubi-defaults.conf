# # Spark Configuration
spark.master=spark://localhost:7077
spark.sql.adaptive.enabled=true
spark.sql.catalogImplementation=hive
spark.serializer=org.apache.spark.serializer.KryoSerializer
spark.jars=/opt/spark/jars/kyuubi-spark-sql-engine_2.12-1.10.1.jar,/opt/spark/jars/kyuubi-common_2.12-1.10.1.jar,/opt/spark/jars/delta-spark_2.12-3.3.0.jar,/opt/spark/jars/delta-storage-3.3.0.jar,/opt/spark/jars/kyuubi-server-plugin-1.10.1.jar,/opt/spark/jars/aws-java-sdk-bundle-1.12.782.jar,/opt/spark/jars/hadoop-aws-3.3.4.jar
spark.jars.packages=io.delta:delta-spark_2.12:3.3.0,org.apache.hadoop:hadoop-aws:3.3.4
spark.driver.extraClassPath=/opt/spark/jars/*
spark.executor.extraClassPath=/opt/spark/jars/*
spark.hive.metastore.uris=thrift://host.docker.internal:9083
spark.hadoop.hadoop.security.authentication=simple

# Memory and Resource Settings
spark.driver.memory=4g
spark.executor.memory=4g
spark.executor.cores=2
spark.executor.instances=2
spark.dynamicAllocation.enabled=true
spark.dynamicAllocation.minExecutors=1
spark.dynamicAllocation.maxExecutors=4
spark.dynamicAllocation.initialExecutors=1
spark.dynamicAllocation.executorIdleTimeout=60s
spark.dynamicAllocation.schedulerBacklogTimeout=1s
spark.dynamicAllocation.sustainedSchedulerBacklogTimeout=1s

# Engine Settings
kyuubi.engine.jdbc.connection.url=jdbc:hive2://localhost:10009
kyuubi.engine.jdbc.connection.driver=org.apache.hive.jdbc.HiveDriver
kyuubi.engine.spark.version=3.5.4
kyuubi.engine.spark.scala.version=2.12
kyuubi.engine.share.level=CONNECTION
kyuubi.engine.type=SPARK_SQL

# Authentication and HA
kyuubi.authentication=NONE
kyuubi.ha.enabled=false

# Session and Operation Timeouts (all in milliseconds)
kyuubi.session.engine.submit.timeout=600000
kyuubi.session.engine.initialize.timeout=600000
kyuubi.session.engine.idle.timeout=600000
kyuubi.operation.timeout=600000

# Frontend Settings
kyuubi.frontend.bind.host=0.0.0.0
kyuubi.frontend.protocols=THRIFT_BINARY,REST
kyuubi.frontend.thrift.binary.bind.port=10009
kyuubi.frontend.rest.bind.port=10099

# S3A Configuration for MinIO
spark.hadoop.fs.defaultFS=s3a://wba
spark.hadoop.fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem
spark.hadoop.fs.s3a.connection.ssl.enabled=false
spark.hadoop.fs.s3a.aws.credentials.provider=org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider
spark.hadoop.fs.s3a.access.key=minioadmin
spark.hadoop.fs.s3a.secret.key=minioadmin
spark.hadoop.fs.s3a.endpoint=http://minio:9000
spark.hadoop.fs.s3a.path.style.access=true

# S3A Performance Optimizations
spark.hadoop.fs.s3a.fast.upload=true
spark.hadoop.fs.s3a.multipart.size=5242880
spark.hadoop.fs.s3a.block.size=5242880
spark.hadoop.fs.s3a.multipart.threshold=5242880
spark.hadoop.fs.s3a.threads.core=10
spark.hadoop.fs.s3a.threads.max=20
spark.hadoop.fs.s3a.max.total.tasks=50
spark.hadoop.fs.s3a.connection.timeout=60000
spark.hadoop.fs.s3a.connection.establish.timeout=60000
spark.hadoop.fs.s3a.socket.timeout=60000
spark.hadoop.fs.s3a.connection.maximum=50
spark.hadoop.fs.s3a.fast.upload.buffer=bytebuffer
spark.hadoop.fs.s3a.fast.upload.active.blocks=2
spark.hadoop.fs.s3a.multipart.purge=false
spark.hadoop.fs.s3a.multipart.purge.age=86400000
spark.hadoop.fs.s3a.retry.limit=10
spark.hadoop.fs.s3a.retry.interval=1000
spark.hadoop.fs.s3a.attempts.maximum=10
spark.hadoop.fs.s3a.connection.request.timeout=60000
spark.hadoop.fs.s3a.threads.keepalivetime=60000

# Delta Lake Configuration
spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension
spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog
spark.delta.logStore.class=io.delta.storage.S3SingleDriverLogStore
spark.sql.warehouse.dir=s3a://wba/warehouse

# Query Optimization
spark.sql.shuffle.partitions=10
spark.sql.adaptive.coalescePartitions.enabled=true
spark.sql.adaptive.coalescePartitions.minPartitionNum=1
spark.sql.adaptive.coalescePartitions.initialPartitionNum=10
spark.sql.adaptive.advisoryPartitionSizeInBytes=128MB
spark.sql.adaptive.skewJoin.enabled=true
spark.sql.adaptive.skewJoin.skewedPartitionFactor=5
spark.sql.adaptive.skewJoin.skewedPartitionThresholdInBytes=256MB
spark.sql.adaptive.localShuffleReader.enabled=true
spark.sql.adaptive.optimizeSkewsInRebalancePartitions.enabled=true
spark.sql.adaptive.rebalancePartitions.enabled=true
spark.sql.adaptive.rebalancePartitions.minPartitionNum=1
spark.sql.adaptive.rebalancePartitions.initialPartitionNum=10

