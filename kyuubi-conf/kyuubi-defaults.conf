# # Spark Configuration
spark.master=spark://host.docker.internal:7077
spark.sql.adaptive.enabled=true
spark.jars.packages=io.delta:delta-core_2.12:2.4.0,org.apache.hadoop:hadoop-aws:3.3.6,org.apache.hadoop:hadoop-common:3.3.6,org.apache.kyuubi:kyuubi-spark-sql-engine_2.12:1.10.0



# # Kyuubi Configuration
# kyuubi.frontend.thrift.binary.protocol.version=BINARY
# kyuubi.frontend.thrift.binary.min.version=0.14.0
# kyuubi.frontend.thrift.binary.allow.older.versions=true
# kyuubi.frontend.thrift.binary.bind.host=0.0.0.0
# kyuubi.frontend.thrift.binary.bind.port=10009
# kyuubi.frontend.protocols=THRIFT_BINARY
# kyuubi.metrics.enabled=false
# kyuubi.engine.type=SPARK_SQL
# kyuubi.engine.share.level=USER
# kyuubi.engine.pool.size=1
# kyuubi.frontend.thrift.binary.authentication=NONE
# kyuubi.frontend.thrift.binary.sasl.qop=none

# Frontend Settings
# kyuubi.frontend.rest.bind.host=0.0.0.0
# kyuubi.frontend.rest.bind.port=10099
# kyuubi.web.ui.host=0.0.0.0

kyuubi.engine.spark.version=3.5.4
kyuubi.engine.spark.scala.version=2.12
# kyuubi.engine.share.level=CONNECTION
# kyuubi.engine.spark.master=local[*]  # Change to yarn if needed
# kyuubi.engine.spark.submit.deployMode=client
# kyuubi.engine.spark.home=/opt/spark
# kyuubi.session.engine.submit.timeout=300000
# kyuubi.session.engine.idle.timeout=3600000
# kyuubi.session.engine.initialize.timeout=600000
# kyuubi.engine.default.database=default
# kyuubi.engine.jdbc.connection.url=jdbc:hive2://localhost:10009/default;auth=none
# kyuubi.jdbc.driver.class=org.apache.hive.jdbc.HiveDriver

kyuubi.authentication=NONE
kyuubi.ha.enabled=false

kyuubi.frontend.bind.host=0.0.0.0
kyuubi.frontend.protocols=THRIFT_BINARY,REST
kyuubi.frontend.thrift.binary.bind.port=10009
kyuubi.frontend.rest.bind.port=10099
# kyuubi.ha.addresses=zookeeper:2181
kyuubi.session.engine.idle.timeout=PT5M
kyuubi.operation.incremental.collect=true
kyuubi.operation.progress.enabled=true
kyuubi.ha.enabled=false

# kyuubi.metrics.reporters=PROMETHEUS
# kyuubi.metrics.prometheus.port=10019

kyuubi.engine.session.initialize.sql \
      show namespaces in tpcds; \
      show namespaces in tpch; \
      show namespaces in postgres

# S3A Configuration for MinIO
spark.hadoop.fs.defaultFS=s3a://wba
spark.hadoop.fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem
spark.hadoop.fs.s3a.connection.ssl.enabled=true
spark.hadoop.fs.s3a.aws.credentials.provider=org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider
spark.hadoop.fs.s3a.access.key=minioadmin
spark.hadoop.fs.s3a.secret.key=minioadmin
spark.hadoop.fs.s3a.endpoint=http://minio:9000
spark.hadoop.fs.s3a.path.style.access=true
spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension
spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog

