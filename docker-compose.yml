services:
  minio:
    image: minio/minio:latest
    container_name: minio
    ports:
      - "9000:9000"
      - "9001:9001"
    environment:
      - MINIO_ROOT_USER=minioadmin
      - MINIO_ROOT_PASSWORD=minioadmin
      - MINIO_HTTP_BODY_SIZE_LIMIT=1G
      - MINIO_HTTP_MIN_PART_SIZE=64M
    command: server /data --console-address ":9001"
    volumes:
      - minio-data:/data
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9000/minio/health/live"]
      interval: 30s
      timeout: 20s
      retries: 3
      start_period: 10s
    networks:
      - spark-net

  postgres:
    hostname: postgres
    container_name: postgres
    image: postgres:10-alpine
    ports:
      - 5432:5432
    environment:
      POSTGRES_USER: admin
      POSTGRES_PASSWORD: admin
      POSTGRES_DB: metastore_db
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U admin -d metastore_db"]
      interval: 1s
      timeout: 10s
      retries: 10
    networks:
      - spark-net

  hive-metastore:
    hostname: hive-metastore
    container_name: hive-metastore
    build:
      context: .
      dockerfile: .devcontainer/Dockerfile.hive
    ports:
      - 9083:9083
    environment:
      SERVICE_NAME: metastore
      DB_DRIVER: postgres
      HIVE_CUSTOM_CONF_DIR: /opt/hive/conf
    volumes:
      - ./hive-config:/opt/hive/conf
      - ./delta-jars/hadoop-aws-3.4.1.jar:/opt/hive/lib/hadoop-aws-3.4.1.jar
      - ./delta-jars/aws-java-sdk-bundle-1.12.782.jar:/opt/hive/lib/aws-java-sdk-bundle-1.12.782.jar
    depends_on:
      - postgres
    networks:
      - spark-net

  hive-server:
    hostname: hive-server
    container_name: hive-server
    image: apache/hive:4.0.0
    ports:
      - 10000:10000
      - 10002:10002
    environment:
      SERVICE_NAME: hiveserver2
      IS_RESUME: "true"
      HIVE_CUSTOM_CONF_DIR: /opt/hive/conf
    volumes:
      - ./hive-config:/opt/hive/conf
      - ./delta-jars/hadoop-aws-3.4.1.jar:/opt/hive/lib/hadoop-aws-3.4.1.jar
      - ./delta-jars/aws-java-sdk-bundle-1.12.782.jar:/opt/hive/lib/aws-java-sdk-bundle-1.12.782.jar
    depends_on:
      - postgres
    networks:
      - spark-net

  spark-init:
    image: bitnami/spark:3.5.4
    container_name: spark-init
    user: "0"  # Run as root to avoid permission issues
    volumes:
      - ./spark-conf:/opt/bitnami/spark/conf:rw
    entrypoint: ["/bin/bash", "-c"]
    command:
      - |
        echo "Fixing Spark configuration permissions..."
        chown -R 1001:1001 /opt/bitnami/spark/conf
        chmod -R u+rwX,g+rX,o+rX /opt/bitnami/spark/conf
        echo "Permissions fixed."
    depends_on:
      minio:
        condition: service_healthy  # Ensure it runs after minio is healthy
    networks:
      - spark-net

  spark-master:
    image: bitnami/spark:3.5.5
    container_name: spark-master
    user: "1001"
    environment:
      - SPARK_MODE=master
      - SPARK_MASTER_PORT=7077
      - SPARK_MASTER_WEBUI_PORT=8080
      - AWS_ACCESS_KEY_ID=minioadmin
      - AWS_SECRET_ACCESS_KEY=minioadmin
      - SPARK_EXTRA_CLASSPATH=/opt/bitnami/spark/jars/*
      - HADOOP_CLASSPATH=/opt/bitnami/spark/jars/*
    ports:
      - "8080:8080"
      - "7077:7077"
    volumes:
      - ./spark-conf:/opt/bitnami/spark/conf:rw,z
      - ./delta-jars:/opt/bitnami/spark/jars
    networks:
      - spark-net
    deploy:
        restart_policy:
          condition: on-failure
          delay: 5s
          max_attempts: 3
    healthcheck:
      test: ["CMD", "true"]
      interval: 5s
      timeout: 5s
      retries: 3
    depends_on:
      spark-init:
        condition: service_completed_successfully
      minio:
        condition: service_healthy
        
  spark-worker:
    image: bitnami/spark:3.5.5
    container_name: spark-worker
    user: "1001"
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_WORKER_MEMORY=4G
      - SPARK_WORKER_CORES=2
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
      - SPARK_EXTRA_CLASSPATH=/opt/bitnami/spark/jars/*
      - HADOOP_CLASSPATH=/opt/bitnami/spark/jars/*
    volumes:
      - ./spark-conf:/opt/bitnami/spark/conf
      - ./delta-jars:/opt/bitnami/spark/jars
    depends_on:
      - spark-master
    networks:
      - spark-net


  deltasharing:
    image: deltaio/delta-sharing-server:0.6.7 
    platform: linux/amd64
    container_name: deltasharing
    ports:
      - "7777:80"
    volumes:
      - ./delta-sharing-conf/delta-sharing-server-config.yaml:/config/delta-sharing-server-config.yaml
      - ./delta-jars:/opt/delta-sharing/jars
    command: ["--config", "/config/delta-sharing-server-config.yaml"]
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:80/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    depends_on:
      - minio
    networks:
      - spark-net

  createbucket:
    container_name: createbucket
    image: minio/mc:latest
    depends_on:
      - minio
    entrypoint: >
      /bin/sh -c "
      until (/usr/bin/mc alias set myminio http://minio:9000 minioadmin minioadmin); do echo 'Waiting for MinIO...'; sleep 1; done;
      /usr/bin/mc mb myminio/wba;
      /usr/bin/mc policy set public myminio/wba;
      echo 'Bucket created successfully `myminio/wba`.';
      echo 'Access permission for `myminio/wba` is set to `public`';
      "
    networks:
      - spark-net

  kyuubi:
    container_name: kyuubi-server
    build: 
      context: .
      dockerfile: .devcontainer/Dockerfile.kyuubi
    ports:
      - "10009:10009"
      - "10099:10099"
    environment:
      - KYUUBI_HOME=/opt/kyuubi
      - KYUUBI_WORK_DIR_ROOT=/opt/kyuubi/work
      - SPARK_HOME=/opt/spark
    healthcheck:
      test: ["CMD", "curl", "-f", "http://kyuubi-server:10099/api/v1/ping"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 20s  
    volumes:
      - ./delta-sharing-conf/delta-sharing-server-config.yaml:/config/delta-sharing-server-config.yaml
    depends_on:
      - spark-master
      - minio
    networks:
      - spark-net

 
  dev:
    container_name: dev
    build: 
      context: .
      dockerfile: .devcontainer/Dockerfile.dev
    volumes:
      - ..:/workspace:cached
      - delta-spark-cache:/root/.cache
      - /var/run/docker.sock:/var/run/docker.sock
      - ./delta-jars:/opt/spark/jars
    user: vscode
    command: sleep infinity
    environment:
      - PYTHONPATH=/workspace
      - SPARK_HOME=/opt/spark
      - DEBIAN_FRONTEND=noninteractive
      - AWS_ACCESS_KEY_ID=minioadmin
      - AWS_SECRET_ACCESS_KEY=minioadmin
      - SPARK_EXTRA_CLASSPATH=/opt/spark/jars/*
      - HADOOP_CLASSPATH=/opt/spark/jars/*
      - SPARK_LOCAL_IP=0.0.0.0
      - SPARK_MASTER=spark://spark-master:7077
      - SPARK_DRIVER_HOST=0.0.0.0
      - SPARK_DRIVER_PORT=0
      - SPARK_BLOCK_MANAGER_PORT=0
      - SPARK_SQL_EXTENSIONS=io.delta.sql.DeltaSparkSessionExtension
      - SPARK_SQL_CATALOG_IMPLEMENTATION=org.apache.spark.sql.delta.catalog.DeltaCatalog
      - HADOOP_HOME=/opt/hadoop
      - HADOOP_CONF_DIR=/opt/hadoop/conf
      - HADOOP_CLASSPATH=/opt/spark/jars/*
      - HADOOP_OPTS="-Dfs.s3a.endpoint=http://minio:9000 -Dfs.s3a.access.key=minioadmin -Dfs.s3a.secret.key=minioadmin -Dfs.s3a.path.style.access=true"
      - SPARK_CLASSPATH=/opt/spark/jars/*
      - SPARK_DRIVER_EXTRA_CLASSPATH=/opt/spark/jars/*
      - SPARK_EXECUTOR_EXTRA_CLASSPATH=/opt/spark/jars/*
    depends_on:
      - kyuubi
      - spark-master
      - minio
    networks:
      - spark-net

networks:
  spark-net:
    driver: bridge

volumes:
  minio-data:
  delta-spark-cache:
  kyuubi-spark-jars: