{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Up\n",
    "\n",
    "Choose python environment /usr/local/bin/python3 and install the required packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DateType, TimestampType, DoubleType\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "def create_spark_session(app_name=\"EHR Data Loader\", aws_access_key=None, aws_secret_key=None):\n",
    "    \"\"\"\n",
    "    Create and return a Spark session configured for Delta Lake with S3 access.\n",
    "    \"\"\"\n",
    "    builder = (SparkSession.builder\n",
    "               .appName(app_name)\n",
    "               .config(\"spark.jars.packages\", \"io.delta:delta-core_2.12:2.4.0,org.apache.hadoop:hadoop-aws:3.3.1\")\n",
    "               .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "               .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\"))\n",
    "\n",
    "    # Configure S3 access if credentials are provided\n",
    "    if aws_access_key and aws_secret_key:\n",
    "        builder = (builder\n",
    "                   .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://localhost:9000\")\n",
    "                   .config(\"spark.hadoop.fs.s3a.access.key\", aws_access_key)\n",
    "                   .config(\"spark.hadoop.fs.s3a.secret.key\", aws_secret_key)\n",
    "                   .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "                   .config(\"spark.hadoop.fs.s3a.aws.credentials.provider\", \"org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider\"))\n",
    "    else:\n",
    "        # Use instance profile or environment variables for authentication\n",
    "        builder = builder.config(\"spark.hadoop.fs.s3a.aws.credentials.provider\",\n",
    "                                 \"com.amazonaws.auth.DefaultAWSCredentialsProviderChain\")\n",
    "\n",
    "    # Additional S3 optimizations\n",
    "    builder = (builder\n",
    "               .config(\"spark.hadoop.fs.s3a.connection.maximum\", 100)\n",
    "               .config(\"spark.hadoop.fs.s3a.experimental.input.fadvise\", \"sequential\")\n",
    "               .config(\"spark.hadoop.fs.s3a.fast.upload\", \"true\")\n",
    "               .config(\"spark.hadoop.fs.s3a.block.size\", 128 * 1024 * 1024))\n",
    "\n",
    "    return builder.getOrCreate()\n",
    "\n",
    "\n",
    "def infer_schema_from_file(spark, file_path, sample_size=1000):\n",
    "    \"\"\"\n",
    "    Infer schema from a CSV file by reading a sample.\n",
    "    \"\"\"\n",
    "    # Read a sample of the CSV file to infer schema\n",
    "    sample_df = spark.read.option(\"header\", \"true\").option(\n",
    "        \"inferSchema\", \"true\").csv(file_path).limit(sample_size)\n",
    "    return sample_df.schema\n",
    "\n",
    "\n",
    "def load_file_to_delta(spark, file_path, database_name, table_name, mode=\"overwrite\", partition_by=None):\n",
    "    \"\"\"\n",
    "    Load a CSV file into a Delta table using a database name.\n",
    "\n",
    "    Args:\n",
    "        spark: SparkSession object.\n",
    "        file_path: Path to the CSV file (can be S3 URI).\n",
    "        database_name: Name of the database to store the Delta table.\n",
    "        table_name: Name of the table to create.\n",
    "        mode: Write mode (overwrite, append, etc.).\n",
    "        partition_by: Column(s) to partition the data by.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Ensure the database exists\n",
    "        spark.sql(f\"CREATE DATABASE IF NOT EXISTS {database_name}\")\n",
    "\n",
    "        # Infer schema from file\n",
    "        schema = infer_schema_from_file(spark, file_path)\n",
    "\n",
    "        # Read the CSV file with the inferred schema\n",
    "        df = spark.read.option(\"header\", \"true\").schema(schema).csv(file_path)\n",
    "\n",
    "        # Add metadata columns\n",
    "        df = df.withColumn(\"ingestion_timestamp\",\n",
    "                           spark.sql(\"current_timestamp()\"))\n",
    "        df = df.withColumn(\"source_file\", spark.sql(\n",
    "            f\"'{file_path.split('/')[-1]}'\"))\n",
    "\n",
    "        # Define the full table name\n",
    "        full_table_name = f\"{database_name}.{table_name}\"\n",
    "\n",
    "        # Write to Delta Lake using saveAsTable\n",
    "        writer = df.write.format(\"delta\").mode(\n",
    "            mode).option(\"overwriteSchema\", \"true\")\n",
    "\n",
    "        if partition_by:\n",
    "            writer = writer.partitionBy(partition_by)\n",
    "\n",
    "        writer.saveAsTable(full_table_name)\n",
    "\n",
    "        print(f\"Successfully loaded {file_path} into table {full_table_name}\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(\n",
    "            f\"Error loading {file_path} into table {full_table_name}: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "def list_s3_files(spark, s3_dir_path, file_extension=\".csv\"):\n",
    "    \"\"\"\n",
    "    List files in an S3 directory with a specific extension.\n",
    "\n",
    "    Args:\n",
    "        spark: SparkSession object\n",
    "        s3_dir_path: S3 directory path (e.g., s3a://bucket-name/path/)\n",
    "        file_extension: File extension to filter by\n",
    "\n",
    "    Returns:\n",
    "        List of file paths\n",
    "    \"\"\"\n",
    "    # Create a DataFrame representing the files\n",
    "    files_df = spark.read.format(\"binaryFile\").load(s3_dir_path)\n",
    "\n",
    "    # Filter files by extension and collect their paths\n",
    "    csv_files = files_df.filter(files_df.path.endswith(\n",
    "        file_extension)).select(\"path\").collect()\n",
    "\n",
    "    return [row.path for row in csv_files]\n",
    "\n",
    "\n",
    "def load_ehr_data_to_delta(ehr_s3_path, database_name, aws_access_key=None, aws_secret_key=None):\n",
    "    \"\"\"\n",
    "    Load all EHR CSV files from S3 into Delta tables using a database.\n",
    "\n",
    "    Args:\n",
    "        ehr_s3_path: S3 URI to directory containing EHR CSV files.\n",
    "        database_name: Database name where tables will be stored.\n",
    "        aws_access_key: AWS access key (optional).\n",
    "        aws_secret_key: AWS secret key (optional).\n",
    "    \"\"\"\n",
    "    # Create Spark session\n",
    "    spark = create_spark_session(\n",
    "        aws_access_key=aws_access_key, aws_secret_key=aws_secret_key)\n",
    "\n",
    "    # Define partition strategies for specific tables\n",
    "    partition_config = {\n",
    "        \"patients.csv\": [\"gender\"],\n",
    "        \"encounters.csv\": [\"year\", \"month\"],\n",
    "        \"medications.csv\": [\"year\"],\n",
    "        \"observations.csv\": [\"year\", \"month\"],\n",
    "        \"procedures.csv\": [\"year\"],\n",
    "        \"imaging_studies.csv\": [\"year\"],\n",
    "        \"conditions.csv\": [\"year\"],\n",
    "        \"immunizations.csv\": [\"year\"],\n",
    "        \"allergies.csv\": None,\n",
    "        \"careplans.csv\": None,\n",
    "        \"organizations.csv\": None,\n",
    "        \"providers.csv\": None\n",
    "    }\n",
    "\n",
    "    # List all CSV files in the S3 directory\n",
    "    s3_files = list_s3_files(spark, ehr_s3_path, \".csv\")\n",
    "\n",
    "    # Process each file\n",
    "    results = {}\n",
    "\n",
    "    for file_path in s3_files:\n",
    "        file_name = file_path.split('/')[-1]\n",
    "        if file_name.endswith(\".csv\"):\n",
    "            table_name = file_name.split('.')[0]\n",
    "\n",
    "            # Get partition columns if defined\n",
    "            partition_by = partition_config.get(file_name)\n",
    "\n",
    "            # Load file to Delta table in the database\n",
    "            success = load_file_to_delta(\n",
    "                spark, file_path, database_name, table_name, partition_by=partition_by)\n",
    "            results[file_name] = success\n",
    "\n",
    "    # Print summary\n",
    "    print(\"\\nSummary of Delta Lake table loading:\")\n",
    "    for file_name, success in results.items():\n",
    "        status = \"Success\" if success else \"Failed\"\n",
    "        print(f\"{file_name}: {status}\")\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # S3 paths\n",
    "    database_name = \"ehr\"\n",
    "\n",
    "    # Option 1: Using AWS credentials\n",
    "    aws_access_key = \"minioadmin\"  # Replace with your key or use None\n",
    "    aws_secret_key = \"minioadmin\"  # Replace with your key or use None\n",
    "\n",
    "    # Update S3 path to use s3a protocol\n",
    "    ehr_s3_path = \"s3a://ehr/\"\n",
    "    # Configure additional S3 settings for MinIO\n",
    "    spark = create_spark_session(\n",
    "        aws_access_key=aws_access_key, aws_secret_key=aws_secret_key)\n",
    "\n",
    "    load_ehr_data_to_delta(ehr_s3_path, database_name,\n",
    "                           aws_access_key, aws_secret_key)\n",
    "\n",
    "    # Option 2: Using instance profile or environment variables\n",
    "    # load_ehr_data_to_delta(ehr_s3_path, delta_s3_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
