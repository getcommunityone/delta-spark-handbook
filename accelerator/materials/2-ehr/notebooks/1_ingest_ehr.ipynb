{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Only remote Spark sessions using Databricks Connect are supported. Could not find connection parameters to start a Spark remote session.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 174\u001b[0m\n\u001b[1;32m    171\u001b[0m aws_access_key \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mminioadmin\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# Replace with your key or use None\u001b[39;00m\n\u001b[1;32m    172\u001b[0m aws_secret_key \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mminioadmin\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# Replace with your key or use None\u001b[39;00m\n\u001b[0;32m--> 174\u001b[0m \u001b[43mload_ehr_data_to_delta\u001b[49m\u001b[43m(\u001b[49m\u001b[43mehr_s3_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatabase_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maws_access_key\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maws_secret_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;66;03m# Option 2: Using instance profile or environment variables\u001b[39;00m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;66;03m# load_ehr_data_to_delta(ehr_s3_path, delta_s3_path)\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[1], line 120\u001b[0m, in \u001b[0;36mload_ehr_data_to_delta\u001b[0;34m(ehr_s3_path, database_name, aws_access_key, aws_secret_key)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;124;03mLoad all EHR CSV files from S3 into Delta tables using a database.\u001b[39;00m\n\u001b[1;32m    112\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;124;03m    aws_secret_key: AWS secret key (optional).\u001b[39;00m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;66;03m# Create Spark session\u001b[39;00m\n\u001b[0;32m--> 120\u001b[0m spark \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_spark_session\u001b[49m\u001b[43m(\u001b[49m\u001b[43maws_access_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maws_access_key\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maws_secret_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maws_secret_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;66;03m# Define partition strategies for specific tables\u001b[39;00m\n\u001b[1;32m    123\u001b[0m partition_config \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpatients.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgender\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    125\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencounters.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myear\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmonth\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    135\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mproviders.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    136\u001b[0m }\n",
      "Cell \u001b[0;32mIn[1], line 35\u001b[0m, in \u001b[0;36mcreate_spark_session\u001b[0;34m(app_name, aws_access_key, aws_secret_key)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# Additional S3 optimizations\u001b[39;00m\n\u001b[1;32m     29\u001b[0m builder \u001b[38;5;241m=\u001b[39m (builder\n\u001b[1;32m     30\u001b[0m           \u001b[38;5;241m.\u001b[39mconfig(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspark.hadoop.fs.s3a.connection.maximum\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m100\u001b[39m)\n\u001b[1;32m     31\u001b[0m           \u001b[38;5;241m.\u001b[39mconfig(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspark.hadoop.fs.s3a.experimental.input.fadvise\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msequential\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     32\u001b[0m           \u001b[38;5;241m.\u001b[39mconfig(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspark.hadoop.fs.s3a.fast.upload\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrue\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     33\u001b[0m           \u001b[38;5;241m.\u001b[39mconfig(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspark.hadoop.fs.s3a.block.size\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m128\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1024\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1024\u001b[39m))\n\u001b[0;32m---> 35\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbuilder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/sql/session.py:526\u001b[0m, in \u001b[0;36mSparkSession.Builder.getOrCreate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    520\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    521\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m PySparkRuntimeError(\n\u001b[1;32m    522\u001b[0m                 error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSESSION_ALREADY_EXIST\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    523\u001b[0m                 message_parameters\u001b[38;5;241m=\u001b[39m{},\n\u001b[1;32m    524\u001b[0m             )\n\u001b[0;32m--> 526\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    527\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOnly remote Spark sessions using Databricks Connect are supported. \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m\n\u001b[1;32m    528\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not find connection parameters to start a Spark remote session.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    529\u001b[0m )\n\u001b[1;32m    531\u001b[0m session \u001b[38;5;241m=\u001b[39m SparkSession\u001b[38;5;241m.\u001b[39m_instantiatedSession\n\u001b[1;32m    532\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m session \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m session\u001b[38;5;241m.\u001b[39m_sc\u001b[38;5;241m.\u001b[39m_jsc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Only remote Spark sessions using Databricks Connect are supported. Could not find connection parameters to start a Spark remote session."
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DateType, TimestampType, DoubleType\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "def create_spark_session(app_name=\"EHR Data Loader\", aws_access_key=None, aws_secret_key=None):\n",
    "    \"\"\"\n",
    "    Create and return a Spark session configured for Delta Lake with S3 access.\n",
    "    \"\"\"\n",
    "    builder = (SparkSession.builder\n",
    "              .appName(app_name)\n",
    "              .config(\"spark.jars.packages\", \"io.delta:delta-core_2.12:2.4.0,org.apache.hadoop:hadoop-aws:3.3.1\")\n",
    "              .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "              .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\"))\n",
    "    \n",
    "    # Configure S3 access if credentials are provided\n",
    "    if aws_access_key and aws_secret_key:\n",
    "        builder = (builder\n",
    "                  .config(\"spark.hadoop.fs.s3a.access.key\", aws_access_key)\n",
    "                  .config(\"spark.hadoop.fs.s3a.secret.key\", aws_secret_key)\n",
    "                  .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "                  .config(\"spark.hadoop.fs.s3a.aws.credentials.provider\", \"org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider\"))\n",
    "    else:\n",
    "        # Use instance profile or environment variables for authentication\n",
    "        builder = builder.config(\"spark.hadoop.fs.s3a.aws.credentials.provider\", \n",
    "                                \"com.amazonaws.auth.DefaultAWSCredentialsProviderChain\")\n",
    "    \n",
    "    # Additional S3 optimizations\n",
    "    builder = (builder\n",
    "              .config(\"spark.hadoop.fs.s3a.connection.maximum\", 100)\n",
    "              .config(\"spark.hadoop.fs.s3a.experimental.input.fadvise\", \"sequential\")\n",
    "              .config(\"spark.hadoop.fs.s3a.fast.upload\", \"true\")\n",
    "              .config(\"spark.hadoop.fs.s3a.block.size\", 128 * 1024 * 1024))\n",
    "    \n",
    "    return builder.getOrCreate()\n",
    "\n",
    "def infer_schema_from_file(spark, file_path, sample_size=1000):\n",
    "    \"\"\"\n",
    "    Infer schema from a CSV file by reading a sample.\n",
    "    \"\"\"\n",
    "    # Read a sample of the CSV file to infer schema\n",
    "    sample_df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(file_path).limit(sample_size)\n",
    "    return sample_df.schema\n",
    "\n",
    "def load_file_to_delta(spark, file_path, database_name, table_name, mode=\"overwrite\", partition_by=None):\n",
    "    \"\"\"\n",
    "    Load a CSV file into a Delta table using a database name.\n",
    "\n",
    "    Args:\n",
    "        spark: SparkSession object.\n",
    "        file_path: Path to the CSV file (can be S3 URI).\n",
    "        database_name: Name of the database to store the Delta table.\n",
    "        table_name: Name of the table to create.\n",
    "        mode: Write mode (overwrite, append, etc.).\n",
    "        partition_by: Column(s) to partition the data by.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Ensure the database exists\n",
    "        spark.sql(f\"CREATE DATABASE IF NOT EXISTS {database_name}\")\n",
    "\n",
    "        # Infer schema from file\n",
    "        schema = infer_schema_from_file(spark, file_path)\n",
    "\n",
    "        # Read the CSV file with the inferred schema\n",
    "        df = spark.read.option(\"header\", \"true\").schema(schema).csv(file_path)\n",
    "\n",
    "        # Add metadata columns\n",
    "        df = df.withColumn(\"ingestion_timestamp\", spark.sql(\"current_timestamp()\"))\n",
    "        df = df.withColumn(\"source_file\", spark.sql(f\"'{file_path.split('/')[-1]}'\"))\n",
    "\n",
    "        # Define the full table name\n",
    "        full_table_name = f\"{database_name}.{table_name}\"\n",
    "\n",
    "        # Write to Delta Lake using saveAsTable\n",
    "        writer = df.write.format(\"delta\").mode(mode).option(\"overwriteSchema\", \"true\")\n",
    "\n",
    "        if partition_by:\n",
    "            writer = writer.partitionBy(partition_by)\n",
    "\n",
    "        writer.saveAsTable(full_table_name)\n",
    "\n",
    "        print(f\"Successfully loaded {file_path} into table {full_table_name}\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {file_path} into table {full_table_name}: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "def list_s3_files(spark, s3_dir_path, file_extension=\".csv\"):\n",
    "    \"\"\"\n",
    "    List files in an S3 directory with a specific extension.\n",
    "    \n",
    "    Args:\n",
    "        spark: SparkSession object\n",
    "        s3_dir_path: S3 directory path (e.g., s3://bucket-name/path/)\n",
    "        file_extension: File extension to filter by\n",
    "        \n",
    "    Returns:\n",
    "        List of file paths\n",
    "    \"\"\"\n",
    "    # Create a DataFrame representing the files\n",
    "    files_df = spark.read.format(\"binaryFile\").load(s3_dir_path)\n",
    "    \n",
    "    # Filter files by extension and collect their paths\n",
    "    csv_files = files_df.filter(files_df.path.endswith(file_extension)).select(\"path\").collect()\n",
    "    \n",
    "    return [row.path for row in csv_files]\n",
    "\n",
    "def load_ehr_data_to_delta(ehr_s3_path, database_name, aws_access_key=None, aws_secret_key=None):\n",
    "    \"\"\"\n",
    "    Load all EHR CSV files from S3 into Delta tables using a database.\n",
    "\n",
    "    Args:\n",
    "        ehr_s3_path: S3 URI to directory containing EHR CSV files.\n",
    "        database_name: Database name where tables will be stored.\n",
    "        aws_access_key: AWS access key (optional).\n",
    "        aws_secret_key: AWS secret key (optional).\n",
    "    \"\"\"\n",
    "    # Create Spark session\n",
    "    spark = create_spark_session(aws_access_key=aws_access_key, aws_secret_key=aws_secret_key)\n",
    "\n",
    "    # Define partition strategies for specific tables\n",
    "    partition_config = {\n",
    "        \"patients.csv\": [\"gender\"],\n",
    "        \"encounters.csv\": [\"year\", \"month\"],\n",
    "        \"medications.csv\": [\"year\"],\n",
    "        \"observations.csv\": [\"year\", \"month\"],\n",
    "        \"procedures.csv\": [\"year\"],\n",
    "        \"imaging_studies.csv\": [\"year\"],\n",
    "        \"conditions.csv\": [\"year\"],\n",
    "        \"immunizations.csv\": [\"year\"],\n",
    "        \"allergies.csv\": None,\n",
    "        \"careplans.csv\": None,\n",
    "        \"organizations.csv\": None,\n",
    "        \"providers.csv\": None\n",
    "    }\n",
    "\n",
    "    # List all CSV files in the S3 directory\n",
    "    s3_files = list_s3_files(spark, ehr_s3_path, \".csv\")\n",
    "\n",
    "    # Process each file\n",
    "    results = {}\n",
    "\n",
    "    for file_path in s3_files:\n",
    "        file_name = file_path.split('/')[-1]\n",
    "        if file_name.endswith(\".csv\"):\n",
    "            table_name = file_name.split('.')[0]\n",
    "\n",
    "            # Get partition columns if defined\n",
    "            partition_by = partition_config.get(file_name)\n",
    "\n",
    "            # Load file to Delta table in the database\n",
    "            success = load_file_to_delta(spark, file_path, database_name, table_name, partition_by=partition_by)\n",
    "            results[file_name] = success\n",
    "\n",
    "    # Print summary\n",
    "    print(\"\\nSummary of Delta Lake table loading:\")\n",
    "    for file_name, success in results.items():\n",
    "        status = \"Success\" if success else \"Failed\"\n",
    "        print(f\"{file_name}: {status}\")\n",
    "\n",
    "    return results\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # S3 paths\n",
    "    ehr_s3_path = \"s3://localhost:9001/ehr/\"\n",
    "    database_name = \"ehr\"\n",
    "    \n",
    "    # Option 1: Using AWS credentials\n",
    "    aws_access_key = \"minioadmin\"  # Replace with your key or use None\n",
    "    aws_secret_key = \"minioadmin\"  # Replace with your key or use None\n",
    "    \n",
    "    load_ehr_data_to_delta(ehr_s3_path, database_name, aws_access_key, aws_secret_key)\n",
    "    \n",
    "    # Option 2: Using instance profile or environment variables\n",
    "    # load_ehr_data_to_delta(ehr_s3_path, delta_s3_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
