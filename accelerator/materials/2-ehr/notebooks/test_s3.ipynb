{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# S3 Security Debugging for PySpark Delta Lake\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s [%(levelname)s] %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler(\"s3_debug.log\"),\n",
    "        logging.StreamHandler(sys.stdout)\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(\"s3-debug\")\n",
    "\n",
    "\n",
    "def create_debug_spark_session(app_name=\"S3 Debug Session\",\n",
    "                               aws_access_key=None,\n",
    "                               aws_secret_key=None,\n",
    "                               endpoint_url=None,\n",
    "                               path_style_access=False,\n",
    "                               log_level=\"INFO\"):\n",
    "    \"\"\"\n",
    "    Create a Spark session with enhanced logging for S3 debugging.\n",
    "\n",
    "    Args:\n",
    "        app_name: Name for the Spark application\n",
    "        aws_access_key: AWS access key\n",
    "        aws_secret_key: AWS secret key\n",
    "        endpoint_url: Custom S3 endpoint (for MinIO, LocalStack, etc.)\n",
    "        path_style_access: Use path-style access instead of virtual-hosted style\n",
    "        log_level: Hadoop log level (INFO, DEBUG, etc.)\n",
    "    \"\"\"\n",
    "    logger.info(f\"Creating debug Spark session with log level {log_level}\")\n",
    "\n",
    "    # Start building the session\n",
    "    builder = (SparkSession.builder\n",
    "               .appName(app_name)\n",
    "               .config(\"spark.jars.packages\",\n",
    "                       \"io.delta:delta-core_2.12:2.4.0,org.apache.hadoop:hadoop-aws:3.3.1\")\n",
    "               .config(\"spark.sql.extensions\",\n",
    "                       \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "               .config(\"spark.sql.catalog.spark_catalog\",\n",
    "                       \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "               # Enable verbose Hadoop logging\n",
    "               .config(\"spark.hadoop.fs.s3a.impl.disable.cache\", \"true\")\n",
    "               .config(f\"spark.hadoop.fs.s3a.logger.class\", \"org.apache.commons.logging.impl.Log4JLogger\")\n",
    "               .config(f\"spark.driver.extraJavaOptions\", f\"-Dlog4j.logger.org.apache.hadoop.fs.s3a={log_level}\")\n",
    "               .config(f\"spark.executor.extraJavaOptions\", f\"-Dlog4j.logger.org.apache.hadoop.fs.s3a={log_level}\")\n",
    "               .config(\"spark.hadoop.fs.s3a.committer.name\", \"directory\")\n",
    "               .config(\"spark.hadoop.fs.s3a.committer.staging.conflict-mode\", \"append\")\n",
    "               .config(\"spark.hadoop.mapreduce.outputcommitter.factory.scheme.s3a\",\n",
    "                       \"org.apache.hadoop.fs.s3a.commit.S3ACommitterFactory\"))\n",
    "\n",
    "    # Configure S3 access if credentials are provided\n",
    "    if aws_access_key and aws_secret_key:\n",
    "        logger.info(\"Configuring with explicit AWS credentials\")\n",
    "        builder = (builder\n",
    "                   .config(\"spark.hadoop.fs.s3a.access.key\", aws_access_key)\n",
    "                   .config(\"spark.hadoop.fs.s3a.secret.key\", aws_secret_key)\n",
    "                   .config(\"spark.hadoop.fs.s3a.aws.credentials.provider\",\n",
    "                           \"org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider\"))\n",
    "    else:\n",
    "        logger.info(\"Using default AWS credential chain\")\n",
    "        builder = builder.config(\"spark.hadoop.fs.s3a.aws.credentials.provider\",\n",
    "                                 \"com.amazonaws.auth.DefaultAWSCredentialsProviderChain\")\n",
    "\n",
    "    # Configure custom endpoint (for MinIO, etc.)\n",
    "    if endpoint_url:\n",
    "        logger.info(f\"Using custom S3 endpoint: {endpoint_url}\")\n",
    "        builder = builder.config(\"spark.hadoop.fs.s3a.endpoint\", endpoint_url)\n",
    "\n",
    "        # For MinIO and other S3-compatible storage systems\n",
    "        if path_style_access:\n",
    "            logger.info(\"Enabling path-style access\")\n",
    "            builder = builder.config(\n",
    "                \"spark.hadoop.fs.s3a.path.style.access\", \"true\")\n",
    "\n",
    "    spark = builder.getOrCreate()\n",
    "\n",
    "    # Print all S3A configurations for debugging\n",
    "    logger.info(\"Current S3A configuration:\")\n",
    "    hadoop_conf = spark.sparkContext._jsc.hadoopConfiguration()\n",
    "    s3a_keys = [key for key in hadoop_conf.iterator() if 's3a' in key]\n",
    "\n",
    "    for key in s3a_keys:\n",
    "        # Don't log the actual secret keys\n",
    "        if 'secret' in key or 'password' in key:\n",
    "            logger.info(f\"{key}: ********\")\n",
    "        else:\n",
    "            logger.info(f\"{key}: {hadoop_conf.get(key)}\")\n",
    "\n",
    "    return spark\n",
    "\n",
    "\n",
    "def test_s3_connection(spark, s3_path):\n",
    "    \"\"\"\n",
    "    Test basic S3 connectivity by listing objects.\n",
    "\n",
    "    Args:\n",
    "        spark: SparkSession\n",
    "        s3_path: S3 path to test (e.g., s3a://bucket/path/)\n",
    "    \"\"\"\n",
    "    logger.info(f\"Testing S3 connectivity to {s3_path}\")\n",
    "    try:\n",
    "        # Try to list files\n",
    "        file_count = spark.sparkContext.textFile(s3_path).count()\n",
    "        logger.info(\n",
    "            f\"Successfully connected to {s3_path}. Found {file_count} partitions.\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to connect to {s3_path}: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "def verify_s3_permissions(spark, s3_bucket, test_operations=None):\n",
    "    \"\"\"\n",
    "    Verify S3 permissions by attempting different operations.\n",
    "\n",
    "    Args:\n",
    "        spark: SparkSession\n",
    "        s3_bucket: Bucket name (without s3:// prefix)\n",
    "        test_operations: List of operations to test ['read', 'write', 'delete']\n",
    "    \"\"\"\n",
    "    if test_operations is None:\n",
    "        test_operations = ['read']\n",
    "\n",
    "    s3_path = f\"s3a://{s3_bucket}/\"\n",
    "    results = {}\n",
    "\n",
    "    logger.info(f\"Verifying S3 permissions on bucket: {s3_bucket}\")\n",
    "\n",
    "    if 'read' in test_operations:\n",
    "        logger.info(\"Testing READ permission...\")\n",
    "        try:\n",
    "            # Try listing files\n",
    "            files = spark.sparkContext._jsc.hadoopConfiguration().get(\"fs.s3a.impl\")\n",
    "            logger.info(f\"S3A implementation: {files}\")\n",
    "            files = spark.read.text(s3_path).count()\n",
    "            logger.info(f\"READ test: Success. Found {files} files.\")\n",
    "            results['read'] = True\n",
    "        except Exception as e:\n",
    "            logger.error(f\"READ test: Failed. Error: {str(e)}\")\n",
    "            results['read'] = False\n",
    "\n",
    "    if 'write' in test_operations:\n",
    "        logger.info(\"Testing WRITE permission...\")\n",
    "        test_data_path = f\"{s3_path}test_write_permissions.txt\"\n",
    "        try:\n",
    "            # Try writing a test file\n",
    "            test_df = spark.createDataFrame([(\"test\",)], [\"col1\"])\n",
    "            test_df.write.mode(\"overwrite\").text(test_data_path)\n",
    "            logger.info(\"WRITE test: Success\")\n",
    "            results['write'] = True\n",
    "        except Exception as e:\n",
    "            logger.error(f\"WRITE test: Failed. Error: {str(e)}\")\n",
    "            results['write'] = False\n",
    "\n",
    "    if 'delete' in test_operations and results.get('write', False):\n",
    "        logger.info(\"Testing DELETE permission...\")\n",
    "        try:\n",
    "            # Try deleting the test file\n",
    "            fs = spark._jvm.org.apache.hadoop.fs.FileSystem.get(\n",
    "                spark._jsc.hadoopConfiguration())\n",
    "            path = spark._jvm.org.apache.hadoop.fs.Path(test_data_path)\n",
    "            deleted = fs.delete(path, True)\n",
    "            logger.info(f\"DELETE test: {'Success' if deleted else 'Failed'}\")\n",
    "            results['delete'] = deleted\n",
    "        except Exception as e:\n",
    "            logger.error(f\"DELETE test: Failed. Error: {str(e)}\")\n",
    "            results['delete'] = False\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def diagnose_s3_issue(error_message):\n",
    "    \"\"\"\n",
    "    Analyze error message and suggest potential fixes.\n",
    "\n",
    "    Args:\n",
    "        error_message: The error message from the exception\n",
    "    \"\"\"\n",
    "    logger.info(f\"Diagnosing S3 error: {error_message}\")\n",
    "\n",
    "    diagnoses = {\n",
    "        \"Status Code: 403\": {\n",
    "            \"issue\": \"Access Denied - Permission issue\",\n",
    "            \"solutions\": [\n",
    "                \"Check IAM permissions for the bucket\",\n",
    "                \"Verify AWS credentials are correct\",\n",
    "                \"Check bucket policy for necessary permissions\"\n",
    "            ]\n",
    "        },\n",
    "        \"Status Code: 404\": {\n",
    "            \"issue\": \"Not Found - Bucket or path doesn't exist\",\n",
    "            \"solutions\": [\n",
    "                \"Verify bucket name and region\",\n",
    "                \"Check if the specified path exists in the bucket\"\n",
    "            ]\n",
    "        },\n",
    "        \"No such file or directory\": {\n",
    "            \"issue\": \"Path not found\",\n",
    "            \"solutions\": [\n",
    "                \"Verify the path exists in the bucket\",\n",
    "                \"Check for typos in the path\"\n",
    "            ]\n",
    "        },\n",
    "        \"Unable to find credentials\": {\n",
    "            \"issue\": \"Missing or invalid credentials\",\n",
    "            \"solutions\": [\n",
    "                \"Provide explicit AWS credentials\",\n",
    "                \"Check environment variables for AWS credentials\",\n",
    "                \"Configure instance profile if running on EC2\"\n",
    "            ]\n",
    "        },\n",
    "        \"Connection timed out\": {\n",
    "            \"issue\": \"Network connectivity issue\",\n",
    "            \"solutions\": [\n",
    "                \"Check network connectivity to S3\",\n",
    "                \"Verify firewall rules allow access to S3\",\n",
    "                \"Check VPC endpoint configuration if using VPC\"\n",
    "            ]\n",
    "        },\n",
    "        \"Credentials expired\": {\n",
    "            \"issue\": \"Temporary credentials have expired\",\n",
    "            \"solutions\": [\n",
    "                \"Refresh temporary credentials\",\n",
    "                \"Use long-term credentials or role assumption\"\n",
    "            ]\n",
    "        },\n",
    "        \"Signature doesn't match\": {\n",
    "            \"issue\": \"Authentication issue - signature mismatch\",\n",
    "            \"solutions\": [\n",
    "                \"Check clock synchronization on your machine\",\n",
    "                \"Verify access key and secret key are correct\",\n",
    "                \"Ensure you're using the correct region for the bucket\"\n",
    "            ]\n",
    "        },\n",
    "        \"NoSuchBucket\": {\n",
    "            \"issue\": \"Bucket doesn't exist\",\n",
    "            \"solutions\": [\n",
    "                \"Verify bucket name\",\n",
    "                \"Create the bucket if it doesn't exist\",\n",
    "                \"Check if you have permission to list buckets\"\n",
    "            ]\n",
    "        },\n",
    "        \"couldn't be established\": {\n",
    "            \"issue\": \"Endpoint configuration problem\",\n",
    "            \"solutions\": [\n",
    "                \"Check endpoint URL if using a custom endpoint\",\n",
    "                \"Verify SSL/TLS configuration if using HTTPS\",\n",
    "                \"Test connectivity to the endpoint from your environment\"\n",
    "            ]\n",
    "        },\n",
    "        \"path.style.access\": {\n",
    "            \"issue\": \"Path style access issue with S3-compatible storage\",\n",
    "            \"solutions\": [\n",
    "                \"Set spark.hadoop.fs.s3a.path.style.access to true for MinIO/custom S3\",\n",
    "                \"Check if your S3-compatible storage supports virtual-hosted style\"\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "\n",
    "    matched_issues = []\n",
    "    for key, diagnosis in diagnoses.items():\n",
    "        if key.lower() in error_message.lower():\n",
    "            matched_issues.append(diagnosis)\n",
    "\n",
    "    if not matched_issues:\n",
    "        return {\n",
    "            \"issue\": \"Unknown issue\",\n",
    "            \"solutions\": [\n",
    "                \"Enable DEBUG level logging for fs.s3a\",\n",
    "                \"Check Hadoop S3A documentation for your specific error\",\n",
    "                \"Verify all S3 configuration parameters\"\n",
    "            ]\n",
    "        }\n",
    "\n",
    "    return matched_issues[0]\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Example configurations\n",
    "    # For AWS S3\n",
    "    # aws_access_key = \"YOUR_AWS_ACCESS_KEY\"\n",
    "    # aws_secret_key = \"YOUR_AWS_SECRET_KEY\"\n",
    "    # s3_path = \"s3a://your-bucket/path/\"\n",
    "    # endpoint_url = None\n",
    "    # path_style_access = False\n",
    "\n",
    "    # For MinIO or other S3-compatible storage\n",
    "    aws_access_key = \"minioadmin\"\n",
    "    aws_secret_key = \"minioadmin\"\n",
    "    endpoint_url = \"http://minio:9000\"\n",
    "    s3_path = \"s3a://ehr/\"\n",
    "    path_style_access = True\n",
    "\n",
    "    try:\n",
    "        # Create debug session with verbose logging\n",
    "        spark = create_debug_spark_session(\n",
    "            aws_access_key=aws_access_key,\n",
    "            aws_secret_key=aws_secret_key,\n",
    "            endpoint_url=endpoint_url,\n",
    "            path_style_access=path_style_access,\n",
    "            log_level=\"DEBUG\"\n",
    "        )\n",
    "\n",
    "        # Test connection\n",
    "        connection_test = test_s3_connection(spark, s3_path)\n",
    "\n",
    "        if connection_test:\n",
    "            logger.info(\"S3 connection successful! Now testing permissions...\")\n",
    "            # Extract bucket name from path\n",
    "            bucket_name = s3_path.split('/')[2]\n",
    "            if not bucket_name:\n",
    "                # Handle empty string after split\n",
    "                bucket_name = s3_path.split('/')[3]\n",
    "\n",
    "            # Test permissions\n",
    "            permissions = verify_s3_permissions(\n",
    "                spark,\n",
    "                bucket_name,\n",
    "                test_operations=['read', 'write', 'delete']\n",
    "            )\n",
    "\n",
    "            for op, result in permissions.items():\n",
    "                logger.info(\n",
    "                    f\"Permission {op.upper()}: {'✅ Success' if result else '❌ Failed'}\")\n",
    "        else:\n",
    "            logger.error(\"S3 connection failed!\")\n",
    "\n",
    "    except Exception as e:\n",
    "        error_msg = str(e)\n",
    "        logger.error(f\"Error encountered: {error_msg}\")\n",
    "\n",
    "        # Diagnose the issue\n",
    "        diagnosis = diagnose_s3_issue(error_msg)\n",
    "\n",
    "        logger.info(\"\\n=== DIAGNOSIS ===\")\n",
    "        logger.info(f\"Issue: {diagnosis['issue']}\")\n",
    "        logger.info(\"Potential solutions:\")\n",
    "        for i, solution in enumerate(diagnosis['solutions'], 1):\n",
    "            logger.info(f\"  {i}. {solution}\")\n",
    "        logger.info(\"=================\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
