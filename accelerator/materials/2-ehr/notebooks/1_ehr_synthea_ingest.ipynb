{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EHR Data Loader\n",
    "\n",
    "This notebook loads EHR (Electronic Health Record) data from S3 into Delta Lake tables. It handles the configuration of a Spark session with Delta Lake integration and manages the schema inference and loading of various healthcare data files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DateType, TimestampType, DoubleType\n",
    "import os\n",
    "from datetime import datetime\n",
    "from pyspark.sql import functions as F\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "import boto3\n",
    "from botocore import UNSIGNED\n",
    "from botocore.config import Config\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Spark Session\n",
    "\n",
    "This function creates a Spark session configured for Delta Lake with S3 access."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_spark_session(app_name=\"EHR Data Loader\", aws_access_key=None, aws_secret_key=None):\n",
    "    \"\"\"\n",
    "    Create and return a Spark session configured for Delta Lake with S3 access.\n",
    "    \"\"\"\n",
    "    # Stop any existing Spark session\n",
    "    try:\n",
    "        SparkSession.builder.getOrCreate().stop()\n",
    "        print(\"Stopped existing Spark session\")\n",
    "    except:\n",
    "        print(\"No existing Spark session to stop\")\n",
    "\n",
    "    # Define the base directory for JAR files\n",
    "    jars_home = '/home/developer/projects/delta-spark-handbook/delta-jars'\n",
    "    if not os.path.exists(jars_home):\n",
    "        raise Exception(f\"JAR directory not found at: {jars_home}\")\n",
    "\n",
    "    # Required JARs\n",
    "    jar_locations = [\n",
    "        f\"{jars_home}/delta-spark_2.12-3.3.0.jar\",\n",
    "        f\"{jars_home}/delta-storage-3.3.0.jar\",\n",
    "        f\"{jars_home}/hadoop-aws-3.3.4.jar\",\n",
    "        f\"{jars_home}/bundle-2.24.12.jar\",\n",
    "        # Add Hadoop client JARs\n",
    "        f\"{jars_home}/hadoop-client-runtime-3.3.4.jar\",\n",
    "        f\"{jars_home}/hadoop-client-api-3.3.4.jar\"\n",
    "    ]\n",
    "\n",
    "    # Verify all JARs exist\n",
    "    for jar in jar_locations:\n",
    "        if not os.path.exists(jar):\n",
    "            raise Exception(f\"Required JAR not found: {jar}\")\n",
    "\n",
    "    # Create Hadoop configuration directory\n",
    "    hadoop_conf_dir = \"hadoop-conf\"\n",
    "    os.makedirs(hadoop_conf_dir, exist_ok=True)\n",
    "\n",
    "    # Write core-site.xml with S3 configuration\n",
    "    core_site_xml = f\"\"\"<?xml version=\"1.0\"?>\n",
    "<configuration>\n",
    "    <property>\n",
    "        <name>fs.s3a.impl</name>\n",
    "        <value>org.apache.hadoop.fs.s3a.S3AFileSystem</value>\n",
    "    </property>\n",
    "    <property>\n",
    "        <name>fs.s3a.endpoint</name>\n",
    "        <value>http://localhost:9000</value>\n",
    "    </property>\n",
    "    <property>\n",
    "        <name>fs.s3a.access.key</name>\n",
    "        <value>{aws_access_key or 'minioadmin'}</value>\n",
    "    </property>\n",
    "    <property>\n",
    "        <name>fs.s3a.secret.key</name>\n",
    "        <value>{aws_secret_key or 'minioadmin'}</value>\n",
    "    </property>\n",
    "    <property>\n",
    "        <name>fs.s3a.path.style.access</name>\n",
    "        <value>true</value>\n",
    "    </property>\n",
    "    <property>\n",
    "        <name>fs.s3a.connection.ssl.enabled</name>\n",
    "        <value>false</value>\n",
    "    </property>\n",
    "</configuration>\"\"\"\n",
    "\n",
    "    with open(f\"{hadoop_conf_dir}/core-site.xml\", \"w\") as f:\n",
    "        f.write(core_site_xml)\n",
    "\n",
    "    # Set environment variables\n",
    "    os.environ[\"HADOOP_CONF_DIR\"] = os.path.abspath(hadoop_conf_dir)\n",
    "    os.environ[\"SPARK_HOME\"] = \"/opt/spark\"\n",
    "    os.environ[\"SPARK_CLASSPATH\"] = \":\".join(\n",
    "        [os.path.abspath(jar) for jar in jar_locations])\n",
    "    os.environ[\"HADOOP_CLASSPATH\"] = os.environ[\"SPARK_CLASSPATH\"]\n",
    "\n",
    "    # Create Spark session with comprehensive configuration\n",
    "    builder = (SparkSession.builder\n",
    "               .appName(app_name)\n",
    "               .master(\"local[*]\")\n",
    "               #.master(\"spark://localhost:7077\") \n",
    "               .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "               .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "               .config(\"spark.sql.catalogImplementation\", \"hive\")\n",
    "               .config(\"spark.sql.adaptive.enabled\", \"true\")\n",
    "               .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\")\n",
    "               .config(\"spark.sql.adaptive.coalescePartitions.skewedPartitionFactor\", \"2\")\n",
    "               .config(\"spark.sql.adaptive.coalescePartitions.skewedPartitionThresholdInBytes\", \"10485760\")\n",
    "               .config(\"spark.sql.adaptive.coalescePartitions.skewedPartitionFactor\", \"2\")\n",
    "               \n",
    "               .config(\"spark.hadoop.javax.jdo.option.ConnectionURL\", \"jdbc:postgresql://localhost:5432/metastore_db\")\n",
    "               .config(\"spark.hadoop.javax.jdo.option.ConnectionDriverName\", \"org.postgresql.Driver\")\n",
    "               .config(\"spark.hadoop.javax.jdo.option.ConnectionUserName\", \"admin\")\n",
    "               .config(\"spark.hadoop.javax.jdo.option.ConnectionPassword\", \"admin\")\n",
    "               .config(\"spark.hadoop.hive.metastore.uris\", \"thrift://localhost:9083\")\n",
    "               .config(\"spark.sql.warehouse.dir\", \"s3a://wba/warehouse\")\n",
    "               .config(\"spark.driver.extraClassPath\", \":\".join([os.path.abspath(jar) for jar in jar_locations]))\n",
    "               .config(\"spark.executor.extraClassPath\", \":\".join([os.path.abspath(jar) for jar in jar_locations]))\n",
    "               .config(\"spark.jars.excludes\", \"org.slf4j:slf4j-log4j12,org.slf4j:slf4j-reload4j,org.slf4j:log4j-slf4j-impl\")\n",
    "               .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "               .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://localhost:9000\")\n",
    "               .config(\"spark.hadoop.fs.s3a.access.key\", aws_access_key or \"minioadmin\")\n",
    "               .config(\"spark.hadoop.fs.s3a.secret.key\", aws_secret_key or \"minioadmin\")\n",
    "               .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\")\n",
    "               .config(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"false\")\n",
    "               .config(\"spark.hadoop.fs.s3a.fast.upload\", \"true\")\n",
    "               .config(\"spark.hadoop.fs.s3a.multipart.size\", \"5242880\")\n",
    "               .config(\"spark.hadoop.fs.s3a.block.size\", \"5242880\")\n",
    "               .config(\"spark.hadoop.fs.s3a.multipart.threshold\", \"5242880\")\n",
    "               .config(\"spark.hadoop.fs.s3a.threads.core\", \"10\")\n",
    "               .config(\"spark.hadoop.fs.s3a.threads.max\", \"20\")\n",
    "               .config(\"spark.hadoop.fs.s3a.max.total.tasks\", \"50\")\n",
    "               .config(\"spark.hadoop.fs.s3a.connection.timeout\", \"60000\")\n",
    "               .config(\"spark.hadoop.fs.s3a.connection.establish.timeout\", \"60000\")\n",
    "               .config(\"spark.hadoop.fs.s3a.socket.timeout\", \"60000\")\n",
    "               .config(\"spark.hadoop.fs.s3a.connection.maximum\", \"50\")\n",
    "               .config(\"spark.hadoop.fs.s3a.fast.upload.buffer\", \"bytebuffer\")\n",
    "               .config(\"spark.hadoop.fs.s3a.fast.upload.active.blocks\", \"2\")\n",
    "               .config(\"spark.hadoop.fs.s3a.multipart.purge\", \"false\")\n",
    "               .config(\"spark.hadoop.fs.s3a.multipart.purge.age\", \"86400000\")\n",
    "               .config(\"spark.hadoop.fs.s3a.retry.limit\", \"10\")\n",
    "               .config(\"spark.hadoop.fs.s3a.retry.interval\", \"1000\")\n",
    "               .config(\"spark.hadoop.fs.s3a.attempts.maximum\", \"10\")\n",
    "               .config(\"spark.hadoop.fs.s3a.connection.request.timeout\", \"60000\")\n",
    "               .config(\"spark.hadoop.fs.s3a.threads.keepalivetime\", \"60000\")\n",
    "               .config(\"spark.driver.memory\",\"2g\")\n",
    "                .config(\"spark.executor.memory\",\"2g\")\n",
    "                .config(\"spark.executor.cores\",\"1\")\n",
    "                .config(\"spark.executor.instances\",\"1\")\n",
    "                .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "                .config(\"spark.hadoop.hive.metastore.warehouse.dir\", \"s3a://wba/warehouse\")\n",
    "                .config(\"spark.delta.logStore.class\", \"org.apache.spark.sql.delta.storage.S3SingleDriverLogStore\")\n",
    "                .config(\"spark.sql.hive.convertMetastoreParquet\", \"false\")\n",
    "                .config(\"spark.sql.hive.metastorePartitionPruning\", \"true\")\n",
    "               .enableHiveSupport())\n",
    "\n",
    "    return builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Schema Inference\n",
    "\n",
    "This function infers the schema from a CSV file by reading a sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_schema_from_file(spark, file_path, sample_size=1000):\n",
    "    \"\"\"\n",
    "    Infer schema from a CSV file by reading a sample.\n",
    "    \"\"\"\n",
    "    # Read a sample of the CSV file to infer schema\n",
    "    sample_df = spark.read.option(\"header\", \"true\").option(\n",
    "        \"inferSchema\", \"true\").csv(file_path).limit(sample_size)\n",
    "    return sample_df.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load File to Delta\n",
    "\n",
    "This function loads a CSV file into a Delta table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_file_to_delta(spark, file_path, database_name, table_name, mode=\"overwrite\", partition_by=None):\n",
    "    \"\"\"\n",
    "    Load a CSV file into a Delta table using a database name.\n",
    "\n",
    "\n",
    "    Args:\n",
    "        spark: SparkSession object.\n",
    "        file_path: Path to the CSV file (can be S3 URI).\n",
    "        database_name: Name of the database to store the Delta table.\n",
    "        table_name: Name of the table to create.\n",
    "        mode: Write mode (overwrite, append, etc.).\n",
    "        partition_by: Column(s) to partition the data by.\n",
    "    \"\"\"\n",
    "\n",
    "    full_table_name = \"unknown\"\n",
    "\n",
    "    try:\n",
    "        # Infer schema from file\n",
    "        schema = infer_schema_from_file(spark, file_path)\n",
    "\n",
    "        # Read the CSV file with the inferred schema\n",
    "        df = spark.read.option(\"header\", \"true\").schema(schema).csv(file_path)\n",
    "\n",
    "        # Convert all column names to lowercase\n",
    "        for col_name in df.columns:\n",
    "            df = df.withColumnRenamed(col_name, col_name.lower())\n",
    "\n",
    "        if \"start\" in df.columns:\n",
    "            df = df.withColumn(\"start_date\", F.to_date(\n",
    "                F.col(\"start\"), \"yyyy-MM-dd\"))\n",
    "\n",
    "            df = df.withColumn(\"year\", F.year(F.col(\"start_date\")))\n",
    "            df = df.withColumn(\"month\", F.month(F.col(\"start_date\")))\n",
    "\n",
    "        # Ensure DATE is properly converted to a date type\n",
    "        if \"date\" in df.columns:\n",
    "            df = df.withColumn(\"date\", F.to_date(F.col(\"date\"), \"yyyy-MM-dd\"))\n",
    "            df = df.withColumn(\"year\", F.year(F.col(\"date\")))\n",
    "            df = df.withColumn(\"month\", F.month(F.col(\"date\")))\n",
    "\n",
    "        # Add metadata columns\n",
    "        df = df.withColumn(\"ingestion_timestamp\", F.current_timestamp())\n",
    "        df = df.withColumn(\"source_file\", F.lit(file_path.split('/')[-1]))\n",
    "\n",
    "        # Define the full table name\n",
    "        full_table_name = f\"{database_name}.{table_name}\"\n",
    "\n",
    "        # Write to Delta Lake using saveAsTable\n",
    "        writer = df.write.format(\"delta\").mode(\n",
    "            mode).option(\"overwriteSchema\", \"true\").option(\"delta.compatibility.symlinkFormatManifest.enabled\", \"false\")\n",
    "\n",
    "        if partition_by:\n",
    "            writer = writer.partitionBy(partition_by)\n",
    "\n",
    "        # Get warehouse dir from Spark config\n",
    "        warehouse_dir = spark.conf.get(\"spark.sql.warehouse.dir\").rstrip(\"/\")\n",
    "\n",
    "        # Format: {warehouse_dir}/{database}.db/{table}\n",
    "        table_path = f\"{warehouse_dir}/{database_name}.db/{table_name}\"\n",
    "        table_full_name = f\"{database_name}.{table_name}\"\n",
    "        \n",
    "        print(f\"üìÅ Calculated table path: {table_path}\")\n",
    "\n",
    "         \n",
    "        #writer.saveAsTable(table_full_name)\n",
    "        writer.save(table_path)\n",
    "\n",
    "\n",
    "        # Then create/refresh the table definition pointing to that location\n",
    "        spark.sql(f\"\"\"\n",
    "         CREATE TABLE IF NOT EXISTS {database_name}.{table_name}\n",
    "            USING DELTA\n",
    "             LOCATION '{table_path}'\n",
    "         \"\"\")\n",
    "\n",
    "        print(f\"Successfully loaded {file_path} into table {full_table_name}\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(\n",
    "            f\"Error loading {file_path} into table {full_table_name}: {str(e)}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List S3 Files\n",
    "\n",
    "This function lists files in an S3 directory with a specific extension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_s3_files(spark, s3_dir_path, file_extension=\".csv\"):\n",
    "    \"\"\"\n",
    "    List files in an S3 directory with a specific extension.\n",
    "\n",
    "    Args:\n",
    "        spark: SparkSession object\n",
    "        s3_dir_path: S3 directory path (e.g., s3a://bucket-name/path/)\n",
    "        file_extension: File extension to filter by\n",
    "\n",
    "    Returns:\n",
    "        List of file paths\n",
    "    \"\"\"\n",
    "    # Create a DataFrame representing the files\n",
    "    files_df = spark.read.format(\"binaryFile\").load(s3_dir_path)\n",
    "\n",
    "    # Filter files by extension and collect their paths\n",
    "    csv_files = files_df.filter(files_df.path.endswith(\n",
    "        file_extension)).select(\"path\").collect()\n",
    "\n",
    "    return [row.path for row in csv_files]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ingest EHR Data  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ingest_ehr_data(ehr_s3_path, database_name, aws_access_key=None, aws_secret_key=None):\n",
    "    \"\"\"\n",
    "    Load all EHR CSV files from S3 into Delta tables using a database.\n",
    "\n",
    "    Args:\n",
    "        ehr_s3_path: S3 URI to directory containing EHR CSV files.\n",
    "        database_name: Database name where tables will be stored.\n",
    "        aws_access_key: AWS access key (optional).\n",
    "        aws_secret_key: AWS secret key (optional).\n",
    "    \"\"\"\n",
    "    # Download Files\n",
    "    vocab_s3_path = \"s3://hls-eng-data-public/data/synthea/\"\n",
    "\n",
    "    parsed = urlparse(vocab_s3_path)\n",
    "    bucket = parsed.netloc\n",
    "    prefix = parsed.path.lstrip('/').rstrip('/')\n",
    "\n",
    "    s3 = boto3.client('s3', config=Config(signature_version=UNSIGNED))\n",
    "    response = s3.list_objects_v2(Bucket=bucket, Prefix=prefix)\n",
    "    available_keys = [obj['Key'] for obj in response.get('Contents', [])]\n",
    "\n",
    "    start_time = time.time()\n",
    "    for key in available_keys:\n",
    "        print(\"‚è≥ Downloading from S3...\")\n",
    "        response = s3.get_object(Bucket=bucket, Key=key)\n",
    "        csv_content = response['Body'].read()\n",
    "        download_time = time.time()\n",
    "        print(f\"‚úÖ Download complete in {download_time - start_time:.2f}s\")\n",
    "\n",
    "        # Save raw .csv.gz file to MinIO\n",
    "        raw_key = key\n",
    "        print(f\"üì§ Saving raw .csv to MinIO: {raw_key}...\")\n",
    "        s3_minio = boto3.client(\n",
    "            \"s3\",\n",
    "            endpoint_url=\"http://localhost:9000\",  # adjust to your MinIO endpoint\n",
    "            aws_access_key_id=\"minioadmin\",\n",
    "            aws_secret_access_key=\"minioadmin\"\n",
    "        )\n",
    "        s3_minio.put_object(\n",
    "            Bucket=f\"{database_name}\",\n",
    "            Key=raw_key,\n",
    "            Body=csv_content\n",
    "        )\n",
    "        print(\"‚úÖ Raw .csv saved to MinIO.\")\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load EHR Data to Delta\n",
    "\n",
    "This function loads all EHR CSV files from S3 into Delta tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_ehr_data_to_delta(ehr_s3_path, database_name, aws_access_key=None, aws_secret_key=None):\n",
    "    \"\"\"\n",
    "    Load all EHR CSV files from S3 into Delta tables using a database.\n",
    "\n",
    "    Args:\n",
    "        ehr_s3_path: S3 URI to directory containing EHR CSV files.\n",
    "        database_name: Database name where tables will be stored.\n",
    "        aws_access_key: AWS access key (optional).\n",
    "        aws_secret_key: AWS secret key (optional).\n",
    "    \"\"\"\n",
    "    # Create Spark session\n",
    "    spark = create_spark_session(\n",
    "        aws_access_key=aws_access_key, aws_secret_key=aws_secret_key)\n",
    "\n",
    "    # Define partition strategies for specific tables\n",
    "    partition_config = {\n",
    "        \"patients.csv\": [\"gender\"],\n",
    "        \"encounters.csv\": [\"year\", \"month\"],\n",
    "        \"medications.csv\": [\"year\"],\n",
    "        \"observations.csv\": [\"year\", \"month\"],\n",
    "        \"procedures.csv\": [\"year\"],\n",
    "        \"imaging_studies.csv\": [\"year\"],\n",
    "        \"conditions.csv\": [\"year\"],\n",
    "        \"immunizations.csv\": [\"year\"],\n",
    "        \"allergies.csv\": None,\n",
    "        \"careplans.csv\": None,\n",
    "        \"organizations.csv\": None,\n",
    "        \"providers.csv\": None,\n",
    "        \"devices.csv\": None,\n",
    "        \"supplies.csv\": None,\n",
    "        \"payer_transitions.csv\": None,\n",
    "        \"payers.csv\": None\n",
    "    }\n",
    "    \n",
    "    # List all CSV files in the S3 directory\n",
    "    s3_files = list_s3_files(spark, ehr_s3_path, \".csv\")\n",
    "    \n",
    "    # Process each file\n",
    "    results = {}\n",
    "\n",
    "    spark.sql(f\"DROP DATABASE IF EXISTS {database_name} CASCADE\")\n",
    "    \n",
    "    # Ensure the database exists\n",
    "    spark.sql(\n",
    "        f\"CREATE DATABASE IF NOT EXISTS {database_name} \")\n",
    "    \n",
    "    for file_path in s3_files:\n",
    "        file_name = file_path.split('/')[-1]\n",
    "        if file_name.endswith(\".csv\"):\n",
    "            table_name = file_name.split('.')[0]\n",
    "\n",
    "            # Get partition columns if defined\n",
    "            partition_by = partition_config.get(file_name)\n",
    "\n",
    "            # Load file to Delta table in the database\n",
    "            success = load_file_to_delta(\n",
    "                spark, file_path, database_name, table_name, partition_by=partition_by)\n",
    "            results[file_name] = success\n",
    "\n",
    "    # Print summary\n",
    "    print(\"\\nSummary of Delta Lake table loading:\")\n",
    "    for file_name, success in results.items():\n",
    "        status = \"Success\" if success else \"Failed\"\n",
    "        print(f\"{file_name}: {status}\")\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Usage\n",
    "\n",
    "Run the following cell to load EHR data to Delta tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "your 131072x1 screen size is bogus. expect trouble\n",
      "25/04/03 18:40:56 WARN Utils: Your hostname, JBLAPTOPW11 resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)\n",
      "25/04/03 18:40:56 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/04/03 18:40:57 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopped existing Spark session\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/03 18:40:59 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n",
      "25/04/03 18:40:59 WARN VersionInfoUtils: The AWS SDK for Java 1.x entered maintenance mode starting July 31, 2024 and will reach end of support on December 31, 2025. For more information, see https://aws.amazon.com/blogs/developer/the-aws-sdk-for-java-1-x-is-in-maintenance-mode-effective-july-31-2024/\n",
      "You can print where on the file system the AWS SDK for Java 1.x core runtime is located by setting the AWS_JAVA_V1_PRINT_LOCATION environment variable or aws.java.v1.printLocation system property to 'true'.\n",
      "This message can be disabled by setting the AWS_JAVA_V1_DISABLE_DEPRECATION_ANNOUNCEMENT environment variable or aws.java.v1.disableDeprecationAnnouncement system property to 'true'.\n",
      "The AWS SDK for Java 1.x is being used here:\n",
      "at java.base/java.lang.Thread.getStackTrace(Thread.java:1619)\n",
      "at com.amazonaws.util.VersionInfoUtils.printDeprecationAnnouncement(VersionInfoUtils.java:81)\n",
      "at com.amazonaws.util.VersionInfoUtils.<clinit>(VersionInfoUtils.java:59)\n",
      "at com.amazonaws.internal.EC2ResourceFetcher.<clinit>(EC2ResourceFetcher.java:44)\n",
      "at com.amazonaws.auth.InstanceMetadataServiceCredentialsFetcher.<init>(InstanceMetadataServiceCredentialsFetcher.java:38)\n",
      "at com.amazonaws.auth.InstanceProfileCredentialsProvider.<init>(InstanceProfileCredentialsProvider.java:111)\n",
      "at com.amazonaws.auth.InstanceProfileCredentialsProvider.<init>(InstanceProfileCredentialsProvider.java:91)\n",
      "at com.amazonaws.auth.InstanceProfileCredentialsProvider.<init>(InstanceProfileCredentialsProvider.java:75)\n",
      "at com.amazonaws.auth.InstanceProfileCredentialsProvider.<clinit>(InstanceProfileCredentialsProvider.java:58)\n",
      "at com.amazonaws.auth.EC2ContainerCredentialsProviderWrapper.initializeProvider(EC2ContainerCredentialsProviderWrapper.java:66)\n",
      "at com.amazonaws.auth.EC2ContainerCredentialsProviderWrapper.<init>(EC2ContainerCredentialsProviderWrapper.java:55)\n",
      "at org.apache.hadoop.fs.s3a.auth.IAMInstanceCredentialsProvider.<init>(IAMInstanceCredentialsProvider.java:49)\n",
      "at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
      "at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)\n",
      "at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n",
      "at java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)\n",
      "at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)\n",
      "at org.apache.hadoop.fs.s3a.S3AUtils.createAWSCredentialProvider(S3AUtils.java:766)\n",
      "at org.apache.hadoop.fs.s3a.S3AUtils.buildAWSProviderList(S3AUtils.java:698)\n",
      "at org.apache.hadoop.fs.s3a.S3AUtils.createAWSCredentialProviderSet(S3AUtils.java:631)\n",
      "at org.apache.hadoop.fs.s3a.S3AFileSystem.bindAWSClient(S3AFileSystem.java:877)\n",
      "at org.apache.hadoop.fs.s3a.S3AFileSystem.initialize(S3AFileSystem.java:534)\n",
      "at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469)\n",
      "at org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)\n",
      "at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)\n",
      "at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)\n",
      "at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)\n",
      "at org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)\n",
      "at org.apache.spark.sql.internal.SharedState$.qualifyWarehousePath(SharedState.scala:288)\n",
      "at org.apache.spark.sql.internal.SharedState.liftedTree1$1(SharedState.scala:80)\n",
      "at org.apache.spark.sql.internal.SharedState.<init>(SharedState.scala:79)\n",
      "at org.apache.spark.sql.SparkSession.$anonfun$sharedState$1(SparkSession.scala:143)\n",
      "at scala.Option.getOrElse(Option.scala:189)\n",
      "at org.apache.spark.sql.SparkSession.sharedState$lzycompute(SparkSession.scala:143)\n",
      "at org.apache.spark.sql.SparkSession.sharedState(SparkSession.scala:142)\n",
      "at org.apache.spark.sql.SparkSession.$anonfun$sessionState$2(SparkSession.scala:162)\n",
      "at scala.Option.getOrElse(Option.scala:189)\n",
      "at org.apache.spark.sql.SparkSession.sessionState$lzycompute(SparkSession.scala:160)\n",
      "at org.apache.spark.sql.SparkSession.sessionState(SparkSession.scala:157)\n",
      "at org.apache.spark.sql.DataFrameReader.<init>(DataFrameReader.scala:699)\n",
      "at org.apache.spark.sql.SparkSession.read(SparkSession.scala:783)\n",
      "at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n",
      "at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "at java.base/java.lang.reflect.Method.invoke(Method.java:569)\n",
      "at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "at py4j.Gateway.invoke(Gateway.java:282)\n",
      "at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "at py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "at py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "at java.base/java.lang.Thread.run(Thread.java:840)\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ Calculated table path: s3a://wba/warehouse/ehr.db/claims_transactions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/03 18:41:21 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "ERROR:root:Exception while sending command.                        (8 + 8) / 50]\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/developer/.local/lib/python3.10/site-packages/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "RuntimeError: reentrant call inside <_io.BufferedReader name=74>\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/developer/.local/lib/python3.10/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/home/developer/.local/lib/python3.10/site-packages/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n",
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/developer/.local/lib/python3.10/site-packages/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "  File \"/usr/lib/python3.10/socket.py\", line 705, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "  File \"/opt/spark/python/pyspark/context.py\", line 381, in signal_handler\n",
      "    self.cancelAllJobs()\n",
      "  File \"/opt/spark/python/pyspark/context.py\", line 2446, in cancelAllJobs\n",
      "    self._jsc.sc().cancelAllJobs()\n",
      "  File \"/home/developer/.local/lib/python3.10/site-packages/py4j/java_gateway.py\", line 1322, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/opt/spark/python/pyspark/errors/exceptions/captured.py\", line 179, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/home/developer/.local/lib/python3.10/site-packages/py4j/protocol.py\", line 334, in get_return_value\n",
      "    raise Py4JError(\n",
      "py4j.protocol.Py4JError: An error occurred while calling o138.sc\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/developer/.local/lib/python3.10/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/home/developer/.local/lib/python3.10/site-packages/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error loading s3a://ehr/data/synthea/ADHD/csv/claims_transactions.csv into table ehr.claims_transactions: An error occurred while calling o231.save\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ Calculated table path: s3a://wba/warehouse/ehr.db/claims\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/03 18:41:40 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "[Stage 11:>                 (0 + 8) / 8][Stage 12:>                 (0 + 0) / 2]\r"
     ]
    }
   ],
   "source": [
    "# S3 paths\n",
    "database_name = \"ehr\"\n",
    "\n",
    "# Option 1: Using AWS credentials\n",
    "aws_access_key = \"minioadmin\"  # Replace with your key or use None\n",
    "aws_secret_key = \"minioadmin\"  # Replace with your key or use None\n",
    "\n",
    "# Update S3 path to use s3a protocol\n",
    "ehr_s3_path = \"s3a://ehr/data/synthea/ADHD/csv\"\n",
    "\n",
    "# ingest_ehr_data(ehr_s3_path, database_name,\n",
    "#                        aws_access_key, aws_secret_key)\n",
    "\n",
    "load_ehr_data_to_delta(ehr_s3_path, database_name,\n",
    "                        aws_access_key, aws_secret_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
