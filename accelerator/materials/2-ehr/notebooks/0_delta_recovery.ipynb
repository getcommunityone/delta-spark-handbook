{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Table Recovery from MinIO S3\n",
    "\n",
    "This notebook provides functions to scan a Spark database directory in MinIO S3 and recreate table definitions that point to existing data. This is useful for scenarios where:\n",
    "\n",
    "1. Your Spark metastore was lost or corrupted\n",
    "2. You need to access existing data from a different Spark cluster\n",
    "3. You need to recover from a Delta Lake table metadata loss\n",
    "\n",
    "The tool will preserve all existing data and simply recreate the table definitions in the Spark catalog."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from urllib.parse import urlparse\n",
    "import os\n",
    "import json\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "your 131072x1 screen size is bogus. expect trouble\n",
      "25/04/06 15:21:42 WARN Utils: Your hostname, JBLAPTOPW11 resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)\n",
      "25/04/06 15:21:42 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/04/06 15:21:44 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/04/06 15:21:46 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopped existing Spark session\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/06 15:21:49 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Create a Spark session with Delta Lake and S3 support\n",
    "def create_spark_session(app_name=\"Table Recovery\", aws_access_key=None, aws_secret_key=None):\n",
    "    \"\"\"Create a Spark session configured for Delta Lake with S3 access.\"\"\"\n",
    "    aws_access_key = aws_access_key or \"minioadmin\"\n",
    "    aws_secret_key = aws_secret_key or \"minioadmin\"\n",
    "    \n",
    "    # Stop any existing session\n",
    "    try:\n",
    "        SparkSession.builder.getOrCreate().stop()\n",
    "        print(\"Stopped existing Spark session\")\n",
    "    except:\n",
    "        print(\"No existing Spark session to stop\")\n",
    "    \n",
    "    # Create new session\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(app_name) \\\n",
    "        .master(\"local[*]\") \\\n",
    "        .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "        .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://localhost:9000\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.access.key\", aws_access_key) \\\n",
    "        .config(\"spark.hadoop.fs.s3a.secret.key\", aws_secret_key) \\\n",
    "        .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"false\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.aws.credentials.provider\", \"org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider\") \\\n",
    "        .config(\"spark.hadoop.javax.jdo.option.ConnectionURL\", \"jdbc:postgresql://localhost:5432/metastore_db\") \\\n",
    "        .config(\"spark.hadoop.javax.jdo.option.ConnectionDriverName\", \"org.postgresql.Driver\") \\\n",
    "        .config(\"spark.hadoop.javax.jdo.option.ConnectionUserName\", \"admin\") \\\n",
    "        .config(\"spark.hadoop.javax.jdo.option.ConnectionPassword\", \"admin\") \\\n",
    "        .config(\"spark.sql.warehouse.dir\", \"s3a://wba/warehouse\") \\\n",
    "               .config(\"spark.hadoop.javax.jdo.option.ConnectionURL\", \"jdbc:postgresql://localhost:5432/metastore_db\") \\\n",
    "               .config(\"spark.hadoop.javax.jdo.option.ConnectionDriverName\", \"org.postgresql.Driver\") \\\n",
    "               .config(\"spark.hadoop.javax.jdo.option.ConnectionUserName\", \"admin\") \\\n",
    "               .config(\"spark.hadoop.javax.jdo.option.ConnectionPassword\", \"admin\") \\\n",
    "               .config(\"spark.hadoop.hive.metastore.uris\", \"thrift://localhost:9083\") \\\n",
    "               .config(\"spark.sql.warehouse.dir\", \"s3a://wba/warehouse\") \\\n",
    "               .config(\"spark.jars.excludes\", \"org.slf4j:slf4j-log4j12,org.slf4j:slf4j-reload4j,org.slf4j:log4j-slf4j-impl\") \\\n",
    "               .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "               .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://localhost:9000\") \\\n",
    "               .config(\"spark.hadoop.fs.s3a.access.key\", aws_access_key or \"minioadmin\") \\\n",
    "               .config(\"spark.hadoop.fs.s3a.secret.key\", aws_secret_key or \"minioadmin\") \\\n",
    "               .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "               .config(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"false\") \\\n",
    "               .config(\"spark.hadoop.fs.s3a.fast.upload\", \"true\") \\\n",
    "               .config(\"spark.hadoop.fs.s3a.multipart.size\", \"5242880\") \\\n",
    "               .config(\"spark.hadoop.fs.s3a.block.size\", \"5242880\") \\\n",
    "               .config(\"spark.hadoop.fs.s3a.multipart.threshold\", \"5242880\") \\\n",
    "               .config(\"spark.hadoop.fs.s3a.threads.core\", \"10\") \\\n",
    "               .config(\"spark.hadoop.fs.s3a.threads.max\", \"20\") \\\n",
    "               .config(\"spark.hadoop.fs.s3a.max.total.tasks\", \"50\") \\\n",
    "               .config(\"spark.hadoop.fs.s3a.connection.timeout\", \"60000\") \\\n",
    "               .config(\"spark.hadoop.fs.s3a.connection.establish.timeout\", \"60000\") \\\n",
    "               .config(\"spark.hadoop.fs.s3a.socket.timeout\", \"60000\") \\\n",
    "               .config(\"spark.hadoop.fs.s3a.connection.maximum\", \"50\") \\\n",
    "               .config(\"spark.hadoop.fs.s3a.fast.upload.buffer\", \"bytebuffer\") \\\n",
    "               .config(\"spark.hadoop.fs.s3a.fast.upload.active.blocks\", \"2\") \\\n",
    "               .config(\"spark.hadoop.fs.s3a.multipart.purge\", \"false\") \\\n",
    "               .config(\"spark.hadoop.fs.s3a.multipart.purge.age\", \"86400000\") \\\n",
    "               .config(\"spark.hadoop.fs.s3a.retry.limit\", \"10\") \\\n",
    "               .config(\"spark.hadoop.fs.s3a.retry.interval\", \"1000\") \\\n",
    "               .config(\"spark.hadoop.fs.s3a.attempts.maximum\", \"10\") \\\n",
    "               .config(\"spark.hadoop.fs.s3a.connection.request.timeout\", \"60000\") \\\n",
    "               .config(\"spark.hadoop.fs.s3a.threads.keepalivetime\", \"60000\") \\\n",
    "                .config(\"spark.executor.cores\",\"1\") \\\n",
    "                .config(\"spark.executor.instances\",\"1\") \\\n",
    "                .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "                .config(\"spark.hadoop.hive.metastore.warehouse.dir\", \"s3a://wba/warehouse\") \\\n",
    "                .config(\"spark.delta.logStore.class\", \"org.apache.spark.sql.delta.storage.S3SingleDriverLogStore\") \\\n",
    "                .config(\"spark.sql.hive.convertMetastoreParquet\", \"false\") \\\n",
    "                .config(\"spark.sql.hive.metastorePartitionPruning\", \"true\")  \\\n",
    "                .enableHiveSupport()  \\\n",
    "                .getOrCreate()\n",
    "    \n",
    "    return spark\n",
    "\n",
    "# Create the session\n",
    "spark = create_spark_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table Recovery Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recreate_tables_from_minio(spark, warehouse_dir, database_name, aws_access_key=None, aws_secret_key=None):\n",
    "    \"\"\"\n",
    "    Scans a Spark database directory in MinIO S3 and generates SQL script\n",
    "    to recreate table definitions pointing to the existing data.\n",
    "    \n",
    "    Args:\n",
    "        spark: SparkSession object\n",
    "        warehouse_dir: S3 path to Spark warehouse (e.g., \"s3a://wba/warehouse\")\n",
    "        database_name: Database name to scan\n",
    "        aws_access_key: MinIO access key (defaults to \"minioadmin\")\n",
    "        aws_secret_key: MinIO secret key (defaults to \"minioadmin\")\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (sql_script, results_dict)\n",
    "        - sql_script: Complete SQL script as a string\n",
    "        - results_dict: Dictionary mapping table names to status\n",
    "    \"\"\"\n",
    "    aws_access_key = aws_access_key or \"minioadmin\"\n",
    "    aws_secret_key = aws_secret_key or \"minioadmin\"\n",
    "    \n",
    "    print(f\"Scanning database {database_name} in warehouse {warehouse_dir}\")\n",
    "    \n",
    "    # Initialize sql script string\n",
    "    sql_script = f\"-- SQL Script to recreate tables from MinIO warehouse: {warehouse_dir}\\n\"\n",
    "    sql_script += f\"-- Database: {database_name}\\n\\n\"\n",
    "    \n",
    "    # Add CREATE DATABASE statement\n",
    "    sql_script += f\"CREATE DATABASE IF NOT EXISTS {database_name};\\n\\n\"\n",
    "    \n",
    "    # Parse the S3 URI to get bucket and prefix\n",
    "    parsed_uri = urlparse(warehouse_dir)\n",
    "    bucket_name = parsed_uri.netloc\n",
    "    prefix = parsed_uri.path.lstrip('/')\n",
    "    \n",
    "    if prefix and not prefix.endswith('/'):\n",
    "        prefix += '/'\n",
    "    \n",
    "    # Full path to the database directory\n",
    "    db_prefix = f\"{prefix}{database_name}.db/\"\n",
    "    \n",
    "    # Initialize S3 client\n",
    "    s3_client = boto3.client(\n",
    "        \"s3\",\n",
    "        endpoint_url=\"http://localhost:9000\",  # adjust to your MinIO endpoint\n",
    "        aws_access_key_id=aws_access_key,\n",
    "        aws_secret_access_key=aws_secret_key\n",
    "    )\n",
    "    \n",
    "    # Get existing tables in Spark catalog\n",
    "    try:\n",
    "        existing_tables = [table.name for table in spark.catalog.listTables(database_name)]\n",
    "        print(f\"Found {len(existing_tables)} existing tables in Spark catalog: {existing_tables}\")\n",
    "    except:\n",
    "        # Database might not exist\n",
    "        existing_tables = []\n",
    "        print(f\"No existing tables found in database {database_name}\")\n",
    "    \n",
    "    # List all folders in the database directory (each folder is a table)\n",
    "    try:\n",
    "        print(f\"Listing objects in bucket '{bucket_name}' with prefix '{db_prefix}'\")\n",
    "        paginator = s3_client.get_paginator('list_objects_v2')\n",
    "        page_iterator = paginator.paginate(Bucket=bucket_name, Prefix=db_prefix, Delimiter='/')\n",
    "        \n",
    "        table_folders = []\n",
    "        for page in page_iterator:\n",
    "            # Process common prefixes (directories)\n",
    "            if 'CommonPrefixes' in page:\n",
    "                for common_prefix in page['CommonPrefixes']:\n",
    "                    prefix_path = common_prefix['Prefix']\n",
    "                    # Get the table name from the path\n",
    "                    table_name = os.path.basename(prefix_path.rstrip('/'))\n",
    "                    table_folders.append((table_name, prefix_path))\n",
    "        \n",
    "        if not table_folders:\n",
    "            print(f\"No table directories found in {warehouse_dir}/{database_name}.db/\")\n",
    "            return sql_script, {}\n",
    "        \n",
    "        print(f\"Found {len(table_folders)} table directories: {[t[0] for t in table_folders]}\")\n",
    "        \n",
    "        # Process each table folder\n",
    "        results = {}\n",
    "        for table_name, folder_path in table_folders:\n",
    "            print(f\"\\nProcessing table: {table_name}\")\n",
    "            \n",
    "            # Check if the table already exists in Spark catalog\n",
    "            if table_name in existing_tables:\n",
    "                print(f\"Table {table_name} already exists in Spark catalog. Will generate DROP statement.\")\n",
    "                sql_script += f\"-- Table {table_name} already exists\\n\"\n",
    "                results[table_name] = \"already exists\"\n",
    "            \n",
    "            # Path to the table in S3\n",
    "            table_path = f\"{warehouse_dir}/{database_name}.db/{table_name}\"\n",
    "            \n",
    "            # Check if this is a Delta table by looking for _delta_log directory\n",
    "            delta_log_prefix = f\"{folder_path}_delta_log/\"\n",
    "            delta_log_response = s3_client.list_objects_v2(\n",
    "                Bucket=bucket_name, \n",
    "                Prefix=delta_log_prefix,\n",
    "                MaxKeys=1\n",
    "            )\n",
    "            \n",
    "            is_delta = 'Contents' in delta_log_response and len(delta_log_response['Contents']) > 0\n",
    "            \n",
    "            if is_delta:\n",
    "                print(f\"Found Delta table at {table_path}\")\n",
    "                \n",
    "                # Try to read the table schema and properties from existing Delta files\n",
    "                try:\n",
    "                    # Check for partition information\n",
    "                    partition_cols = []\n",
    "                    try:\n",
    "                        # Look for partition metadata in Delta log\n",
    "                        metadata_prefix = f\"{folder_path}_delta_log/00000000000000000000.json\"\n",
    "                        response = s3_client.get_object(Bucket=bucket_name, Key=metadata_prefix)\n",
    "                        metadata = json.loads(response['Body'].read().decode('utf-8'))\n",
    "                        \n",
    "                        if 'partitionColumns' in metadata:\n",
    "                            partition_cols = metadata['partitionColumns']\n",
    "                            print(f\"Detected partition columns: {partition_cols}\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error reading partition info: {str(e)}\")\n",
    "                    \n",
    "                    # Generate DROP statement if table exists\n",
    "                    sql_script += f\"-- Drop table if it exists\\n\"\n",
    "                    sql_script += f\"DROP TABLE IF EXISTS {database_name}.{table_name};\\n\\n\"\n",
    "                    \n",
    "                    # Generate CREATE TABLE statement\n",
    "                    sql_script += f\"-- Create Delta table pointing to existing data\\n\"\n",
    "                    sql_script += f\"CREATE TABLE {database_name}.{table_name}\\n\"\n",
    "                    sql_script += f\"USING DELTA\\n\"\n",
    "                    sql_script += f\"LOCATION '{table_path}';\\n\\n\"\n",
    "                    \n",
    "                    results[table_name] = \"success\"\n",
    "                    print(f\"Generated SQL for table {database_name}.{table_name}\")\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    error_msg = f\"Error generating SQL for table {table_name}: {str(e)}\"\n",
    "                    print(error_msg)\n",
    "                    sql_script += f\"-- {error_msg}\\n\\n\"\n",
    "                    results[table_name] = f\"error: {str(e)}\"\n",
    "            else:\n",
    "                note = f\"No Delta log found for {table_name}. This might not be a Delta table.\"\n",
    "                print(note)\n",
    "                sql_script += f\"-- {note}\\n\\n\"\n",
    "                results[table_name] = \"not a Delta table\"\n",
    "        \n",
    "        return sql_script, results\n",
    "    \n",
    "    except Exception as e:\n",
    "        error_msg = f\"Error scanning database directory: {str(e)}\"\n",
    "        print(error_msg)\n",
    "        sql_script += f\"-- {error_msg}\\n\"\n",
    "        return sql_script, {\"error\": str(e)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the Table Recovery Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scanning database omop531 in warehouse s3a://wba/warehouse\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/06 15:21:51 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n",
      "25/04/06 15:21:51 WARN VersionInfoUtils: The AWS SDK for Java 1.x entered maintenance mode starting July 31, 2024 and will reach end of support on December 31, 2025. For more information, see https://aws.amazon.com/blogs/developer/the-aws-sdk-for-java-1-x-is-in-maintenance-mode-effective-july-31-2024/\n",
      "You can print where on the file system the AWS SDK for Java 1.x core runtime is located by setting the AWS_JAVA_V1_PRINT_LOCATION environment variable or aws.java.v1.printLocation system property to 'true'.\n",
      "This message can be disabled by setting the AWS_JAVA_V1_DISABLE_DEPRECATION_ANNOUNCEMENT environment variable or aws.java.v1.disableDeprecationAnnouncement system property to 'true'.\n",
      "The AWS SDK for Java 1.x is being used here:\n",
      "at java.base/java.lang.Thread.getStackTrace(Thread.java:1619)\n",
      "at com.amazonaws.util.VersionInfoUtils.printDeprecationAnnouncement(VersionInfoUtils.java:81)\n",
      "at com.amazonaws.util.VersionInfoUtils.<clinit>(VersionInfoUtils.java:59)\n",
      "at com.amazonaws.ClientConfiguration.<clinit>(ClientConfiguration.java:95)\n",
      "at org.apache.hadoop.fs.s3a.S3AUtils.createAwsConf(S3AUtils.java:1258)\n",
      "at org.apache.hadoop.fs.s3a.DefaultS3ClientFactory.createS3Client(DefaultS3ClientFactory.java:114)\n",
      "at org.apache.hadoop.fs.s3a.S3AFileSystem.bindAWSClient(S3AFileSystem.java:898)\n",
      "at org.apache.hadoop.fs.s3a.S3AFileSystem.initialize(S3AFileSystem.java:534)\n",
      "at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469)\n",
      "at org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)\n",
      "at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)\n",
      "at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)\n",
      "at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)\n",
      "at org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)\n",
      "at org.apache.spark.sql.internal.SharedState$.qualifyWarehousePath(SharedState.scala:288)\n",
      "at org.apache.spark.sql.internal.SharedState.liftedTree1$1(SharedState.scala:80)\n",
      "at org.apache.spark.sql.internal.SharedState.<init>(SharedState.scala:79)\n",
      "at org.apache.spark.sql.SparkSession.$anonfun$sharedState$1(SparkSession.scala:143)\n",
      "at scala.Option.getOrElse(Option.scala:189)\n",
      "at org.apache.spark.sql.SparkSession.sharedState$lzycompute(SparkSession.scala:143)\n",
      "at org.apache.spark.sql.SparkSession.sharedState(SparkSession.scala:142)\n",
      "at org.apache.spark.sql.SparkSession.$anonfun$sessionState$2(SparkSession.scala:162)\n",
      "at scala.Option.getOrElse(Option.scala:189)\n",
      "at org.apache.spark.sql.SparkSession.sessionState$lzycompute(SparkSession.scala:160)\n",
      "at org.apache.spark.sql.SparkSession.sessionState(SparkSession.scala:157)\n",
      "at org.apache.spark.sql.internal.CatalogImpl.sessionCatalog(CatalogImpl.scala:49)\n",
      "at org.apache.spark.sql.internal.CatalogImpl.resolveNamespace(CatalogImpl.scala:425)\n",
      "at org.apache.spark.sql.internal.CatalogImpl.listTablesInternal(CatalogImpl.scala:144)\n",
      "at org.apache.spark.sql.internal.CatalogImpl.listTables(CatalogImpl.scala:128)\n",
      "at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n",
      "at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "at java.base/java.lang.reflect.Method.invoke(Method.java:569)\n",
      "at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "at py4j.Gateway.invoke(Gateway.java:282)\n",
      "at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "at py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "at py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "at java.base/java.lang.Thread.run(Thread.java:840)\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 existing tables in Spark catalog: ['concept']\n",
      "Listing objects in bucket 'wba' with prefix 'warehouse/omop531.db/'\n",
      "Found 39 table directories: ['attribute_definition', 'care_site', 'cdm_source', 'cohort', 'cohort_attribute', 'cohort_definition', 'concept', 'concept_ancestor', 'concept_class', 'concept_relationship', 'concept_synonym', 'condition_era', 'condition_occurrence', 'cost', 'death', 'device_exposure', 'domain', 'dose_era', 'drug_era', 'drug_exposure', 'drug_strength', 'fact_relationship', 'location', 'measurement', 'metadata', 'note', 'note_nlp', 'observation', 'observation_period', 'payer_plan_period', 'person', 'procedure_occurrence', 'provider', 'relationship', 'source_to_concept_map', 'specimen', 'visit_detail', 'visit_occurrence', 'vocabulary']\n",
      "\n",
      "Processing table: attribute_definition\n",
      "Found Delta table at s3a://wba/warehouse/omop531.db/attribute_definition\n",
      "Error reading partition info: Extra data: line 2 column 1 (char 359)\n",
      "Generated SQL for table omop531.attribute_definition\n",
      "\n",
      "Processing table: care_site\n",
      "Found Delta table at s3a://wba/warehouse/omop531.db/care_site\n",
      "Error reading partition info: Extra data: line 2 column 1 (char 359)\n",
      "Generated SQL for table omop531.care_site\n",
      "\n",
      "Processing table: cdm_source\n",
      "Found Delta table at s3a://wba/warehouse/omop531.db/cdm_source\n",
      "Error reading partition info: Extra data: line 2 column 1 (char 359)\n",
      "Generated SQL for table omop531.cdm_source\n",
      "\n",
      "Processing table: cohort\n",
      "Found Delta table at s3a://wba/warehouse/omop531.db/cohort\n",
      "Error reading partition info: Extra data: line 2 column 1 (char 359)\n",
      "Generated SQL for table omop531.cohort\n",
      "\n",
      "Processing table: cohort_attribute\n",
      "Found Delta table at s3a://wba/warehouse/omop531.db/cohort_attribute\n",
      "Error reading partition info: Extra data: line 2 column 1 (char 359)\n",
      "Generated SQL for table omop531.cohort_attribute\n",
      "\n",
      "Processing table: cohort_definition\n",
      "Found Delta table at s3a://wba/warehouse/omop531.db/cohort_definition\n",
      "Error reading partition info: Extra data: line 2 column 1 (char 359)\n",
      "Generated SQL for table omop531.cohort_definition\n",
      "\n",
      "Processing table: concept\n",
      "Table concept already exists in Spark catalog. Will generate DROP statement.\n",
      "Found Delta table at s3a://wba/warehouse/omop531.db/concept\n",
      "Error reading partition info: Extra data: line 2 column 1 (char 359)\n",
      "Generated SQL for table omop531.concept\n",
      "\n",
      "Processing table: concept_ancestor\n",
      "Found Delta table at s3a://wba/warehouse/omop531.db/concept_ancestor\n",
      "Error reading partition info: Extra data: line 2 column 1 (char 359)\n",
      "Generated SQL for table omop531.concept_ancestor\n",
      "\n",
      "Processing table: concept_class\n",
      "Found Delta table at s3a://wba/warehouse/omop531.db/concept_class\n",
      "Error reading partition info: Extra data: line 2 column 1 (char 359)\n",
      "Generated SQL for table omop531.concept_class\n",
      "\n",
      "Processing table: concept_relationship\n",
      "Found Delta table at s3a://wba/warehouse/omop531.db/concept_relationship\n",
      "Error reading partition info: Extra data: line 2 column 1 (char 359)\n",
      "Generated SQL for table omop531.concept_relationship\n",
      "\n",
      "Processing table: concept_synonym\n",
      "Found Delta table at s3a://wba/warehouse/omop531.db/concept_synonym\n",
      "Error reading partition info: Extra data: line 2 column 1 (char 359)\n",
      "Generated SQL for table omop531.concept_synonym\n",
      "\n",
      "Processing table: condition_era\n",
      "Found Delta table at s3a://wba/warehouse/omop531.db/condition_era\n",
      "Error reading partition info: Extra data: line 2 column 1 (char 359)\n",
      "Generated SQL for table omop531.condition_era\n",
      "\n",
      "Processing table: condition_occurrence\n",
      "Found Delta table at s3a://wba/warehouse/omop531.db/condition_occurrence\n",
      "Error reading partition info: Extra data: line 2 column 1 (char 359)\n",
      "Generated SQL for table omop531.condition_occurrence\n",
      "\n",
      "Processing table: cost\n",
      "Found Delta table at s3a://wba/warehouse/omop531.db/cost\n",
      "Error reading partition info: Extra data: line 2 column 1 (char 359)\n",
      "Generated SQL for table omop531.cost\n",
      "\n",
      "Processing table: death\n",
      "Found Delta table at s3a://wba/warehouse/omop531.db/death\n",
      "Error reading partition info: Extra data: line 2 column 1 (char 359)\n",
      "Generated SQL for table omop531.death\n",
      "\n",
      "Processing table: device_exposure\n",
      "Found Delta table at s3a://wba/warehouse/omop531.db/device_exposure\n",
      "Error reading partition info: Extra data: line 2 column 1 (char 359)\n",
      "Generated SQL for table omop531.device_exposure\n",
      "\n",
      "Processing table: domain\n",
      "Found Delta table at s3a://wba/warehouse/omop531.db/domain\n",
      "Error reading partition info: Extra data: line 2 column 1 (char 359)\n",
      "Generated SQL for table omop531.domain\n",
      "\n",
      "Processing table: dose_era\n",
      "Found Delta table at s3a://wba/warehouse/omop531.db/dose_era\n",
      "Error reading partition info: Extra data: line 2 column 1 (char 359)\n",
      "Generated SQL for table omop531.dose_era\n",
      "\n",
      "Processing table: drug_era\n",
      "Found Delta table at s3a://wba/warehouse/omop531.db/drug_era\n",
      "Error reading partition info: Extra data: line 2 column 1 (char 359)\n",
      "Generated SQL for table omop531.drug_era\n",
      "\n",
      "Processing table: drug_exposure\n",
      "Found Delta table at s3a://wba/warehouse/omop531.db/drug_exposure\n",
      "Error reading partition info: Extra data: line 2 column 1 (char 359)\n",
      "Generated SQL for table omop531.drug_exposure\n",
      "\n",
      "Processing table: drug_strength\n",
      "Found Delta table at s3a://wba/warehouse/omop531.db/drug_strength\n",
      "Error reading partition info: Extra data: line 2 column 1 (char 359)\n",
      "Generated SQL for table omop531.drug_strength\n",
      "\n",
      "Processing table: fact_relationship\n",
      "Found Delta table at s3a://wba/warehouse/omop531.db/fact_relationship\n",
      "Error reading partition info: Extra data: line 2 column 1 (char 359)\n",
      "Generated SQL for table omop531.fact_relationship\n",
      "\n",
      "Processing table: location\n",
      "Found Delta table at s3a://wba/warehouse/omop531.db/location\n",
      "Error reading partition info: Extra data: line 2 column 1 (char 359)\n",
      "Generated SQL for table omop531.location\n",
      "\n",
      "Processing table: measurement\n",
      "Found Delta table at s3a://wba/warehouse/omop531.db/measurement\n",
      "Error reading partition info: Extra data: line 2 column 1 (char 359)\n",
      "Generated SQL for table omop531.measurement\n",
      "\n",
      "Processing table: metadata\n",
      "Found Delta table at s3a://wba/warehouse/omop531.db/metadata\n",
      "Error reading partition info: Extra data: line 2 column 1 (char 359)\n",
      "Generated SQL for table omop531.metadata\n",
      "\n",
      "Processing table: note\n",
      "Found Delta table at s3a://wba/warehouse/omop531.db/note\n",
      "Error reading partition info: Extra data: line 2 column 1 (char 359)\n",
      "Generated SQL for table omop531.note\n",
      "\n",
      "Processing table: note_nlp\n",
      "Found Delta table at s3a://wba/warehouse/omop531.db/note_nlp\n",
      "Error reading partition info: Extra data: line 2 column 1 (char 359)\n",
      "Generated SQL for table omop531.note_nlp\n",
      "\n",
      "Processing table: observation\n",
      "Found Delta table at s3a://wba/warehouse/omop531.db/observation\n",
      "Error reading partition info: Extra data: line 2 column 1 (char 359)\n",
      "Generated SQL for table omop531.observation\n",
      "\n",
      "Processing table: observation_period\n",
      "Found Delta table at s3a://wba/warehouse/omop531.db/observation_period\n",
      "Error reading partition info: Extra data: line 2 column 1 (char 359)\n",
      "Generated SQL for table omop531.observation_period\n",
      "\n",
      "Processing table: payer_plan_period\n",
      "Found Delta table at s3a://wba/warehouse/omop531.db/payer_plan_period\n",
      "Error reading partition info: Extra data: line 2 column 1 (char 359)\n",
      "Generated SQL for table omop531.payer_plan_period\n",
      "\n",
      "Processing table: person\n",
      "Found Delta table at s3a://wba/warehouse/omop531.db/person\n",
      "Error reading partition info: Extra data: line 2 column 1 (char 359)\n",
      "Generated SQL for table omop531.person\n",
      "\n",
      "Processing table: procedure_occurrence\n",
      "Found Delta table at s3a://wba/warehouse/omop531.db/procedure_occurrence\n",
      "Error reading partition info: Extra data: line 2 column 1 (char 359)\n",
      "Generated SQL for table omop531.procedure_occurrence\n",
      "\n",
      "Processing table: provider\n",
      "Found Delta table at s3a://wba/warehouse/omop531.db/provider\n",
      "Error reading partition info: Extra data: line 2 column 1 (char 359)\n",
      "Generated SQL for table omop531.provider\n",
      "\n",
      "Processing table: relationship\n",
      "Found Delta table at s3a://wba/warehouse/omop531.db/relationship\n",
      "Error reading partition info: Extra data: line 2 column 1 (char 359)\n",
      "Generated SQL for table omop531.relationship\n",
      "\n",
      "Processing table: source_to_concept_map\n",
      "Found Delta table at s3a://wba/warehouse/omop531.db/source_to_concept_map\n",
      "Error reading partition info: Extra data: line 2 column 1 (char 359)\n",
      "Generated SQL for table omop531.source_to_concept_map\n",
      "\n",
      "Processing table: specimen\n",
      "Found Delta table at s3a://wba/warehouse/omop531.db/specimen\n",
      "Error reading partition info: Extra data: line 2 column 1 (char 359)\n",
      "Generated SQL for table omop531.specimen\n",
      "\n",
      "Processing table: visit_detail\n",
      "Found Delta table at s3a://wba/warehouse/omop531.db/visit_detail\n",
      "Error reading partition info: Extra data: line 2 column 1 (char 359)\n",
      "Generated SQL for table omop531.visit_detail\n",
      "\n",
      "Processing table: visit_occurrence\n",
      "Found Delta table at s3a://wba/warehouse/omop531.db/visit_occurrence\n",
      "Error reading partition info: Extra data: line 2 column 1 (char 359)\n",
      "Generated SQL for table omop531.visit_occurrence\n",
      "\n",
      "Processing table: vocabulary\n",
      "Found Delta table at s3a://wba/warehouse/omop531.db/vocabulary\n",
      "Error reading partition info: Extra data: line 2 column 1 (char 359)\n",
      "Generated SQL for table omop531.vocabulary\n",
      "\n",
      "Table recreation results:\n",
      "attribute_definition: success\n",
      "care_site: success\n",
      "cdm_source: success\n",
      "cohort: success\n",
      "cohort_attribute: success\n",
      "cohort_definition: success\n",
      "concept: success\n",
      "concept_ancestor: success\n",
      "concept_class: success\n",
      "concept_relationship: success\n",
      "concept_synonym: success\n",
      "condition_era: success\n",
      "condition_occurrence: success\n",
      "cost: success\n",
      "death: success\n",
      "device_exposure: success\n",
      "domain: success\n",
      "dose_era: success\n",
      "drug_era: success\n",
      "drug_exposure: success\n",
      "drug_strength: success\n",
      "fact_relationship: success\n",
      "location: success\n",
      "measurement: success\n",
      "metadata: success\n",
      "note: success\n",
      "note_nlp: success\n",
      "observation: success\n",
      "observation_period: success\n",
      "payer_plan_period: success\n",
      "person: success\n",
      "procedure_occurrence: success\n",
      "provider: success\n",
      "relationship: success\n",
      "source_to_concept_map: success\n",
      "specimen: success\n",
      "visit_detail: success\n",
      "visit_occurrence: success\n",
      "vocabulary: success\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Configure these parameters for your environment\n",
    "warehouse_dir = \"s3a://wba/warehouse\"  # Change to your warehouse directory\n",
    "aws_access_key = \"minioadmin\"          # Change if not using default\n",
    "aws_secret_key = \"minioadmin\"          # Change if not using default\n",
    "\n",
    "\n",
    "# database_name = \"ehr\"                  # Change to your database name\n",
    "\n",
    "# # Check if database exists and drop if requested to recreate\n",
    "# databases = [db.name for db in spark.catalog.listDatabases()]\n",
    "\n",
    "# if database_name in databases:\n",
    "#     print(f\"Database '{database_name}' already exists.\")\n",
    "#     # Option to return early if you don't want to recreate an existing database\n",
    "#     # return {\"status\": \"Database already exists, no action taken\"}\n",
    "\n",
    "# # Create database if it doesn't exist\n",
    "# db_location = f\"{warehouse_dir}/{database_name}.db\"\n",
    "# spark.sql(f\"CREATE DATABASE IF NOT EXISTS {database_name}\")\n",
    "# print(f\"Database '{database_name}' created or confirmed at location '{db_location}'\")\n",
    "\n",
    "# # Use the database\n",
    "# spark.sql(f\"USE {database_name}\")\n",
    "\n",
    "# # Run the recovery process\n",
    "# sql_script, results = recreate_tables_from_minio(\n",
    "#     spark=spark,\n",
    "#     warehouse_dir=warehouse_dir,\n",
    "#     database_name=database_name,\n",
    "#     aws_access_key=aws_access_key,\n",
    "#     aws_secret_key=aws_secret_key\n",
    "# )\n",
    "\n",
    "# # Print results\n",
    "# print(\"\\nTable recreation results:\")\n",
    "# for table, status in results.items():\n",
    "#     print(f\"{table}: {status}\")\n",
    "    \n",
    "# #  write to a file\n",
    "# file_name = f'0_recreate_tables_{database_name}.sql'\n",
    "# with open(file_name, 'w') as f:\n",
    "#     f.write(sql_script)\n",
    "\n",
    "\n",
    "database_name = \"omop531\"                  # Change to your database name\n",
    "    \n",
    "# Run the recovery process\n",
    "sql_script, results = recreate_tables_from_minio(\n",
    "    spark=spark,\n",
    "    warehouse_dir=warehouse_dir,\n",
    "    database_name=database_name,\n",
    "    aws_access_key=aws_access_key,\n",
    "    aws_secret_key=aws_secret_key\n",
    ")\n",
    "\n",
    "# Print results\n",
    "print(\"\\nTable recreation results:\")\n",
    "for table, status in results.items():\n",
    "    print(f\"{table}: {status}\")\n",
    "    \n",
    "#  write to a file\n",
    "file_name = f'0_recreate_tables_{database_name}.sql'\n",
    "with open(file_name, 'w') as f:\n",
    "    f.write(sql_script)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List Tables After Recovery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tables in database omop531 after recovery:\n",
      "- concept (EXTERNAL)\n"
     ]
    }
   ],
   "source": [
    "# List all tables in the database after recovery\n",
    "print(f\"\\nTables in database {database_name} after recovery:\")\n",
    "for table in spark.catalog.listTables(database_name):\n",
    "    print(f\"- {table.name} ({table.tableType})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query a Recovered Table\n",
    "\n",
    "Now you can test one of the recovered tables by querying it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table omop531.patients does not exist.\n"
     ]
    }
   ],
   "source": [
    "# Replace 'table_name' with one of your recovered tables\n",
    "table_name = \"patients\"  # Change this to one of your actual tables\n",
    "\n",
    "try:\n",
    "    # Check if table exists\n",
    "    if spark.catalog.tableExists(f\"{database_name}.{table_name}\"):\n",
    "        # Read the table\n",
    "        df = spark.table(f\"{database_name}.{table_name}\")\n",
    "        \n",
    "        # Show schema\n",
    "        print(f\"Schema for {database_name}.{table_name}:\")\n",
    "        df.printSchema()\n",
    "        \n",
    "        # Show sample data\n",
    "        print(f\"\\nSample data from {database_name}.{table_name}:\")\n",
    "        df.show(5)\n",
    "        \n",
    "        # Show count\n",
    "        count = df.count()\n",
    "        print(f\"\\nTotal records: {count}\")\n",
    "    else:\n",
    "        print(f\"Table {database_name}.{table_name} does not exist.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error querying table: {str(e)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
