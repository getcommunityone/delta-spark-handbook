{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct vocabulary tables, based on tables downloaded from Athena website and available here on s3://hls-eng-data-public/data/rwe/omop-vocabs/ If you like to download a different dataset, downoad the vocabularies from Athena and use databricks dbfs api utilities to upload downloaded vocabularies to dbfs under your vocab_path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "from typing import List\n",
    "from pyspark.sql import SparkSession\n",
    "import logging\n",
    "\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Define the base directory\n",
    "\n",
    "\n",
    "def create_spark_session(app_name=\"EHR Data Loader\", aws_access_key=None, aws_secret_key=None):\n",
    "    \"\"\"\n",
    "    Create and return a Spark session configured for Delta Lake with S3 access.\n",
    "    \"\"\"\n",
    "    # Stop any existing Spark session\n",
    "    try:\n",
    "        SparkSession.builder.getOrCreate().stop()\n",
    "        print(\"Stopped existing Spark session\")\n",
    "    except:\n",
    "        print(\"No existing Spark session to stop\")\n",
    "\n",
    "\n",
    "    # Get the absolute path two levels up from the current file's directory\n",
    "    jars_home = '/home/developer/projects/delta-spark-handbook/delta-jars'\n",
    "\n",
    "    print(f\"Jars home is set to: {jars_home}\")\n",
    "\n",
    "    # Define the base directory for JAR files\n",
    "    if not os.path.exists(jars_home):\n",
    "        raise Exception(f\"JAR directory not found at: {jars_home}\")\n",
    "\n",
    "    # Required JARs\n",
    "    jar_locations = [\n",
    "        f\"{jars_home}/delta-spark_2.12-3.3.0.jar\",\n",
    "        f\"{jars_home}/delta-storage-3.3.0.jar\",\n",
    "        f\"{jars_home}/hadoop-aws-3.3.4.jar\",\n",
    "        f\"{jars_home}/bundle-2.24.12.jar\",\n",
    "        # Add Hadoop client JARs\n",
    "        f\"{jars_home}/hadoop-client-3.4.1.jar\",\n",
    "        f\"{jars_home}/hadoop-client-runtime-3.4.1.jar\",\n",
    "        f\"{jars_home}/hadoop-client-api-3.4.1.jar\"\n",
    "    ]\n",
    "\n",
    "    # Verify all JARs exist\n",
    "    for jar in jar_locations:\n",
    "        if not os.path.exists(jar):\n",
    "            raise Exception(f\"Required JAR not found: {jar}\")\n",
    "\n",
    "    # Create Hadoop configuration directory\n",
    "    hadoop_conf_dir = \"hadoop-conf\"\n",
    "    os.makedirs(hadoop_conf_dir, exist_ok=True)\n",
    "\n",
    "    # Write core-site.xml with S3 configuration\n",
    "    core_site_xml = f\"\"\"<?xml version=\"1.0\"?>\n",
    "<configuration>\n",
    "    <property>\n",
    "        <name>fs.s3a.impl</name>\n",
    "        <value>org.apache.hadoop.fs.s3a.S3AFileSystem</value>\n",
    "    </property>\n",
    "    <property>\n",
    "        <name>fs.s3a.endpoint</name>\n",
    "        <value>http://localhost:9000</value>\n",
    "    </property>\n",
    "    <property>\n",
    "        <name>fs.s3a.access.key</name>\n",
    "        <value>{aws_access_key or 'minioadmin'}</value>\n",
    "    </property>\n",
    "    <property>\n",
    "        <name>fs.s3a.secret.key</name>\n",
    "        <value>{aws_secret_key or 'minioadmin'}</value>\n",
    "    </property>\n",
    "    <property>\n",
    "        <name>fs.s3a.path.style.access</name>\n",
    "        <value>true</value>\n",
    "    </property>\n",
    "    <property>\n",
    "        <name>fs.s3a.connection.ssl.enabled</name>\n",
    "        <value>false</value>\n",
    "    </property>\n",
    "</configuration>\"\"\"\n",
    "\n",
    "    with open(f\"{hadoop_conf_dir}/core-site.xml\", \"w\") as f:\n",
    "        f.write(core_site_xml)\n",
    "\n",
    "    # Set environment variables\n",
    "    os.environ[\"HADOOP_CONF_DIR\"] = os.path.abspath(hadoop_conf_dir)\n",
    "    os.environ[\"SPARK_HOME\"] = \"/opt/spark\"\n",
    "    os.environ[\"SPARK_CLASSPATH\"] = \":\".join(\n",
    "        [os.path.abspath(jar) for jar in jar_locations])\n",
    "    os.environ[\"HADOOP_CLASSPATH\"] = os.environ[\"SPARK_CLASSPATH\"]\n",
    "\n",
    "    # Create Spark session with comprehensive configuration\n",
    "    builder = (SparkSession.builder\n",
    "               .appName(app_name)\n",
    "               .master(\"local[*]\")\n",
    "               .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "               .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "               .config(\"spark.sql.catalogImplementation\", \"hive\")\n",
    "               .config(\"spark.hadoop.javax.jdo.option.ConnectionURL\", \"jdbc:postgresql://localhost:5432/metastore_db\")\n",
    "               .config(\"spark.hadoop.javax.jdo.option.ConnectionDriverName\", \"org.postgresql.Driver\")\n",
    "               .config(\"spark.hadoop.javax.jdo.option.ConnectionUserName\", \"admin\")\n",
    "               .config(\"spark.hadoop.javax.jdo.option.ConnectionPassword\", \"admin\")\n",
    "               .config(\"spark.hadoop.hive.metastore.uris\", \"thrift://localhost:9083\")\n",
    "               .config(\"spark.sql.warehouse.dir\", \"s3a://wba/warehouse\")\n",
    "               .config(\"spark.driver.extraClassPath\", \":\".join([os.path.abspath(jar) for jar in jar_locations]))\n",
    "               .config(\"spark.executor.extraClassPath\", \":\".join([os.path.abspath(jar) for jar in jar_locations]))\n",
    "               .config(\"spark.jars.excludes\", \"org.slf4j:slf4j-log4j12,org.slf4j:slf4j-reload4j,org.slf4j:log4j-slf4j-impl\")\n",
    "               .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "               .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://localhost:9000\")\n",
    "               .config(\"spark.hadoop.fs.s3a.access.key\", aws_access_key or \"minioadmin\")\n",
    "               .config(\"spark.hadoop.fs.s3a.secret.key\", aws_secret_key or \"minioadmin\")\n",
    "               .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\")\n",
    "               .config(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"false\")\n",
    "               .config(\"spark.hadoop.fs.s3a.fast.upload\", \"true\")\n",
    "               .config(\"spark.hadoop.fs.s3a.multipart.size\", \"5242880\")\n",
    "               .config(\"spark.hadoop.fs.s3a.block.size\", \"5242880\")\n",
    "               .config(\"spark.hadoop.fs.s3a.multipart.threshold\", \"5242880\")\n",
    "               .config(\"spark.hadoop.fs.s3a.threads.core\", \"10\")\n",
    "               .config(\"spark.hadoop.fs.s3a.threads.max\", \"20\")\n",
    "               .config(\"spark.hadoop.fs.s3a.max.total.tasks\", \"50\")\n",
    "               .config(\"spark.hadoop.fs.s3a.connection.timeout\", \"60000\")\n",
    "               .config(\"spark.hadoop.fs.s3a.connection.establish.timeout\", \"60000\")\n",
    "               .config(\"spark.hadoop.fs.s3a.socket.timeout\", \"60000\")\n",
    "               .config(\"spark.hadoop.fs.s3a.connection.maximum\", \"50\")\n",
    "               .config(\"spark.hadoop.fs.s3a.fast.upload.buffer\", \"bytebuffer\")\n",
    "               .config(\"spark.hadoop.fs.s3a.fast.upload.active.blocks\", \"2\")\n",
    "               .config(\"spark.hadoop.fs.s3a.multipart.purge\", \"false\")\n",
    "               .config(\"spark.hadoop.fs.s3a.multipart.purge.age\", \"86400000\")\n",
    "               .config(\"spark.hadoop.fs.s3a.retry.limit\", \"10\")\n",
    "               .config(\"spark.hadoop.fs.s3a.retry.interval\", \"1000\")\n",
    "               .config(\"spark.hadoop.fs.s3a.attempts.maximum\", \"10\")\n",
    "               .config(\"spark.hadoop.fs.s3a.connection.request.timeout\", \"60000\")\n",
    "               .config(\"spark.hadoop.fs.s3a.threads.keepalivetime\", \"60000\")\n",
    "               .enableHiveSupport())\n",
    "\n",
    "    return builder.getOrCreate()\n",
    "\n",
    "aws_access_key = \"minioadmin\"\n",
    "aws_secret_key = \"minioadmin\"\n",
    "\n",
    "spark = create_spark_session(\n",
    "    app_name=\"OMOP Vocab Setup\", aws_access_key=aws_access_key, aws_secret_key=aws_secret_key)\n",
    "\n",
    "spark.sql(\"SHOW DATABASES\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from urllib.parse import urlparse\n",
    "from botocore import UNSIGNED\n",
    "from botocore.config import Config\n",
    "\n",
    "# Variables\n",
    "database_name = \"omop531\"\n",
    "project_name = 'omop-cdm-100K'\n",
    "vocab_s3_path = \"s3://hls-eng-data-public/data/rwe/omop-vocabs/\"\n",
    "\n",
    "# Output\n",
    "print(f\"Using OMOP version {database_name}\")\n",
    "print(f\"Using vocabulary tables in {vocab_s3_path}\")\n",
    "\n",
    "# Set the database in Spark\n",
    "spark.sql(f\"USE {database_name}\")\n",
    "\n",
    "# Parse S3 path\n",
    "parsed = urlparse(vocab_s3_path)\n",
    "bucket = parsed.netloc\n",
    "prefix = parsed.path.lstrip('/')\n",
    "\n",
    "# Initialize S3 client\n",
    "s3 = boto3.client('s3', config=boto3.session.Config(signature_version=UNSIGNED))\n",
    "\n",
    "# List objects in S3 prefix\n",
    "response = s3.list_objects_v2(Bucket=bucket, Prefix=prefix)\n",
    "\n",
    "# Display the list of files\n",
    "if 'Contents' in response:\n",
    "    for obj in response['Contents']:\n",
    "        print(obj['Key'])\n",
    "else:\n",
    "    print(\"No files found in the specified S3 path.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/30 17:53:42 WARN SQLConf: The SQL config 'spark.sql.legacy.parquet.datetimeRebaseModeInWrite' has been deprecated in Spark v3.2 and may be removed in the future. Use 'spark.sql.parquet.datetimeRebaseModeInWrite' instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📦 Processing DOMAIN...\n",
      "⏳ Downloading from S3...\n",
      "✅ Download complete in 0.10s\n",
      "📤 Saving raw .csv.gz to MinIO: data/rwe/omop-vocabs/DOMAIN.csv.gz...\n",
      "✅ Raw .csv.gz saved to MinIO.\n",
      "⏳ Reading CSV with Spark...\n",
      "✅ Spark read complete in 0.17s\n",
      "⏳ Writing to Delta format: s3a://wba/warehouse/omop531.db/domain...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Delta write complete in 2.95s\n",
      "⏳ Registering Delta table:  omop531.domain...\n",
      "✅ Successfully registered s3a://wba/warehouse/omop531.db/domain into table  omop531.domain\n",
      "✅ Completed domain in 3.27s → omop531.domain\n",
      "\n",
      "📦 Processing CONCEPT...\n",
      "⏳ Downloading from S3...\n",
      "✅ Download complete in 5.32s\n",
      "📤 Saving raw .csv.gz to MinIO: data/rwe/omop-vocabs/CONCEPT.csv.gz...\n",
      "✅ Raw .csv.gz saved to MinIO.\n",
      "⏳ Reading CSV with Spark...\n",
      "✅ Spark read complete in 3.74s\n",
      "⏳ Writing to Delta format: s3a://wba/warehouse/omop531.db/concept...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/30 17:53:56 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Delta write complete in 29.04s\n",
      "⏳ Registering Delta table:  omop531.concept...\n",
      "✅ Successfully registered s3a://wba/warehouse/omop531.db/concept into table  omop531.concept\n",
      "✅ Completed concept in 38.15s → omop531.concept\n",
      "\n",
      "📦 Processing VOCABULARY...\n",
      "⏳ Downloading from S3...\n",
      "✅ Download complete in 0.53s\n",
      "📤 Saving raw .csv.gz to MinIO: data/rwe/omop-vocabs/VOCABULARY.csv.gz...\n",
      "✅ Raw .csv.gz saved to MinIO.\n",
      "⏳ Reading CSV with Spark...\n",
      "✅ Spark read complete in 0.24s\n",
      "⏳ Writing to Delta format: s3a://wba/warehouse/omop531.db/vocabulary...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Delta write complete in 5.12s\n",
      "⏳ Registering Delta table:  omop531.vocabulary...\n",
      "✅ Successfully registered s3a://wba/warehouse/omop531.db/vocabulary into table  omop531.vocabulary\n",
      "✅ Completed vocabulary in 5.96s → omop531.vocabulary\n",
      "\n",
      "📦 Processing CONCEPT_ANCESTOR...\n",
      "⏳ Downloading from S3...\n",
      "✅ Download complete in 24.52s\n",
      "📤 Saving raw .csv.gz to MinIO: data/rwe/omop-vocabs/CONCEPT_ANCESTOR.csv.gz...\n",
      "✅ Raw .csv.gz saved to MinIO.\n",
      "⏳ Reading CSV with Spark...\n",
      "✅ Spark read complete in 27.48s\n",
      "⏳ Writing to Delta format: s3a://wba/warehouse/omop531.db/concept_ancestor...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/30 17:55:25 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "25/03/30 17:56:25 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "25/03/30 17:56:26 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "25/03/30 17:56:27 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "25/03/30 17:56:27 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "25/03/30 17:56:28 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "25/03/30 17:56:28 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "25/03/30 17:56:28 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "25/03/30 17:57:35 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "25/03/30 17:57:42 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "25/03/30 17:57:42 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Delta write complete in 167.36s\n",
      "⏳ Registering Delta table:  omop531.concept_ancestor...\n",
      "✅ Successfully registered s3a://wba/warehouse/omop531.db/concept_ancestor into table  omop531.concept_ancestor\n",
      "✅ Completed concept_ancestor in 219.50s → omop531.concept_ancestor\n",
      "\n",
      "📦 Processing CONCEPT_RELATIONSHIP...\n",
      "⏳ Downloading from S3...\n",
      "✅ Download complete in 6.39s\n",
      "📤 Saving raw .csv.gz to MinIO: data/rwe/omop-vocabs/CONCEPT_RELATIONSHIP.csv.gz...\n",
      "✅ Raw .csv.gz saved to MinIO.\n",
      "⏳ Reading CSV with Spark...\n",
      "✅ Spark read complete in 15.21s\n",
      "⏳ Writing to Delta format: s3a://wba/warehouse/omop531.db/concept_relationship...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/30 17:58:34 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "25/03/30 17:59:34 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "25/03/30 17:59:35 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "25/03/30 17:59:37 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "25/03/30 17:59:41 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "25/03/30 17:59:42 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Delta write complete in 115.87s\n",
      "⏳ Registering Delta table:  omop531.concept_relationship...\n",
      "✅ Successfully registered s3a://wba/warehouse/omop531.db/concept_relationship into table  omop531.concept_relationship\n",
      "✅ Completed concept_relationship in 137.56s → omop531.concept_relationship\n",
      "\n",
      "📦 Processing RELATIONSHIP...\n",
      "⏳ Downloading from S3...\n",
      "✅ Download complete in 0.47s\n",
      "📤 Saving raw .csv.gz to MinIO: data/rwe/omop-vocabs/RELATIONSHIP.csv.gz...\n",
      "✅ Raw .csv.gz saved to MinIO.\n",
      "⏳ Reading CSV with Spark...\n",
      "✅ Spark read complete in 0.28s\n",
      "⏳ Writing to Delta format: s3a://wba/warehouse/omop531.db/relationship...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Delta write complete in 2.64s\n",
      "⏳ Registering Delta table:  omop531.relationship...\n",
      "✅ Successfully registered s3a://wba/warehouse/omop531.db/relationship into table  omop531.relationship\n",
      "✅ Completed relationship in 3.51s → omop531.relationship\n",
      "\n",
      "📦 Processing CONCEPT_SYNONYM...\n",
      "⏳ Downloading from S3...\n",
      "✅ Download complete in 8.45s\n",
      "📤 Saving raw .csv.gz to MinIO: data/rwe/omop-vocabs/CONCEPT_SYNONYM.csv.gz...\n",
      "✅ Raw .csv.gz saved to MinIO.\n",
      "⏳ Reading CSV with Spark...\n",
      "✅ Spark read complete in 8.73s\n",
      "⏳ Writing to Delta format: s3a://wba/warehouse/omop531.db/concept_synonym...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/30 18:00:52 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Delta write complete in 29.79s\n",
      "⏳ Registering Delta table:  omop531.concept_synonym...\n",
      "✅ Successfully registered s3a://wba/warehouse/omop531.db/concept_synonym into table  omop531.concept_synonym\n",
      "✅ Completed concept_synonym in 47.11s → omop531.concept_synonym\n",
      "\n",
      "📦 Processing CONCEPT_CLASS...\n",
      "⏳ Downloading from S3...\n",
      "✅ Download complete in 0.59s\n",
      "📤 Saving raw .csv.gz to MinIO: data/rwe/omop-vocabs/CONCEPT_CLASS.csv.gz...\n",
      "✅ Raw .csv.gz saved to MinIO.\n",
      "⏳ Reading CSV with Spark...\n",
      "✅ Spark read complete in 0.21s\n",
      "⏳ Writing to Delta format: s3a://wba/warehouse/omop531.db/concept_class...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Delta write complete in 4.02s\n",
      "⏳ Registering Delta table:  omop531.concept_class...\n",
      "✅ Successfully registered s3a://wba/warehouse/omop531.db/concept_class into table  omop531.concept_class\n",
      "✅ Completed concept_class in 4.96s → omop531.concept_class\n",
      "\n",
      "📦 Processing DRUG_STRENGTH...\n",
      "⏳ Downloading from S3...\n",
      "✅ Download complete in 1.75s\n",
      "📤 Saving raw .csv.gz to MinIO: data/rwe/omop-vocabs/DRUG_STRENGTH.csv.gz...\n",
      "✅ Raw .csv.gz saved to MinIO.\n",
      "⏳ Reading CSV with Spark...\n",
      "✅ Spark read complete in 1.38s\n",
      "⏳ Writing to Delta format: s3a://wba/warehouse/omop531.db/drug_strength...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/30 18:01:31 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Delta write complete in 33.03s\n",
      "⏳ Registering Delta table:  omop531.drug_strength...\n",
      "✅ Successfully registered s3a://wba/warehouse/omop531.db/drug_strength into table  omop531.drug_strength\n",
      "✅ Completed drug_strength in 36.56s → omop531.drug_strength\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import to_date\n",
    "import tempfile, os, io, gzip, time\n",
    "from urllib.parse import urlparse\n",
    "import boto3\n",
    "from botocore import UNSIGNED\n",
    "from botocore.config import Config\n",
    "\n",
    "# -----------------------------------\n",
    "# Spark legacy behavior for OMOP dates\n",
    "# -----------------------------------\n",
    "spark.sql(\"set spark.sql.legacy.timeParserPolicy=LEGACY\")\n",
    "spark.sql(\"set spark.sql.legacy.parquet.datetimeRebaseModeInWrite=LEGACY\")\n",
    "\n",
    "# -----------------------------------\n",
    "# Config\n",
    "# -----------------------------------\n",
    "database_name = \"omop531\"\n",
    "vocab_s3_path = \"s3://hls-eng-data-public/data/rwe/omop-vocabs/\"\n",
    "output_path_base = \"s3a://my-minio-bucket/omop-vocab-delta\"\n",
    "\n",
    "parsed = urlparse(vocab_s3_path)\n",
    "bucket = parsed.netloc\n",
    "prefix = parsed.path.lstrip('/').rstrip('/')\n",
    "\n",
    "# -----------------------------------\n",
    "# Get public file list via boto3\n",
    "# -----------------------------------\n",
    "s3 = boto3.client('s3', config=Config(signature_version=UNSIGNED))\n",
    "response = s3.list_objects_v2(Bucket=bucket, Prefix=prefix)\n",
    "available_keys = [obj['Key'] for obj in response.get('Contents', [])]\n",
    "\n",
    "# -----------------------------------\n",
    "# OMOP tables\n",
    "# -----------------------------------\n",
    "tablelist = [\n",
    "    \"DOMAIN\", \"CONCEPT\", \"VOCABULARY\", \"CONCEPT_ANCESTOR\", \"CONCEPT_RELATIONSHIP\",\n",
    "    \"RELATIONSHIP\", \"CONCEPT_SYNONYM\", \"CONCEPT_CLASS\", \"DRUG_STRENGTH\"\n",
    "]\n",
    " \n",
    "# -----------------------------------\n",
    "# Main loop using Spark\n",
    "# -----------------------------------\n",
    "for table_name in tablelist:\n",
    "    print(f\"\\n📦 Processing {table_name}...\")\n",
    "\n",
    "    key = f\"{prefix}/{table_name}.csv.gz\"\n",
    "    if key not in available_keys:\n",
    "        print(f\"❌ File not found in S3: {key}\")\n",
    "        continue\n",
    "\n",
    "    start_time = time.time()\n",
    "    try:\n",
    "        print(\"⏳ Downloading from S3...\")\n",
    "        response = s3.get_object(Bucket=bucket, Key=key)\n",
    "        gz_content = response['Body'].read()\n",
    "        download_time = time.time()\n",
    "        print(f\"✅ Download complete in {download_time - start_time:.2f}s\")\n",
    "\n",
    "        # Save raw .csv.gz file to MinIO\n",
    "        raw_gz_key = key\n",
    "        print(f\"📤 Saving raw .csv.gz to MinIO: {raw_gz_key}...\")\n",
    "        s3_minio = boto3.client(\n",
    "            \"s3\",\n",
    "            endpoint_url=\"http://localhost:9000\",  # adjust to your MinIO endpoint\n",
    "            aws_access_key_id=\"minioadmin\",\n",
    "            aws_secret_access_key=\"minioadmin\"\n",
    "        )\n",
    "        s3_minio.put_object(\n",
    "            Bucket=f\"{database_name}\",\n",
    "            Key=raw_gz_key,\n",
    "            Body=gz_content\n",
    "        )\n",
    "        print(\"✅ Raw .csv.gz saved to MinIO.\")\n",
    "        \n",
    "        # Use decompression + in-memory IO stream to avoid temp file I/O\n",
    "        with gzip.GzipFile(fileobj=io.BytesIO(gz_content)) as gz:\n",
    "            content = gz.read().decode('utf-8')\n",
    "\n",
    "        with tempfile.NamedTemporaryFile(suffix=\".csv\", delete=False, mode='w', encoding='utf-8') as tmp_file:\n",
    "            tmp_file.write(content)\n",
    "            tmp_path = tmp_file.name\n",
    "        \n",
    "        print(\"⏳ Reading CSV with Spark...\")\n",
    "        \n",
    "        df = spark.read \\\n",
    "            .option(\"header\", \"true\") \\\n",
    "            .option(\"inferSchema\", \"false\") \\\n",
    "            .option(\"mode\", \"PERMISSIVE\") \\\n",
    "            .option(\"quote\", \"\\\"\") \\\n",
    "            .option(\"escape\", \"\\\"\") \\\n",
    "            .csv(tmp_path)  \n",
    "     \n",
    "        read_time = time.time()\n",
    "        print(f\"✅ Spark read complete in {read_time - download_time:.2f}s\")\n",
    "\n",
    "        output_path_base = f\"s3a://{database_name}\"\n",
    "\n",
    "\n",
    "        # Handle date columns\n",
    "        if table_name in [\"CONCEPT\", \"CONCEPT_RELATIONSHIP\", \"DRUG_STRENGTH\"]:\n",
    "            df = df.withColumn(\"valid_start_date\", to_date(\"valid_start_date\", \"yyyy-MM-dd\")) \\\n",
    "                   .withColumn(\"valid_end_date\", to_date(\"valid_end_date\", \"yyyy-MM-dd\"))\n",
    "\n",
    "        # Get warehouse dir from Spark config\n",
    "        warehouse_dir = spark.conf.get(\"spark.sql.warehouse.dir\").rstrip(\"/\")\n",
    "\n",
    "        table_name = table_name.lower()\n",
    "        # Format: {warehouse_dir}/{database}.db/{table_name}\n",
    "        table_path = f\"{warehouse_dir}/{database_name}.db/{table_name}\"\n",
    "\n",
    "        print(f\"⏳ Writing to Delta format: {table_path}...\")\n",
    "        df.write.format(\"delta\") \\\n",
    "            .mode(\"overwrite\") \\\n",
    "            .option(\"overwriteSchema\", \"true\") \\\n",
    "            .save(f\"{table_path}\")\n",
    "        write_time = time.time()\n",
    "        print(f\"✅ Delta write complete in {write_time - read_time:.2f}s\")\n",
    "\n",
    "        print(f\"⏳ Registering Delta table:  {database_name}.{table_name}...\")\n",
    " \n",
    "        spark.sql(f\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS {database_name}.{table_name}\n",
    "        USING DELTA\n",
    "            LOCATION '{table_path}'\n",
    "        \"\"\")\n",
    "\n",
    "        register_time = time.time()\n",
    "        print(f\"✅ Successfully registered {table_path} into table  {database_name}.{table_name}\")\n",
    "\n",
    "        os.remove(tmp_path)\n",
    "        \n",
    "        total_time = register_time - start_time\n",
    "        print(f\"✅ Completed {table_name} in {total_time:.2f}s → {database_name}.{table_name}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error processing {table_name}: {e}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
