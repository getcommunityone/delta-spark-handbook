{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct vocabulary tables, based on tables downloaded from Athena website and available here on s3://hls-eng-data-public/data/rwe/omop-vocabs/ If you like to download a different dataset, downoad the vocabularies from Athena and use databricks dbfs api utilities to upload downloaded vocabularies to dbfs under your vocab_path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "from typing import List\n",
    "from pyspark.sql import SparkSession\n",
    "import logging\n",
    "\n",
    "# Add parent directory to path for relative import\n",
    "notebook_dir = os.path.dirname(os.path.abspath('__file__'))\n",
    "project_root = os.path.abspath(os.path.join(notebook_dir, '..', '..', '..', '..'))\n",
    "sys.path.append(project_root)\n",
    "\n",
    "from c1_core.aws_storage_service.aws_storage_bucket import AwsStorageBucket\n",
    "from c1_core.delta_lake_service.delta_table import DeltaTable\n",
    "\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Define the base directory\n",
    "\n",
    "\n",
    "def create_spark_session(app_name=\"EHR Data Loader\", aws_access_key=None, aws_secret_key=None):\n",
    "    \"\"\"\n",
    "    Create and return a Spark session configured for Delta Lake with S3 access.\n",
    "    \"\"\"\n",
    "    # Stop any existing Spark session\n",
    "    try:\n",
    "        SparkSession.builder.getOrCreate().stop()\n",
    "        print(\"Stopped existing Spark session\")\n",
    "    except:\n",
    "        print(\"No existing Spark session to stop\")\n",
    "\n",
    "\n",
    "    # Get the absolute path two levels up from the current file's directory\n",
    "    jars_home = '/home/developer/projects/delta-spark-handbook/delta-jars'\n",
    "\n",
    "    print(f\"Jars home is set to: {jars_home}\")\n",
    "\n",
    "    # Define the base directory for JAR files\n",
    "    if not os.path.exists(jars_home):\n",
    "        raise Exception(f\"JAR directory not found at: {jars_home}\")\n",
    "\n",
    "    # Required JARs\n",
    "    jar_locations = [\n",
    "        f\"{jars_home}/delta-spark_2.12-3.3.0.jar\",\n",
    "        f\"{jars_home}/delta-storage-3.3.0.jar\",\n",
    "        f\"{jars_home}/hadoop-aws-3.3.4.jar\",\n",
    "        f\"{jars_home}/bundle-2.24.12.jar\",\n",
    "        # Add Hadoop client JARs\n",
    "        f\"{jars_home}/hadoop-client-3.3.4.jar\",\n",
    "        f\"{jars_home}/hadoop-client-runtime-3.3.4.jar\",\n",
    "        f\"{jars_home}/hadoop-client-api-3.3.4.jar\"\n",
    "    ]\n",
    "\n",
    "    # Verify all JARs exist\n",
    "    for jar in jar_locations:\n",
    "        if not os.path.exists(jar):\n",
    "            raise Exception(f\"Required JAR not found: {jar}\")\n",
    "\n",
    "    # Create Hadoop configuration directory\n",
    "    hadoop_conf_dir = \"hadoop-conf\"\n",
    "    os.makedirs(hadoop_conf_dir, exist_ok=True)\n",
    "\n",
    "    # Write core-site.xml with S3 configuration\n",
    "    core_site_xml = f\"\"\"<?xml version=\"1.0\"?>\n",
    "<configuration>\n",
    "    <property>\n",
    "        <name>fs.s3a.impl</name>\n",
    "        <value>org.apache.hadoop.fs.s3a.S3AFileSystem</value>\n",
    "    </property>\n",
    "    <property>\n",
    "        <name>fs.s3a.endpoint</name>\n",
    "        <value>http://localhost:9000</value>\n",
    "    </property>\n",
    "    <property>\n",
    "        <name>fs.s3a.access.key</name>\n",
    "        <value>{aws_access_key or 'minioadmin'}</value>\n",
    "    </property>\n",
    "    <property>\n",
    "        <name>fs.s3a.secret.key</name>\n",
    "        <value>{aws_secret_key or 'minioadmin'}</value>\n",
    "    </property>\n",
    "    <property>\n",
    "        <name>fs.s3a.path.style.access</name>\n",
    "        <value>true</value>\n",
    "    </property>\n",
    "    <property>\n",
    "        <name>fs.s3a.connection.ssl.enabled</name>\n",
    "        <value>false</value>\n",
    "    </property>\n",
    "</configuration>\"\"\"\n",
    "\n",
    "    with open(f\"{hadoop_conf_dir}/core-site.xml\", \"w\") as f:\n",
    "        f.write(core_site_xml)\n",
    "\n",
    "    # Set environment variables\n",
    "    os.environ[\"HADOOP_CONF_DIR\"] = os.path.abspath(hadoop_conf_dir)\n",
    "    os.environ[\"SPARK_HOME\"] = \"/opt/spark\"\n",
    "    os.environ[\"SPARK_CLASSPATH\"] = \":\".join(\n",
    "        [os.path.abspath(jar) for jar in jar_locations])\n",
    "    os.environ[\"HADOOP_CLASSPATH\"] = os.environ[\"SPARK_CLASSPATH\"]\n",
    "\n",
    "    # Create Spark session with comprehensive configuration\n",
    "    builder = (SparkSession.builder\n",
    "               .appName(app_name)\n",
    "               .master(\"local[*]\")\n",
    "               #.master(\"spark://localhost:7077\")\n",
    "               .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "               .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "               .config(\"spark.sql.catalogImplementation\", \"hive\")\n",
    "               .config(\"spark.hadoop.javax.jdo.option.ConnectionURL\", \"jdbc:postgresql://localhost:5432/metastore_db\")\n",
    "               .config(\"spark.hadoop.javax.jdo.option.ConnectionDriverName\", \"org.postgresql.Driver\")\n",
    "               .config(\"spark.hadoop.javax.jdo.option.ConnectionUserName\", \"admin\")\n",
    "               .config(\"spark.hadoop.javax.jdo.option.ConnectionPassword\", \"admin\")\n",
    "               .config(\"spark.hadoop.hive.metastore.uris\", \"thrift://localhost:9083\")\n",
    "               .config(\"spark.sql.warehouse.dir\", \"s3a://wba/warehouse\")\n",
    "               .config(\"spark.driver.extraClassPath\", \":\".join([os.path.abspath(jar) for jar in jar_locations]))\n",
    "               .config(\"spark.executor.extraClassPath\", \":\".join([os.path.abspath(jar) for jar in jar_locations]))\n",
    "               .config(\"spark.jars.excludes\", \"org.slf4j:slf4j-log4j12,org.slf4j:slf4j-reload4j,org.slf4j:log4j-slf4j-impl\")\n",
    "               .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "               .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://localhost:9000\")\n",
    "               .config(\"spark.hadoop.fs.s3a.access.key\", aws_access_key or \"minioadmin\")\n",
    "               .config(\"spark.hadoop.fs.s3a.secret.key\", aws_secret_key or \"minioadmin\")\n",
    "               .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\")\n",
    "               .config(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"false\")\n",
    "               .config(\"spark.hadoop.fs.s3a.fast.upload\", \"true\")\n",
    "               .config(\"spark.hadoop.fs.s3a.multipart.size\", \"5242880\")\n",
    "               .config(\"spark.hadoop.fs.s3a.block.size\", \"5242880\")\n",
    "               .config(\"spark.hadoop.fs.s3a.multipart.threshold\", \"5242880\")\n",
    "               .config(\"spark.hadoop.fs.s3a.threads.core\", \"10\")\n",
    "               .config(\"spark.hadoop.fs.s3a.threads.max\", \"20\")\n",
    "               .config(\"spark.hadoop.fs.s3a.max.total.tasks\", \"50\")\n",
    "               .config(\"spark.hadoop.fs.s3a.connection.timeout\", \"60000\")\n",
    "               .config(\"spark.hadoop.fs.s3a.connection.establish.timeout\", \"60000\")\n",
    "               .config(\"spark.hadoop.fs.s3a.socket.timeout\", \"60000\")\n",
    "               .config(\"spark.hadoop.fs.s3a.connection.maximum\", \"50\")\n",
    "               .config(\"spark.hadoop.fs.s3a.fast.upload.buffer\", \"bytebuffer\")\n",
    "               .config(\"spark.hadoop.fs.s3a.fast.upload.active.blocks\", \"2\")\n",
    "               .config(\"spark.hadoop.fs.s3a.multipart.purge\", \"false\")\n",
    "               .config(\"spark.hadoop.fs.s3a.multipart.purge.age\", \"86400000\")\n",
    "               .config(\"spark.hadoop.fs.s3a.retry.limit\", \"10\")\n",
    "               .config(\"spark.hadoop.fs.s3a.retry.interval\", \"1000\")\n",
    "               .config(\"spark.hadoop.fs.s3a.attempts.maximum\", \"10\")\n",
    "               .config(\"spark.hadoop.fs.s3a.connection.request.timeout\", \"60000\")\n",
    "               .config(\"spark.hadoop.fs.s3a.threads.keepalivetime\", \"60000\")\n",
    "               .enableHiveSupport())\n",
    "\n",
    "\n",
    "             \n",
    "    return builder.getOrCreate()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using vocabulary tables in s3://hls-eng-data-public/data/rwe/omop-vocabs/\n",
      "data/rwe/omop-vocabs/CONCEPT.csv.gz\n",
      "data/rwe/omop-vocabs/CONCEPT_ANCESTOR.csv.gz\n",
      "data/rwe/omop-vocabs/CONCEPT_CLASS.csv.gz\n",
      "data/rwe/omop-vocabs/CONCEPT_RELATIONSHIP.csv.gz\n",
      "data/rwe/omop-vocabs/CONCEPT_SYNONYM.csv.gz\n",
      "data/rwe/omop-vocabs/DOMAIN.csv.gz\n",
      "data/rwe/omop-vocabs/DRUG_STRENGTH.csv.gz\n",
      "data/rwe/omop-vocabs/RELATIONSHIP.csv.gz\n",
      "data/rwe/omop-vocabs/VOCABULARY.csv.gz\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "from urllib.parse import urlparse\n",
    "from botocore import UNSIGNED\n",
    "from botocore.config import Config\n",
    "\n",
    "\n",
    "project_name = 'omop-cdm-100K'\n",
    "vocab_s3_path = \"s3://hls-eng-data-public/data/rwe/omop-vocabs/\"\n",
    "\n",
    "print(f\"Using vocabulary tables in {vocab_s3_path}\")\n",
    "\n",
    "# Parse S3 path\n",
    "parsed = urlparse(vocab_s3_path)\n",
    "bucket = parsed.netloc\n",
    "prefix = parsed.path.lstrip('/')\n",
    "\n",
    "# Initialize S3 client\n",
    "s3 = boto3.client('s3', config=boto3.session.Config(signature_version=UNSIGNED))\n",
    "\n",
    "# List objects in S3 prefix\n",
    "response = s3.list_objects_v2(Bucket=bucket, Prefix=prefix)\n",
    "\n",
    "# Display the list of files\n",
    "if 'Contents' in response:\n",
    "    for obj in response['Contents']:\n",
    "        print(obj['Key'])\n",
    "else:\n",
    "    print(\"No files found in the specified S3 path.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_date\n",
    "import tempfile, os, io, gzip, time\n",
    "from urllib.parse import urlparse\n",
    "import boto3\n",
    "from botocore import UNSIGNED\n",
    "from botocore.config import Config\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ingest():\n",
    "    \"\"\"Main function to orchestrate the data processing workflow.\"\"\"\n",
    " \n",
    "    \n",
    "    # Configuration\n",
    "\n",
    "    MINIO_BUCKET = \"omop531\"\n",
    "    S3_BUCKET = \"hls-eng-data-public\"\n",
    "    S3_PREFIX = \"data/rwe/omop-vocabs/\"\n",
    "    \n",
    "    result = AwsStorageBucket.download_files_from_s3_to_minio(\n",
    "    minio_bucket_name=MINIO_BUCKET,\n",
    "    s3_path=f\"s3://{S3_BUCKET}/{S3_PREFIX}/\",\n",
    "    data_product_id=\"test-product\",\n",
    "    environment=\"test\",\n",
    "    minio_endpoint=\"http://localhost:9000\",  # Use localhost since running on host\n",
    "    minio_access_key=\"minioadmin\",\n",
    "    minio_secret_key=\"minioadmin\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def data_load():\n",
    "\n",
    "    aws_access_key = \"minioadmin\"\n",
    "    aws_secret_key = \"minioadmin\"\n",
    "\n",
    "    spark = create_spark_session(\n",
    "        app_name=\"OMOP Vocab Setup\", aws_access_key=aws_access_key, aws_secret_key=aws_secret_key)\n",
    "\n",
    "    spark.conf.set(\"spark.sql.parquet.datetimeRebaseModeInWrite\", \"LEGACY\")\n",
    "\n",
    "    spark.sql(\"SHOW DATABASES\").show()\n",
    "\n",
    "    database_name = \"omop531\"\n",
    "\n",
    "    # Create database if it doesn't exist\n",
    "    print(f\"Creating database {database_name} if it doesn't exist...\")\n",
    "    spark.sql(f\"CREATE DATABASE IF NOT EXISTS {database_name}\")\n",
    "    \n",
    "\n",
    "    # OMOP tables to process\n",
    "    # tablelist = [\n",
    "    #     \"DOMAIN\", \"CONCEPT\", \"VOCABULARY\", \"CONCEPT_ANCESTOR\", \"CONCEPT_RELATIONSHIP\",\n",
    "    #     \"RELATIONSHIP\", \"CONCEPT_SYNONYM\", \"CONCEPT_CLASS\", \"DRUG_STRENGTH\"\n",
    "    #]\n",
    "    \n",
    "    MINIO_BUCKET = \"omop531\"\n",
    "    S3_BUCKET = \"hls-eng-data-public\"\n",
    "    S3_LOCAL_BUCKET = \"omop531\"\n",
    "    S3_PREFIX = \"data/rwe/omop-vocabs\"\n",
    "    \n",
    "    # tablelist = AwsStorageBucket.list_s3_files_boto(\n",
    "    #      s3_dir_path=f\"s3://{S3_BUCKET}/{S3_PREFIX}/\",\n",
    "    #      file_extension=\".gz\",\n",
    "    #      data_product_id=\"test-product\",\n",
    "    #      environment=\"test\",\n",
    "    #      public_access=True\n",
    "    # )\n",
    "    \n",
    "    tablelist = AwsStorageBucket.list_s3_files_boto(\n",
    "         s3_dir_path=f\"s3://{S3_LOCAL_BUCKET}/{S3_PREFIX}/\",\n",
    "         file_extension=\".gz\",\n",
    "         data_product_id=\"test-product\",\n",
    "         environment=\"test\",\n",
    "         public_access=True,\n",
    "         endpoint_url=\"http://localhost:9000/\",\n",
    "         aws_access_key_id=\"minioadmin\",\n",
    "         aws_secret_access_key=\"minioadmin\"\n",
    "    )\n",
    "      \n",
    "    print(tablelist)\n",
    "    # tablelist = AwsStorageBucket.list_s3_files_minio(\n",
    "    #     s3_dir_path=f\"s3://{S3_LOCAL_BUCKET}/{S3_PREFIX}/\",\n",
    "    #     file_extension=\".gz\",\n",
    "    #     data_product_id=\"test-product\",\n",
    "    #     environment=\"test\",\n",
    "    #     aws_access_key_id=\"minioadmin\",\n",
    "    #     aws_secret_access_key=\"minioadmin\"\n",
    "    # )\n",
    "    \n",
    "    # tablelist = [\n",
    "    #         \"s3a://omop531/data/rwe/omop-vocabs/CONCEPT.csv.gz\",\n",
    "    #         \"s3a://omop531/data/rwe/omop-vocabs/VOCABULARY.csv.gz\",\n",
    "    #         \"s3a://omop531/data/rwe/omop-vocabs/CONCEPT_ANCESTOR.csv.gz\",\n",
    "    #         \"s3a://omop531/data/rwe/omop-vocabs/CONCEPT_RELATIONSHIP.csv.gz\",\n",
    "    #         \"s3a://omop531/data/rwe/omop-vocabs/RELATIONSHIP.csv.gz\",\n",
    "    #         \"s3a://omop531/data/rwe/omop-vocabs/CONCEPT_SYNONYM.csv.gz\",\n",
    "    #         \"s3a://omop531/data/rwe/omop-vocabs/CONCEPT_CLASS.csv.gz\",\n",
    "    #         \"s3a://omop531/data/rwe/omop-vocabs/DOMAIN.csv.gz\",\n",
    "    #         \"s3a://omop531/data/rwe/omop-vocabs/DRUG_STRENGTH.csv.gz\"\n",
    "    #     ]\n",
    "    # Update paths that start with 's3://' to 's3a://'\n",
    "    tablelist = [\n",
    "        table_path.replace(\"s3://\", \"s3a://\", 1) if table_path.startswith(\"s3://\") else table_path\n",
    "        for table_path in tablelist\n",
    "    ]\n",
    "\n",
    "    # Process each table\n",
    "    for table_path in tablelist:\n",
    "        print(f\"\\nüì¶ Processing {table_path}...\")\n",
    "        table_file = os.path.basename(table_path)\n",
    "        table_name = table_file.split(\".\")[0].lower()  # removes \".gz\" or any extension\n",
    "        DeltaTable.load_file_to_delta(spark, table_path, database_name, table_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopped existing Spark session\n",
      "Jars home is set to: /home/developer/projects/delta-spark-handbook/delta-jars\n",
      "+---------+\n",
      "|namespace|\n",
      "+---------+\n",
      "|  default|\n",
      "|  omop531|\n",
      "+---------+\n",
      "\n",
      "Creating database omop531 if it doesn't exist...\n",
      "2025-04-06 15:15:52 - INFO - Listing files in MinIO path: s3://omop531/data/rwe/omop-vocabs/ with extension: .gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:aws_storage_service:aws_storage_bucket.py:Listing files in MinIO path: s3://omop531/data/rwe/omop-vocabs/ with extension: .gz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-06 15:15:52 - INFO - Found 9 files.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:aws_storage_service:aws_storage_bucket.py:Found 9 files.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['s3://omop531/data/rwe/omop-vocabs/CONCEPT.csv.gz', 's3://omop531/data/rwe/omop-vocabs/CONCEPT_ANCESTOR.csv.gz', 's3://omop531/data/rwe/omop-vocabs/CONCEPT_CLASS.csv.gz', 's3://omop531/data/rwe/omop-vocabs/CONCEPT_RELATIONSHIP.csv.gz', 's3://omop531/data/rwe/omop-vocabs/CONCEPT_SYNONYM.csv.gz', 's3://omop531/data/rwe/omop-vocabs/DOMAIN.csv.gz', 's3://omop531/data/rwe/omop-vocabs/DRUG_STRENGTH.csv.gz', 's3://omop531/data/rwe/omop-vocabs/RELATIONSHIP.csv.gz', 's3://omop531/data/rwe/omop-vocabs/VOCABULARY.csv.gz']\n",
      "\n",
      "üì¶ Processing s3a://omop531/data/rwe/omop-vocabs/CONCEPT.csv.gz...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ Calculated table path: s3a://wba/warehouse/omop531.db/concept\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/06 15:16:22 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "25/04/06 15:18:43 WARN HiveExternalCatalog: Couldn't find corresponding Hive SerDe for data source provider delta. Persisting data source table `spark_catalog`.`omop531`.`concept` into Hive metastore in Spark SQL specific format, which is NOT compatible with Hive.\n",
      "25/04/06 15:18:43 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded s3a://omop531/data/rwe/omop-vocabs/CONCEPT.csv.gz into table omop531.concept\n",
      "\n",
      "üì¶ Processing s3a://omop531/data/rwe/omop-vocabs/CONCEPT_ANCESTOR.csv.gz...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ Calculated table path: s3a://wba/warehouse/omop531.db/concept_ancestor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:py4j.clientserver:Error while receiving.                       (0 + 1) / 1]\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/developer/.local/lib/python3.10/site-packages/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "RuntimeError: reentrant call inside <_io.BufferedReader name=71>\n",
      "INFO:py4j.clientserver:Closing down clientserver connection\n",
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/developer/.local/lib/python3.10/site-packages/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "RuntimeError: reentrant call inside <_io.BufferedReader name=71>\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/developer/.local/lib/python3.10/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/home/developer/.local/lib/python3.10/site-packages/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n",
      "INFO:py4j.clientserver:Error while receiving.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/developer/.local/lib/python3.10/site-packages/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "  File \"/usr/lib/python3.10/socket.py\", line 705, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "  File \"/opt/spark/python/pyspark/context.py\", line 381, in signal_handler\n",
      "    self.cancelAllJobs()\n",
      "  File \"/opt/spark/python/pyspark/context.py\", line 2446, in cancelAllJobs\n",
      "    self._jsc.sc().cancelAllJobs()\n",
      "  File \"/home/developer/.local/lib/python3.10/site-packages/py4j/java_gateway.py\", line 1322, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/opt/spark/python/pyspark/errors/exceptions/captured.py\", line 179, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/home/developer/.local/lib/python3.10/site-packages/py4j/protocol.py\", line 334, in get_return_value\n",
      "    raise Py4JError(\n",
      "py4j.protocol.Py4JError: An error occurred while calling o612.sc\n",
      "INFO:py4j.clientserver:Closing down clientserver connection\n",
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/developer/.local/lib/python3.10/site-packages/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "  File \"/usr/lib/python3.10/socket.py\", line 705, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "  File \"/opt/spark/python/pyspark/context.py\", line 381, in signal_handler\n",
      "    self.cancelAllJobs()\n",
      "  File \"/opt/spark/python/pyspark/context.py\", line 2446, in cancelAllJobs\n",
      "    self._jsc.sc().cancelAllJobs()\n",
      "  File \"/home/developer/.local/lib/python3.10/site-packages/py4j/java_gateway.py\", line 1322, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/opt/spark/python/pyspark/errors/exceptions/captured.py\", line 179, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/home/developer/.local/lib/python3.10/site-packages/py4j/protocol.py\", line 334, in get_return_value\n",
      "    raise Py4JError(\n",
      "py4j.protocol.Py4JError: An error occurred while calling o612.sc\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/developer/.local/lib/python3.10/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/home/developer/.local/lib/python3.10/site-packages/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n",
      "INFO:py4j.clientserver:Closing down clientserver connection\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error loading s3a://omop531/data/rwe/omop-vocabs/CONCEPT_ANCESTOR.csv.gz into table omop531.concept_ancestor: An error occurred while calling o709.save\n",
      "\n",
      "üì¶ Processing s3a://omop531/data/rwe/omop-vocabs/CONCEPT_CLASS.csv.gz...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ Calculated table path: s3a://wba/warehouse/omop531.db/concept_class\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/06 15:28:51 WARN HiveExternalCatalog: Couldn't find corresponding Hive SerDe for data source provider delta. Persisting data source table `spark_catalog`.`omop531`.`concept_class` into Hive metastore in Spark SQL specific format, which is NOT compatible with Hive.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded s3a://omop531/data/rwe/omop-vocabs/CONCEPT_CLASS.csv.gz into table omop531.concept_class\n",
      "\n",
      "üì¶ Processing s3a://omop531/data/rwe/omop-vocabs/CONCEPT_RELATIONSHIP.csv.gz...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    \n",
    "    # ingest()\n",
    "    data_load()\n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
