{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "your 131072x1 screen size is bogus. expect trouble\n",
            "25/04/04 16:43:40 WARN Utils: Your hostname, JBLAPTOPW11 resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)\n",
            "25/04/04 16:43:40 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
            "25/04/04 16:43:41 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
            "25/04/04 16:43:41 WARN DependencyUtils: Local jar /workspace/delta-jars/delta-spark_2.12-3.3.0.jar does not exist, skipping.\n",
            "25/04/04 16:43:41 WARN DependencyUtils: Local jar /workspace/delta-jars/delta-storage-3.3.0.jar does not exist, skipping.\n",
            "25/04/04 16:43:41 WARN DependencyUtils: Local jar /workspace/delta-jars/hadoop-aws-3.3.2.jar does not exist, skipping.\n",
            "25/04/04 16:43:41 WARN DependencyUtils: Local jar /workspace/delta-jars/aws-java-sdk-bundle-1.12.782.jar does not exist, skipping.\n",
            "25/04/04 16:43:41 WARN DependencyUtils: Local jar /workspace/delta-jars/kyuubi/externals/engines/spark/kyuubi-spark-sql-engine_2.12-1.10.0.jar does not exist, skipping.\n",
            "25/04/04 16:43:41 WARN DependencyUtils: Local jar /workspace/delta-jars/kyuubi/externals/engines/spark/kyuubi-common_2.12-1.10.0.jar does not exist, skipping.\n",
            "25/04/04 16:43:41 INFO SparkContext: Running Spark version 3.5.5\n",
            "25/04/04 16:43:41 INFO SparkContext: OS info Linux, 5.15.167.4-microsoft-standard-WSL2, amd64\n",
            "25/04/04 16:43:41 INFO SparkContext: Java version 17.0.14\n",
            "25/04/04 16:43:41 INFO ResourceUtils: ==============================================================\n",
            "25/04/04 16:43:41 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
            "25/04/04 16:43:41 INFO ResourceUtils: ==============================================================\n",
            "25/04/04 16:43:41 INFO SparkContext: Submitted application: DeltaExample\n",
            "25/04/04 16:43:41 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
            "25/04/04 16:43:41 INFO ResourceProfile: Limiting resource is cpu\n",
            "25/04/04 16:43:41 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
            "25/04/04 16:43:41 INFO SecurityManager: Changing view acls to: developer\n",
            "25/04/04 16:43:41 INFO SecurityManager: Changing modify acls to: developer\n",
            "25/04/04 16:43:41 INFO SecurityManager: Changing view acls groups to: \n",
            "25/04/04 16:43:41 INFO SecurityManager: Changing modify acls groups to: \n",
            "25/04/04 16:43:41 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: developer; groups with view permissions: EMPTY; users with modify permissions: developer; groups with modify permissions: EMPTY\n",
            "25/04/04 16:43:42 INFO Utils: Successfully started service 'sparkDriver' on port 34177.\n",
            "25/04/04 16:43:42 INFO SparkEnv: Registering MapOutputTracker\n",
            "25/04/04 16:43:42 INFO SparkEnv: Registering BlockManagerMaster\n",
            "25/04/04 16:43:42 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
            "25/04/04 16:43:42 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
            "25/04/04 16:43:42 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
            "25/04/04 16:43:42 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-e5dd4fc6-d7ba-4926-9e8e-90a7a838451b\n",
            "25/04/04 16:43:42 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB\n",
            "25/04/04 16:43:42 INFO SparkEnv: Registering OutputCommitCoordinator\n",
            "25/04/04 16:43:42 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI\n",
            "25/04/04 16:43:42 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
            "25/04/04 16:43:42 ERROR SparkContext: Failed to add /workspace/delta-jars/delta-spark_2.12-3.3.0.jar to Spark environment\n",
            "java.io.FileNotFoundException: Jar /workspace/delta-jars/delta-spark_2.12-3.3.0.jar not found\n",
            "\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)\n",
            "\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)\n",
            "\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)\n",
            "\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)\n",
            "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
            "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
            "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
            "\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)\n",
            "\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n",
            "\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
            "\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)\n",
            "\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n",
            "\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)\n",
            "\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)\n",
            "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n",
            "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
            "\tat py4j.Gateway.invoke(Gateway.java:238)\n",
            "\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n",
            "\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n",
            "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
            "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
            "25/04/04 16:43:42 ERROR SparkContext: Failed to add /workspace/delta-jars/delta-storage-3.3.0.jar to Spark environment\n",
            "java.io.FileNotFoundException: Jar /workspace/delta-jars/delta-storage-3.3.0.jar not found\n",
            "\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)\n",
            "\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)\n",
            "\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)\n",
            "\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)\n",
            "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
            "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
            "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
            "\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)\n",
            "\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n",
            "\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
            "\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)\n",
            "\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n",
            "\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)\n",
            "\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)\n",
            "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n",
            "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
            "\tat py4j.Gateway.invoke(Gateway.java:238)\n",
            "\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n",
            "\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n",
            "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
            "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
            "25/04/04 16:43:42 ERROR SparkContext: Failed to add /workspace/delta-jars/hadoop-aws-3.3.2.jar to Spark environment\n",
            "java.io.FileNotFoundException: Jar /workspace/delta-jars/hadoop-aws-3.3.2.jar not found\n",
            "\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)\n",
            "\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)\n",
            "\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)\n",
            "\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)\n",
            "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
            "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
            "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
            "\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)\n",
            "\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n",
            "\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
            "\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)\n",
            "\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n",
            "\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)\n",
            "\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)\n",
            "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n",
            "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
            "\tat py4j.Gateway.invoke(Gateway.java:238)\n",
            "\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n",
            "\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n",
            "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
            "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
            "25/04/04 16:43:42 ERROR SparkContext: Failed to add /workspace/delta-jars/aws-java-sdk-bundle-1.12.782.jar to Spark environment\n",
            "java.io.FileNotFoundException: Jar /workspace/delta-jars/aws-java-sdk-bundle-1.12.782.jar not found\n",
            "\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)\n",
            "\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)\n",
            "\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)\n",
            "\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)\n",
            "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
            "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
            "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
            "\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)\n",
            "\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n",
            "\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
            "\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)\n",
            "\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n",
            "\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)\n",
            "\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)\n",
            "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n",
            "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
            "\tat py4j.Gateway.invoke(Gateway.java:238)\n",
            "\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n",
            "\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n",
            "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
            "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
            "25/04/04 16:43:42 ERROR SparkContext: Failed to add /workspace/delta-jars/kyuubi/externals/engines/spark/kyuubi-spark-sql-engine_2.12-1.10.0.jar to Spark environment\n",
            "java.io.FileNotFoundException: Jar /workspace/delta-jars/kyuubi/externals/engines/spark/kyuubi-spark-sql-engine_2.12-1.10.0.jar not found\n",
            "\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)\n",
            "\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)\n",
            "\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)\n",
            "\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)\n",
            "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
            "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
            "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
            "\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)\n",
            "\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n",
            "\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
            "\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)\n",
            "\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n",
            "\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)\n",
            "\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)\n",
            "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n",
            "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
            "\tat py4j.Gateway.invoke(Gateway.java:238)\n",
            "\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n",
            "\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n",
            "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
            "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
            "25/04/04 16:43:42 ERROR SparkContext: Failed to add /workspace/delta-jars/kyuubi/externals/engines/spark/kyuubi-common_2.12-1.10.0.jar to Spark environment\n",
            "java.io.FileNotFoundException: Jar /workspace/delta-jars/kyuubi/externals/engines/spark/kyuubi-common_2.12-1.10.0.jar not found\n",
            "\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)\n",
            "\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)\n",
            "\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)\n",
            "\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)\n",
            "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
            "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
            "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
            "\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)\n",
            "\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n",
            "\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
            "\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)\n",
            "\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n",
            "\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)\n",
            "\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)\n",
            "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n",
            "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
            "\tat py4j.Gateway.invoke(Gateway.java:238)\n",
            "\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n",
            "\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n",
            "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
            "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
            "25/04/04 16:43:42 INFO Executor: Starting executor ID driver on host 10.255.255.254\n",
            "25/04/04 16:43:42 INFO Executor: OS info Linux, 5.15.167.4-microsoft-standard-WSL2, amd64\n",
            "25/04/04 16:43:42 INFO Executor: Java version 17.0.14\n",
            "25/04/04 16:43:42 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): 'file:/workspace/delta-jars/delta-spark_2.12-3.3.0.jar,/workspace/delta-jars/delta-storage-3.3.0.jar,/workspace/delta-jars/hadoop-aws-3.3.2.jar,/workspace/delta-jars/aws-java-sdk-bundle-1.12.782.jar,/workspace/delta-jars/kyuubi/externals/engines/spark/kyuubi-spark-sql-engine_2.12-1.10.0.jar,/workspace/delta-jars/kyuubi/externals/engines/spark/kyuubi-common_2.12-1.10.0.jar,file:/home/developer/projects/delta-spark-handbook/accelerator/materials/1-getting-started/notebooks/kyuubi-common_2.12-1.10.0.jar'\n",
            "25/04/04 16:43:42 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@61fb9751 for default.\n",
            "25/04/04 16:43:42 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 43641.\n",
            "25/04/04 16:43:42 INFO NettyBlockTransferService: Server created on 10.255.255.254:43641\n",
            "25/04/04 16:43:42 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
            "25/04/04 16:43:42 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 10.255.255.254, 43641, None)\n",
            "25/04/04 16:43:42 INFO BlockManagerMasterEndpoint: Registering block manager 10.255.255.254:43641 with 434.4 MiB RAM, BlockManagerId(driver, 10.255.255.254, 43641, None)\n",
            "25/04/04 16:43:42 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 10.255.255.254, 43641, None)\n",
            "25/04/04 16:43:42 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 10.255.255.254, 43641, None)\n",
            "25/04/04 16:43:43 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\n",
            "25/04/04 16:43:43 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n",
            "25/04/04 16:43:43 INFO MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
            "25/04/04 16:43:43 INFO MetricsSystemImpl: s3a-file-system metrics system started\n",
            "25/04/04 16:43:43 WARN VersionInfoUtils: The AWS SDK for Java 1.x entered maintenance mode starting July 31, 2024 and will reach end of support on December 31, 2025. For more information, see https://aws.amazon.com/blogs/developer/the-aws-sdk-for-java-1-x-is-in-maintenance-mode-effective-july-31-2024/\n",
            "You can print where on the file system the AWS SDK for Java 1.x core runtime is located by setting the AWS_JAVA_V1_PRINT_LOCATION environment variable or aws.java.v1.printLocation system property to 'true'.\n",
            "This message can be disabled by setting the AWS_JAVA_V1_DISABLE_DEPRECATION_ANNOUNCEMENT environment variable or aws.java.v1.disableDeprecationAnnouncement system property to 'true'.\n",
            "The AWS SDK for Java 1.x is being used here:\n",
            "at java.base/java.lang.Thread.getStackTrace(Thread.java:1619)\n",
            "at com.amazonaws.util.VersionInfoUtils.printDeprecationAnnouncement(VersionInfoUtils.java:81)\n",
            "at com.amazonaws.util.VersionInfoUtils.<clinit>(VersionInfoUtils.java:59)\n",
            "at com.amazonaws.ClientConfiguration.<clinit>(ClientConfiguration.java:95)\n",
            "at org.apache.hadoop.fs.s3a.S3AUtils.createAwsConf(S3AUtils.java:1258)\n",
            "at org.apache.hadoop.fs.s3a.DefaultS3ClientFactory.createS3Client(DefaultS3ClientFactory.java:114)\n",
            "at org.apache.hadoop.fs.s3a.S3AFileSystem.bindAWSClient(S3AFileSystem.java:898)\n",
            "at org.apache.hadoop.fs.s3a.S3AFileSystem.initialize(S3AFileSystem.java:534)\n",
            "at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469)\n",
            "at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:537)\n",
            "at org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)\n",
            "at org.apache.spark.sql.internal.SharedState$.qualifyWarehousePath(SharedState.scala:288)\n",
            "at org.apache.spark.sql.internal.SharedState.liftedTree1$1(SharedState.scala:80)\n",
            "at org.apache.spark.sql.internal.SharedState.<init>(SharedState.scala:79)\n",
            "at org.apache.spark.sql.SparkSession.$anonfun$sharedState$1(SparkSession.scala:143)\n",
            "at scala.Option.getOrElse(Option.scala:189)\n",
            "at org.apache.spark.sql.SparkSession.sharedState$lzycompute(SparkSession.scala:143)\n",
            "at org.apache.spark.sql.SparkSession.sharedState(SparkSession.scala:142)\n",
            "at org.apache.spark.sql.SparkSession.$anonfun$sessionState$2(SparkSession.scala:162)\n",
            "at scala.Option.getOrElse(Option.scala:189)\n",
            "at org.apache.spark.sql.SparkSession.sessionState$lzycompute(SparkSession.scala:160)\n",
            "at org.apache.spark.sql.SparkSession.sessionState(SparkSession.scala:157)\n",
            "at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
            "at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n",
            "at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
            "at java.base/java.lang.reflect.Method.invoke(Method.java:569)\n",
            "at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
            "at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
            "at py4j.Gateway.invoke(Gateway.java:282)\n",
            "at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
            "at py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
            "at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
            "at py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
            "at java.base/java.lang.Thread.run(Thread.java:840)\n",
            "25/04/04 16:43:44 INFO SharedState: Warehouse path is 's3a://wba/warehouse'.\n",
            "25/04/04 16:43:47 INFO CodeGenerator: Code generated in 337.279914 ms\n",
            "25/04/04 16:43:47 INFO SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0\n",
            "25/04/04 16:43:47 INFO DAGScheduler: Got job 0 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
            "25/04/04 16:43:47 INFO DAGScheduler: Final stage: ResultStage 0 (showString at NativeMethodAccessorImpl.java:0)\n",
            "25/04/04 16:43:47 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/04/04 16:43:47 INFO DAGScheduler: Missing parents: List()\n",
            "25/04/04 16:43:47 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[6] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
            "25/04/04 16:43:47 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 12.6 KiB, free 434.4 MiB)\n",
            "25/04/04 16:43:47 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 6.6 KiB, free 434.4 MiB)\n",
            "25/04/04 16:43:47 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.255.255.254:43641 (size: 6.6 KiB, free: 434.4 MiB)\n",
            "25/04/04 16:43:47 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1585\n",
            "25/04/04 16:43:47 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[6] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
            "25/04/04 16:43:47 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0\n",
            "25/04/04 16:43:47 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (10.255.255.254, executor driver, partition 0, PROCESS_LOCAL, 8979 bytes) \n",
            "25/04/04 16:43:47 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)\n",
            "25/04/04 16:43:48 INFO CodeGenerator: Code generated in 12.973507 ms(0 + 1) / 1]\n",
            "25/04/04 16:43:48 INFO PythonRunner: Times: total = 979, boot = 850, init = 129, finish = 0\n",
            "25/04/04 16:43:49 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1886 bytes result sent to driver\n",
            "25/04/04 16:43:49 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1224 ms on 10.255.255.254 (executor driver) (1/1)\n",
            "25/04/04 16:43:49 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
            "25/04/04 16:43:49 INFO PythonAccumulatorV2: Connected to AccumulatorServer at host: 127.0.0.1 port: 41533\n",
            "25/04/04 16:43:49 INFO DAGScheduler: ResultStage 0 (showString at NativeMethodAccessorImpl.java:0) finished in 1.587 s\n",
            "25/04/04 16:43:49 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/04/04 16:43:49 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished\n",
            "25/04/04 16:43:49 INFO DAGScheduler: Job 0 finished: showString at NativeMethodAccessorImpl.java:0, took 1.656961 s\n",
            "25/04/04 16:43:49 INFO SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0\n",
            "25/04/04 16:43:49 INFO DAGScheduler: Got job 1 (showString at NativeMethodAccessorImpl.java:0) with 4 output partitions\n",
            "25/04/04 16:43:49 INFO DAGScheduler: Final stage: ResultStage 1 (showString at NativeMethodAccessorImpl.java:0)\n",
            "25/04/04 16:43:49 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/04/04 16:43:49 INFO DAGScheduler: Missing parents: List()\n",
            "25/04/04 16:43:49 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[6] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
            "25/04/04 16:43:49 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 12.6 KiB, free 434.4 MiB)\n",
            "25/04/04 16:43:49 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 6.6 KiB, free 434.4 MiB)\n",
            "25/04/04 16:43:49 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 10.255.255.254:43641 (size: 6.6 KiB, free: 434.4 MiB)\n",
            "25/04/04 16:43:49 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1585\n",
            "25/04/04 16:43:49 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(1, 2, 3, 4))\n",
            "25/04/04 16:43:49 INFO TaskSchedulerImpl: Adding task set 1.0 with 4 tasks resource profile 0\n",
            "25/04/04 16:43:49 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (10.255.255.254, executor driver, partition 1, PROCESS_LOCAL, 8979 bytes) \n",
            "25/04/04 16:43:49 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 2) (10.255.255.254, executor driver, partition 2, PROCESS_LOCAL, 8979 bytes) \n",
            "25/04/04 16:43:49 INFO TaskSetManager: Starting task 2.0 in stage 1.0 (TID 3) (10.255.255.254, executor driver, partition 3, PROCESS_LOCAL, 9015 bytes) \n",
            "25/04/04 16:43:49 INFO TaskSetManager: Starting task 3.0 in stage 1.0 (TID 4) (10.255.255.254, executor driver, partition 4, PROCESS_LOCAL, 8979 bytes) \n",
            "25/04/04 16:43:49 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)\n",
            "25/04/04 16:43:49 INFO Executor: Running task 1.0 in stage 1.0 (TID 2)\n",
            "25/04/04 16:43:49 INFO Executor: Running task 3.0 in stage 1.0 (TID 4)\n",
            "25/04/04 16:43:49 INFO Executor: Running task 2.0 in stage 1.0 (TID 3)\n",
            "25/04/04 16:43:49 INFO PythonRunner: Times: total = 185, boot = -81, init = 266, finish = 0\n",
            "25/04/04 16:43:49 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 1843 bytes result sent to driver\n",
            "25/04/04 16:43:49 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 209 ms on 10.255.255.254 (executor driver) (1/4)\n",
            "25/04/04 16:43:49 INFO PythonRunner: Times: total = 261, boot = 31, init = 229, finish = 1\n",
            "25/04/04 16:43:49 INFO Executor: Finished task 2.0 in stage 1.0 (TID 3). 1881 bytes result sent to driver\n",
            "25/04/04 16:43:49 INFO TaskSetManager: Finished task 2.0 in stage 1.0 (TID 3) in 290 ms on 10.255.255.254 (executor driver) (2/4)\n",
            "25/04/04 16:43:49 INFO PythonRunner: Times: total = 274, boot = 19, init = 255, finish = 0\n",
            "25/04/04 16:43:49 INFO PythonRunner: Times: total = 285, boot = 9, init = 275, finish = 1\n",
            "25/04/04 16:43:49 INFO Executor: Finished task 3.0 in stage 1.0 (TID 4). 1886 bytes result sent to driver\n",
            "25/04/04 16:43:49 INFO Executor: Finished task 1.0 in stage 1.0 (TID 2). 1886 bytes result sent to driver\n",
            "25/04/04 16:43:49 INFO TaskSetManager: Finished task 3.0 in stage 1.0 (TID 4) in 311 ms on 10.255.255.254 (executor driver) (3/4)\n",
            "25/04/04 16:43:49 INFO TaskSetManager: Finished task 1.0 in stage 1.0 (TID 2) in 316 ms on 10.255.255.254 (executor driver) (4/4)\n",
            "25/04/04 16:43:49 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool \n",
            "25/04/04 16:43:49 INFO DAGScheduler: ResultStage 1 (showString at NativeMethodAccessorImpl.java:0) finished in 0.330 s\n",
            "25/04/04 16:43:49 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/04/04 16:43:49 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished\n",
            "25/04/04 16:43:49 INFO DAGScheduler: Job 1 finished: showString at NativeMethodAccessorImpl.java:0, took 0.335755 s\n",
            "25/04/04 16:43:49 INFO SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0\n",
            "25/04/04 16:43:49 INFO DAGScheduler: Got job 2 (showString at NativeMethodAccessorImpl.java:0) with 3 output partitions\n",
            "25/04/04 16:43:49 INFO DAGScheduler: Final stage: ResultStage 2 (showString at NativeMethodAccessorImpl.java:0)\n",
            "25/04/04 16:43:49 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/04/04 16:43:49 INFO DAGScheduler: Missing parents: List()\n",
            "25/04/04 16:43:49 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[6] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
            "25/04/04 16:43:49 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 12.6 KiB, free 434.4 MiB)\n",
            "25/04/04 16:43:49 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 6.6 KiB, free 434.3 MiB)\n",
            "25/04/04 16:43:49 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 10.255.255.254:43641 (size: 6.6 KiB, free: 434.4 MiB)\n",
            "25/04/04 16:43:49 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1585\n",
            "25/04/04 16:43:49 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 2 (MapPartitionsRDD[6] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(5, 6, 7))\n",
            "25/04/04 16:43:49 INFO TaskSchedulerImpl: Adding task set 2.0 with 3 tasks resource profile 0\n",
            "25/04/04 16:43:49 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 5) (10.255.255.254, executor driver, partition 5, PROCESS_LOCAL, 8979 bytes) \n",
            "25/04/04 16:43:49 INFO TaskSetManager: Starting task 1.0 in stage 2.0 (TID 6) (10.255.255.254, executor driver, partition 6, PROCESS_LOCAL, 8979 bytes) \n",
            "25/04/04 16:43:49 INFO TaskSetManager: Starting task 2.0 in stage 2.0 (TID 7) (10.255.255.254, executor driver, partition 7, PROCESS_LOCAL, 9015 bytes) \n",
            "25/04/04 16:43:49 INFO Executor: Running task 0.0 in stage 2.0 (TID 5)\n",
            "25/04/04 16:43:49 INFO Executor: Running task 2.0 in stage 2.0 (TID 7)\n",
            "25/04/04 16:43:49 INFO Executor: Running task 1.0 in stage 2.0 (TID 6)\n",
            "25/04/04 16:43:49 INFO PythonRunner: Times: total = 163, boot = -164, init = 327, finish = 0\n",
            "25/04/04 16:43:49 INFO PythonRunner: Times: total = 163, boot = -74, init = 237, finish = 0\n",
            "25/04/04 16:43:49 INFO Executor: Finished task 0.0 in stage 2.0 (TID 5). 1843 bytes result sent to driver\n",
            "25/04/04 16:43:49 INFO Executor: Finished task 1.0 in stage 2.0 (TID 6). 1843 bytes result sent to driver\n",
            "25/04/04 16:43:49 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 5) in 185 ms on 10.255.255.254 (executor driver) (1/3)\n",
            "25/04/04 16:43:49 INFO TaskSetManager: Finished task 1.0 in stage 2.0 (TID 6) in 184 ms on 10.255.255.254 (executor driver) (2/3)\n",
            "25/04/04 16:43:49 INFO PythonRunner: Times: total = 175, boot = -60, init = 235, finish = 0\n",
            "25/04/04 16:43:49 INFO Executor: Finished task 2.0 in stage 2.0 (TID 7). 1881 bytes result sent to driver\n",
            "25/04/04 16:43:49 INFO TaskSetManager: Finished task 2.0 in stage 2.0 (TID 7) in 194 ms on 10.255.255.254 (executor driver) (3/3)\n",
            "25/04/04 16:43:49 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool \n",
            "25/04/04 16:43:49 INFO DAGScheduler: ResultStage 2 (showString at NativeMethodAccessorImpl.java:0) finished in 0.216 s\n",
            "25/04/04 16:43:49 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/04/04 16:43:49 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished\n",
            "25/04/04 16:43:49 INFO DAGScheduler: Job 2 finished: showString at NativeMethodAccessorImpl.java:0, took 0.225615 s\n",
            "25/04/04 16:43:50 INFO CodeGenerator: Code generated in 17.484724 ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---+----+\n",
            "| id|name|\n",
            "+---+----+\n",
            "|  1|John|\n",
            "|  2|Jane|\n",
            "+---+----+\n",
            "\n",
            "Testing S3 connection with parquet write...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "25/04/04 16:43:51 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/04/04 16:43:51 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/04/04 16:43:51 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/04/04 16:43:51 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/04/04 16:43:51 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/04/04 16:43:51 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/04/04 16:43:51 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/04/04 16:43:51 INFO CodeGenerator: Code generated in 13.370137 ms\n",
            "25/04/04 16:43:51 INFO SparkContext: Starting job: save at NativeMethodAccessorImpl.java:0\n",
            "25/04/04 16:43:51 INFO DAGScheduler: Got job 3 (save at NativeMethodAccessorImpl.java:0) with 8 output partitions\n",
            "25/04/04 16:43:51 INFO DAGScheduler: Final stage: ResultStage 3 (save at NativeMethodAccessorImpl.java:0)\n",
            "25/04/04 16:43:51 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/04/04 16:43:51 INFO DAGScheduler: Missing parents: List()\n",
            "25/04/04 16:43:51 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[8] at save at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
            "25/04/04 16:43:51 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 222.8 KiB, free 434.1 MiB)\n",
            "25/04/04 16:43:51 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 80.8 KiB, free 434.0 MiB)\n",
            "25/04/04 16:43:51 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 10.255.255.254:43641 (size: 80.8 KiB, free: 434.3 MiB)\n",
            "25/04/04 16:43:51 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1585\n",
            "25/04/04 16:43:51 INFO DAGScheduler: Submitting 8 missing tasks from ResultStage 3 (MapPartitionsRDD[8] at save at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7))\n",
            "25/04/04 16:43:51 INFO TaskSchedulerImpl: Adding task set 3.0 with 8 tasks resource profile 0\n",
            "25/04/04 16:43:51 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 8) (10.255.255.254, executor driver, partition 0, PROCESS_LOCAL, 8979 bytes) \n",
            "25/04/04 16:43:51 INFO TaskSetManager: Starting task 1.0 in stage 3.0 (TID 9) (10.255.255.254, executor driver, partition 1, PROCESS_LOCAL, 8979 bytes) \n",
            "25/04/04 16:43:51 INFO TaskSetManager: Starting task 2.0 in stage 3.0 (TID 10) (10.255.255.254, executor driver, partition 2, PROCESS_LOCAL, 8979 bytes) \n",
            "25/04/04 16:43:51 INFO TaskSetManager: Starting task 3.0 in stage 3.0 (TID 11) (10.255.255.254, executor driver, partition 3, PROCESS_LOCAL, 9015 bytes) \n",
            "25/04/04 16:43:51 INFO TaskSetManager: Starting task 4.0 in stage 3.0 (TID 12) (10.255.255.254, executor driver, partition 4, PROCESS_LOCAL, 8979 bytes) \n",
            "25/04/04 16:43:51 INFO TaskSetManager: Starting task 5.0 in stage 3.0 (TID 13) (10.255.255.254, executor driver, partition 5, PROCESS_LOCAL, 8979 bytes) \n",
            "25/04/04 16:43:51 INFO TaskSetManager: Starting task 6.0 in stage 3.0 (TID 14) (10.255.255.254, executor driver, partition 6, PROCESS_LOCAL, 8979 bytes) \n",
            "25/04/04 16:43:51 INFO TaskSetManager: Starting task 7.0 in stage 3.0 (TID 15) (10.255.255.254, executor driver, partition 7, PROCESS_LOCAL, 9015 bytes) \n",
            "25/04/04 16:43:51 INFO Executor: Running task 0.0 in stage 3.0 (TID 8)\n",
            "25/04/04 16:43:51 INFO Executor: Running task 2.0 in stage 3.0 (TID 10)\n",
            "25/04/04 16:43:51 INFO Executor: Running task 3.0 in stage 3.0 (TID 11)\n",
            "25/04/04 16:43:51 INFO Executor: Running task 1.0 in stage 3.0 (TID 9)\n",
            "25/04/04 16:43:51 INFO Executor: Running task 4.0 in stage 3.0 (TID 12)\n",
            "25/04/04 16:43:51 INFO Executor: Running task 5.0 in stage 3.0 (TID 13)\n",
            "25/04/04 16:43:51 INFO Executor: Running task 6.0 in stage 3.0 (TID 14)\n",
            "25/04/04 16:43:51 INFO Executor: Running task 7.0 in stage 3.0 (TID 15)\n",
            "25/04/04 16:43:51 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 10.255.255.254:43641 in memory (size: 6.6 KiB, free: 434.3 MiB)\n",
            "25/04/04 16:43:51 INFO CodeGenerator: Code generated in 33.574164 ms\n",
            "25/04/04 16:43:51 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/04/04 16:43:51 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/04/04 16:43:51 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/04/04 16:43:51 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/04/04 16:43:51 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/04/04 16:43:51 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/04/04 16:43:51 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 10.255.255.254:43641 in memory (size: 6.6 KiB, free: 434.3 MiB)\n",
            "25/04/04 16:43:51 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/04/04 16:43:51 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/04/04 16:43:51 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/04/04 16:43:51 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/04/04 16:43:51 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/04/04 16:43:51 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/04/04 16:43:51 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/04/04 16:43:51 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/04/04 16:43:51 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/04/04 16:43:51 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/04/04 16:43:51 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/04/04 16:43:51 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/04/04 16:43:51 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/04/04 16:43:51 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/04/04 16:43:51 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/04/04 16:43:51 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/04/04 16:43:52 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/04/04 16:43:52 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/04/04 16:43:52 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/04/04 16:43:52 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/04/04 16:43:52 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/04/04 16:43:52 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/04/04 16:43:52 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/04/04 16:43:52 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/04/04 16:43:52 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/04/04 16:43:52 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/04/04 16:43:52 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/04/04 16:43:52 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/04/04 16:43:52 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/04/04 16:43:52 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/04/04 16:43:52 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/04/04 16:43:52 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/04/04 16:43:52 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/04/04 16:43:52 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/04/04 16:43:52 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/04/04 16:43:52 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/04/04 16:43:52 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 10.255.255.254:43641 in memory (size: 6.6 KiB, free: 434.3 MiB)\n",
            "25/04/04 16:43:52 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/04/04 16:43:52 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/04/04 16:43:52 INFO CodecConfig: Compression: SNAPPY\n",
            "25/04/04 16:43:52 INFO PythonRunner: Times: total = 411, boot = -2133, init = 2544, finish = 0\n",
            "25/04/04 16:43:52 INFO CodecConfig: Compression: SNAPPY\n",
            "25/04/04 16:43:52 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/04/04 16:43:52 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/04/04 16:43:52 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/04/04 16:43:52 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/04/04 16:43:52 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202504041643516287367409870632176_0003_m_000005_13\n",
            "25/04/04 16:43:52 INFO PythonRunner: Times: total = 547, boot = -2122, init = 2668, finish = 1\n",
            "25/04/04 16:43:52 INFO CodecConfig: Compression: SNAPPY\n",
            "25/04/04 16:43:52 INFO CodecConfig: Compression: SNAPPY\n",
            "25/04/04 16:43:52 INFO ParquetOutputFormat: ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]\n",
            "25/04/04 16:43:52 INFO ParquetOutputFormat: ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]\n",
            "25/04/04 16:43:52 INFO Executor: Finished task 5.0 in stage 3.0 (TID 13). 2875 bytes result sent to driver\n",
            "25/04/04 16:43:52 INFO TaskSetManager: Finished task 5.0 in stage 3.0 (TID 13) in 821 ms on 10.255.255.254 (executor driver) (1/8)\n",
            "25/04/04 16:43:52 INFO PythonRunner: Times: total = 630, boot = 24, init = 606, finish = 0\n",
            "25/04/04 16:43:52 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202504041643516287367409870632176_0003_m_000004_12\n",
            "25/04/04 16:43:52 INFO Executor: Finished task 4.0 in stage 3.0 (TID 12). 2832 bytes result sent to driver\n",
            "25/04/04 16:43:52 INFO TaskSetManager: Finished task 4.0 in stage 3.0 (TID 12) in 865 ms on 10.255.255.254 (executor driver) (2/8)\n",
            "25/04/04 16:43:52 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:\n",
            "{\n",
            "  \"type\" : \"struct\",\n",
            "  \"fields\" : [ {\n",
            "    \"name\" : \"id\",\n",
            "    \"type\" : \"long\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"name\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  } ]\n",
            "}\n",
            "and corresponding Parquet message type:\n",
            "message spark_schema {\n",
            "  optional int64 id;\n",
            "  optional binary name (STRING);\n",
            "}\n",
            "\n",
            "       \n",
            "25/04/04 16:43:52 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:\n",
            "{\n",
            "  \"type\" : \"struct\",\n",
            "  \"fields\" : [ {\n",
            "    \"name\" : \"id\",\n",
            "    \"type\" : \"long\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"name\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  } ]\n",
            "}\n",
            "and corresponding Parquet message type:\n",
            "message spark_schema {\n",
            "  optional int64 id;\n",
            "  optional binary name (STRING);\n",
            "}\n",
            "\n",
            "       \n",
            "25/04/04 16:43:52 INFO PythonRunner: Times: total = 727, boot = -2128, init = 2855, finish = 0\n",
            "25/04/04 16:43:52 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202504041643516287367409870632176_0003_m_000006_14\n",
            "25/04/04 16:43:52 INFO Executor: Finished task 6.0 in stage 3.0 (TID 14). 2832 bytes result sent to driver\n",
            "25/04/04 16:43:52 INFO TaskSetManager: Finished task 6.0 in stage 3.0 (TID 14) in 936 ms on 10.255.255.254 (executor driver) (3/8)\n",
            "25/04/04 16:43:52 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202504041643516287367409870632176_0003_m_000002_10\n",
            "25/04/04 16:43:52 INFO Executor: Finished task 2.0 in stage 3.0 (TID 10). 2832 bytes result sent to driver\n",
            "25/04/04 16:43:52 INFO TaskSetManager: Finished task 2.0 in stage 3.0 (TID 10) in 987 ms on 10.255.255.254 (executor driver) (4/8)\n",
            "25/04/04 16:43:52 INFO CodecConfig: Compression: SNAPPY             (4 + 4) / 8]\n",
            "25/04/04 16:43:52 INFO CodecConfig: Compression: SNAPPY\n",
            "25/04/04 16:43:52 INFO ParquetOutputFormat: ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]\n",
            "25/04/04 16:43:52 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:\n",
            "{\n",
            "  \"type\" : \"struct\",\n",
            "  \"fields\" : [ {\n",
            "    \"name\" : \"id\",\n",
            "    \"type\" : \"long\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"name\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  } ]\n",
            "}\n",
            "and corresponding Parquet message type:\n",
            "message spark_schema {\n",
            "  optional int64 id;\n",
            "  optional binary name (STRING);\n",
            "}\n",
            "\n",
            "       \n",
            "25/04/04 16:43:52 INFO CodecPool: Got brand-new compressor [.snappy]\n",
            "25/04/04 16:43:52 INFO CodecPool: Got brand-new compressor [.snappy]\n",
            "25/04/04 16:43:52 INFO CodecPool: Got brand-new compressor [.snappy]\n",
            "25/04/04 16:43:52 INFO PythonRunner: Times: total = 1027, boot = 94, init = 933, finish = 0\n",
            "25/04/04 16:43:52 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202504041643516287367409870632176_0003_m_000001_9\n",
            "25/04/04 16:43:52 INFO Executor: Finished task 1.0 in stage 3.0 (TID 9). 2832 bytes result sent to driver\n",
            "25/04/04 16:43:52 INFO TaskSetManager: Finished task 1.0 in stage 3.0 (TID 9) in 1286 ms on 10.255.255.254 (executor driver) (5/8)\n",
            "25/04/04 16:43:53 INFO PythonRunner: Times: total = 975, boot = 65, init = 910, finish = 0\n",
            "25/04/04 16:43:53 INFO PythonRunner: Times: total = 571, boot = -2360, init = 2931, finish = 0\n",
            "25/04/04 16:43:53 INFO PythonRunner: Times: total = 964, boot = 38, init = 926, finish = 0\n",
            "25/04/04 16:43:53 INFO FileOutputCommitter: Saved output of task 'attempt_202504041643516287367409870632176_0003_m_000003_11' to s3a://wba/test.parquet/_temporary/0/task_202504041643516287367409870632176_0003_m_000003\n",
            "25/04/04 16:43:53 INFO SparkHadoopMapRedUtil: attempt_202504041643516287367409870632176_0003_m_000003_11: Committed. Elapsed time: 349 ms.\n",
            "25/04/04 16:43:53 INFO Executor: Finished task 3.0 in stage 3.0 (TID 11). 2918 bytes result sent to driver\n",
            "25/04/04 16:43:53 INFO TaskSetManager: Finished task 3.0 in stage 3.0 (TID 11) in 2311 ms on 10.255.255.254 (executor driver) (6/8)\n",
            "25/04/04 16:43:53 INFO FileOutputCommitter: Saved output of task 'attempt_202504041643516287367409870632176_0003_m_000000_8' to s3a://wba/test.parquet/_temporary/0/task_202504041643516287367409870632176_0003_m_000000\n",
            "25/04/04 16:43:53 INFO SparkHadoopMapRedUtil: attempt_202504041643516287367409870632176_0003_m_000000_8: Committed. Elapsed time: 345 ms.\n",
            "25/04/04 16:43:53 INFO Executor: Finished task 0.0 in stage 3.0 (TID 8). 2875 bytes result sent to driver\n",
            "25/04/04 16:43:53 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 8) in 2325 ms on 10.255.255.254 (executor driver) (7/8)\n",
            "25/04/04 16:43:53 INFO FileOutputCommitter: Saved output of task 'attempt_202504041643516287367409870632176_0003_m_000007_15' to s3a://wba/test.parquet/_temporary/0/task_202504041643516287367409870632176_0003_m_000007\n",
            "25/04/04 16:43:53 INFO SparkHadoopMapRedUtil: attempt_202504041643516287367409870632176_0003_m_000007_15: Committed. Elapsed time: 417 ms.\n",
            "25/04/04 16:43:53 INFO Executor: Finished task 7.0 in stage 3.0 (TID 15). 2918 bytes result sent to driver\n",
            "25/04/04 16:43:53 INFO TaskSetManager: Finished task 7.0 in stage 3.0 (TID 15) in 2356 ms on 10.255.255.254 (executor driver) (8/8)\n",
            "25/04/04 16:43:53 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool \n",
            "25/04/04 16:43:53 INFO DAGScheduler: ResultStage 3 (save at NativeMethodAccessorImpl.java:0) finished in 2.422 s\n",
            "25/04/04 16:43:53 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/04/04 16:43:53 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished\n",
            "25/04/04 16:43:53 INFO DAGScheduler: Job 3 finished: save at NativeMethodAccessorImpl.java:0, took 2.429549 s\n",
            "25/04/04 16:43:53 INFO FileFormatWriter: Start to commit write Job dc1001a9-99a0-434b-80f2-e58ce4015164.\n",
            "25/04/04 16:43:54 INFO FileFormatWriter: Write Job dc1001a9-99a0-434b-80f2-e58ce4015164 committed. Elapsed time: 586 ms.\n",
            "25/04/04 16:43:54 INFO FileFormatWriter: Finished processing stats for write job dc1001a9-99a0-434b-80f2-e58ce4015164.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "S3 connection successful\n",
            "Attempting to write Delta table...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "25/04/04 16:43:54 INFO DeltaLog: Creating initial snapshot without metadata, because the directory is empty\n",
            "25/04/04 16:43:55 INFO DummySnapshot: [tableId=e7c86550-2fbe-43e3-9fd1-7cbe08afdc0e] Created snapshot DummySnapshot(path=s3a://wba/example-table/_delta_log, version=-1, metadata=Metadata(3dae6760-3192-40e2-bfbe-de7fc05fde41,null,null,Format(parquet,Map()),null,List(),Map(),Some(1743803034998)), logSegment=LogSegment(s3a://wba/example-table/_delta_log,-1,List(),org.apache.spark.sql.delta.EmptyCheckpointProvider$@4dbc4421,-1), checksumOpt=None)\n",
            "25/04/04 16:43:55 INFO DeltaLog: Creating initial snapshot without metadata, because the directory is empty\n",
            "25/04/04 16:43:55 INFO DummySnapshot: [tableId=3dae6760-3192-40e2-bfbe-de7fc05fde41] Created snapshot DummySnapshot(path=s3a://wba/example-table/_delta_log, version=-1, metadata=Metadata(e5c5e22a-8438-442f-a077-67f4433e0980,null,null,Format(parquet,Map()),null,List(),Map(),Some(1743803035214)), logSegment=LogSegment(s3a://wba/example-table/_delta_log,-1,List(),org.apache.spark.sql.delta.EmptyCheckpointProvider$@4dbc4421,-1), checksumOpt=None)\n",
            "25/04/04 16:43:55 INFO OptimisticTransaction: [tableId=e5c5e22a,txnId=06e43a65] Updated metadata from - to Metadata(027ba8ff-d622-4f81-8ab9-6e03980ceab8,null,null,Format(parquet,Map()),{\"type\":\"struct\",\"fields\":[{\"name\":\"id\",\"type\":\"long\",\"nullable\":true,\"metadata\":{}},{\"name\":\"name\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}}]},List(),Map(delta.compatibility.symlinkFormatManifest.enabled -> false),Some(1743803035287))\n",
            "25/04/04 16:43:55 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/04/04 16:43:56 INFO SparkContext: Starting job: save at NativeMethodAccessorImpl.java:0\n",
            "25/04/04 16:43:56 INFO DAGScheduler: Got job 4 (save at NativeMethodAccessorImpl.java:0) with 8 output partitions\n",
            "25/04/04 16:43:56 INFO DAGScheduler: Final stage: ResultStage 4 (save at NativeMethodAccessorImpl.java:0)\n",
            "25/04/04 16:43:56 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/04/04 16:43:56 INFO DAGScheduler: Missing parents: List()\n",
            "25/04/04 16:43:56 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[9] at save at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
            "25/04/04 16:43:56 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 344.3 KiB, free 433.8 MiB)\n",
            "25/04/04 16:43:56 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 126.0 KiB, free 433.6 MiB)\n",
            "25/04/04 16:43:56 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 10.255.255.254:43641 (size: 126.0 KiB, free: 434.2 MiB)\n",
            "25/04/04 16:43:56 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1585\n",
            "25/04/04 16:43:56 INFO DAGScheduler: Submitting 8 missing tasks from ResultStage 4 (MapPartitionsRDD[9] at save at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7))\n",
            "25/04/04 16:43:56 INFO TaskSchedulerImpl: Adding task set 4.0 with 8 tasks resource profile 0\n",
            "25/04/04 16:43:56 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 16) (10.255.255.254, executor driver, partition 0, PROCESS_LOCAL, 8979 bytes) \n",
            "25/04/04 16:43:56 INFO TaskSetManager: Starting task 1.0 in stage 4.0 (TID 17) (10.255.255.254, executor driver, partition 1, PROCESS_LOCAL, 8979 bytes) \n",
            "25/04/04 16:43:56 INFO TaskSetManager: Starting task 2.0 in stage 4.0 (TID 18) (10.255.255.254, executor driver, partition 2, PROCESS_LOCAL, 8979 bytes) \n",
            "25/04/04 16:43:56 INFO TaskSetManager: Starting task 3.0 in stage 4.0 (TID 19) (10.255.255.254, executor driver, partition 3, PROCESS_LOCAL, 9015 bytes) \n",
            "25/04/04 16:43:56 INFO TaskSetManager: Starting task 4.0 in stage 4.0 (TID 20) (10.255.255.254, executor driver, partition 4, PROCESS_LOCAL, 8979 bytes) \n",
            "25/04/04 16:43:56 INFO TaskSetManager: Starting task 5.0 in stage 4.0 (TID 21) (10.255.255.254, executor driver, partition 5, PROCESS_LOCAL, 8979 bytes) \n",
            "25/04/04 16:43:56 INFO TaskSetManager: Starting task 6.0 in stage 4.0 (TID 22) (10.255.255.254, executor driver, partition 6, PROCESS_LOCAL, 8979 bytes) \n",
            "25/04/04 16:43:56 INFO TaskSetManager: Starting task 7.0 in stage 4.0 (TID 23) (10.255.255.254, executor driver, partition 7, PROCESS_LOCAL, 9015 bytes) \n",
            "25/04/04 16:43:56 INFO Executor: Running task 0.0 in stage 4.0 (TID 16)\n",
            "25/04/04 16:43:56 INFO Executor: Running task 1.0 in stage 4.0 (TID 17)\n",
            "25/04/04 16:43:56 INFO Executor: Running task 2.0 in stage 4.0 (TID 18)\n",
            "25/04/04 16:43:56 INFO Executor: Running task 4.0 in stage 4.0 (TID 20)\n",
            "25/04/04 16:43:56 INFO Executor: Running task 7.0 in stage 4.0 (TID 23)\n",
            "25/04/04 16:43:56 INFO Executor: Running task 6.0 in stage 4.0 (TID 22)\n",
            "25/04/04 16:43:56 INFO Executor: Running task 3.0 in stage 4.0 (TID 19)\n",
            "25/04/04 16:43:56 INFO Executor: Running task 5.0 in stage 4.0 (TID 21)\n",
            "25/04/04 16:43:56 INFO PythonRunner: Times: total = 331, boot = -3997, init = 4328, finish = 0\n",
            "25/04/04 16:43:56 INFO CodeGenerator: Code generated in 126.238987 ms\n",
            "25/04/04 16:43:56 INFO PythonRunner: Times: total = 473, boot = -3845, init = 4318, finish = 0\n",
            "25/04/04 16:43:56 INFO PythonRunner: Times: total = 478, boot = -3466, init = 3944, finish = 0\n",
            "25/04/04 16:43:56 INFO PythonRunner: Times: total = 495, boot = -3795, init = 4290, finish = 0\n",
            "25/04/04 16:43:56 INFO PythonRunner: Times: total = 573, boot = -3857, init = 4430, finish = 0\n",
            "25/04/04 16:43:56 INFO CodeGenerator: Code generated in 95.461071 ms\n",
            "25/04/04 16:43:56 INFO CodeGenerator: Code generated in 17.946204 ms\n",
            "25/04/04 16:43:56 INFO CodecConfig: Compression: SNAPPY\n",
            "25/04/04 16:43:56 INFO CodecConfig: Compression: SNAPPY\n",
            "25/04/04 16:43:56 INFO CodecConfig: Compression: SNAPPY\n",
            "25/04/04 16:43:56 INFO CodecConfig: Compression: SNAPPY\n",
            "25/04/04 16:43:56 INFO CodecConfig: Compression: SNAPPY\n",
            "25/04/04 16:43:56 INFO ParquetOutputFormat: ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]\n",
            "25/04/04 16:43:56 INFO ParquetOutputFormat: ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]\n",
            "25/04/04 16:43:56 INFO CodecConfig: Compression: SNAPPY\n",
            "25/04/04 16:43:56 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:\n",
            "{\n",
            "  \"type\" : \"struct\",\n",
            "  \"fields\" : [ {\n",
            "    \"name\" : \"id\",\n",
            "    \"type\" : \"long\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"name\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  } ]\n",
            "}\n",
            "and corresponding Parquet message type:\n",
            "message spark_schema {\n",
            "  optional int64 id;\n",
            "  optional binary name (STRING);\n",
            "}\n",
            "\n",
            "       \n",
            "25/04/04 16:43:56 INFO Executor: Finished task 4.0 in stage 4.0 (TID 20). 2945 bytes result sent to driver\n",
            "25/04/04 16:43:56 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:\n",
            "{\n",
            "  \"type\" : \"struct\",\n",
            "  \"fields\" : [ {\n",
            "    \"name\" : \"id\",\n",
            "    \"type\" : \"long\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"name\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  } ]\n",
            "}\n",
            "and corresponding Parquet message type:\n",
            "message spark_schema {\n",
            "  optional int64 id;\n",
            "  optional binary name (STRING);\n",
            "}\n",
            "\n",
            "       \n",
            "25/04/04 16:43:56 INFO Executor: Finished task 6.0 in stage 4.0 (TID 22). 2902 bytes result sent to driver\n",
            "25/04/04 16:43:56 INFO Executor: Finished task 2.0 in stage 4.0 (TID 18). 2902 bytes result sent to driver\n",
            "25/04/04 16:43:56 INFO Executor: Finished task 5.0 in stage 4.0 (TID 21). 2945 bytes result sent to driver\n",
            "25/04/04 16:43:56 INFO Executor: Finished task 1.0 in stage 4.0 (TID 17). 2902 bytes result sent to driver\n",
            "25/04/04 16:43:56 INFO ParquetOutputFormat: ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]\n",
            "25/04/04 16:43:56 INFO TaskSetManager: Finished task 6.0 in stage 4.0 (TID 22) in 881 ms on 10.255.255.254 (executor driver) (1/8)\n",
            "25/04/04 16:43:56 INFO TaskSetManager: Finished task 2.0 in stage 4.0 (TID 18) in 884 ms on 10.255.255.254 (executor driver) (2/8)\n",
            "25/04/04 16:43:56 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:\n",
            "{\n",
            "  \"type\" : \"struct\",\n",
            "  \"fields\" : [ {\n",
            "    \"name\" : \"id\",\n",
            "    \"type\" : \"long\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"name\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  } ]\n",
            "}\n",
            "and corresponding Parquet message type:\n",
            "message spark_schema {\n",
            "  optional int64 id;\n",
            "  optional binary name (STRING);\n",
            "}\n",
            "\n",
            "       \n",
            "25/04/04 16:43:56 INFO TaskSetManager: Finished task 5.0 in stage 4.0 (TID 21) in 884 ms on 10.255.255.254 (executor driver) (3/8)\n",
            "25/04/04 16:43:56 INFO TaskSetManager: Finished task 1.0 in stage 4.0 (TID 17) in 887 ms on 10.255.255.254 (executor driver) (4/8)\n",
            "25/04/04 16:43:56 INFO TaskSetManager: Finished task 4.0 in stage 4.0 (TID 20) in 887 ms on 10.255.255.254 (executor driver) (5/8)\n",
            "25/04/04 16:43:56 INFO PythonRunner: Times: total = 337, boot = -3395, init = 3731, finish = 1\n",
            "25/04/04 16:43:56 INFO PythonRunner: Times: total = 548, boot = -3463, init = 4011, finish = 0\n",
            "25/04/04 16:43:56 INFO PythonRunner: Times: total = 430, boot = -3717, init = 4147, finish = 0\n",
            "25/04/04 16:43:57 INFO CodeGenerator: Code generated in 17.518936 ms(5 + 3) / 8]\n",
            "25/04/04 16:43:57 INFO CodeGenerator: Code generated in 17.176412 ms\n",
            "25/04/04 16:43:57 INFO Executor: Finished task 3.0 in stage 4.0 (TID 19). 3851 bytes result sent to driver\n",
            "25/04/04 16:43:57 INFO Executor: Finished task 7.0 in stage 4.0 (TID 23). 3851 bytes result sent to driver\n",
            "25/04/04 16:43:57 INFO Executor: Finished task 0.0 in stage 4.0 (TID 16). 3753 bytes result sent to driver\n",
            "25/04/04 16:43:57 INFO TaskSetManager: Finished task 3.0 in stage 4.0 (TID 19) in 1098 ms on 10.255.255.254 (executor driver) (6/8)\n",
            "25/04/04 16:43:57 INFO TaskSetManager: Finished task 7.0 in stage 4.0 (TID 23) in 1095 ms on 10.255.255.254 (executor driver) (7/8)\n",
            "25/04/04 16:43:57 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 16) in 1101 ms on 10.255.255.254 (executor driver) (8/8)\n",
            "25/04/04 16:43:57 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool \n",
            "25/04/04 16:43:57 INFO DAGScheduler: ResultStage 4 (save at NativeMethodAccessorImpl.java:0) finished in 1.149 s\n",
            "25/04/04 16:43:57 INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/04/04 16:43:57 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished\n",
            "25/04/04 16:43:57 INFO DAGScheduler: Job 4 finished: save at NativeMethodAccessorImpl.java:0, took 1.155328 s\n",
            "25/04/04 16:43:57 INFO DeltaFileFormatWriter: Start to commit write Job 4933c0f2-1bc6-411a-8f30-2504dcd40f6f.\n",
            "25/04/04 16:43:57 INFO DeltaFileFormatWriter: Write Job 4933c0f2-1bc6-411a-8f30-2504dcd40f6f committed. Elapsed time: 1 ms.\n",
            "25/04/04 16:43:57 INFO DeltaFileFormatWriter: Finished processing stats for write job 4933c0f2-1bc6-411a-8f30-2504dcd40f6f.\n",
            "25/04/04 16:43:59 INFO CodeGenerator: Code generated in 434.166618 ms\n",
            "25/04/04 16:43:59 INFO SparkContext: Starting job: $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128\n",
            "25/04/04 16:43:59 INFO DAGScheduler: Job 5 finished: $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128, took 0.001161 s\n",
            "25/04/04 16:43:59 INFO OptimisticTransaction: [tableId=e5c5e22a,txnId=06e43a65] Attempting to commit version 0 with 5 actions with Serializable isolation level\n",
            "25/04/04 16:44:00 INFO OptimisticTransaction: Incremental commit: starting with snapshot version -1\n",
            "25/04/04 16:44:00 INFO CodeGenerator: Code generated in 214.601411 ms\n",
            "25/04/04 16:44:00 INFO SparkContext: Starting job: $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128\n",
            "25/04/04 16:44:00 INFO DAGScheduler: Job 6 finished: $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128, took 0.000323 s\n",
            "25/04/04 16:44:00 INFO DeltaLog: Creating a new snapshot v0 for commit version 0\n",
            "25/04/04 16:44:00 INFO DeltaLog: Loading version 0.\n",
            "25/04/04 16:44:00 INFO Snapshot: [tableId=e5c5e22a-8438-442f-a077-67f4433e0980] Created snapshot Snapshot(path=s3a://wba/example-table/_delta_log, version=0, metadata=Metadata(027ba8ff-d622-4f81-8ab9-6e03980ceab8,null,null,Format(parquet,Map()),{\"type\":\"struct\",\"fields\":[{\"name\":\"id\",\"type\":\"long\",\"nullable\":true,\"metadata\":{}},{\"name\":\"name\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}}]},List(),Map(delta.compatibility.symlinkFormatManifest.enabled -> false),Some(1743803035287)), logSegment=LogSegment(s3a://wba/example-table/_delta_log,0,List(S3AFileStatus{path=s3a://wba/example-table/_delta_log/00000000000000000000.json; isDirectory=false; length=1484; replication=1; blocksize=33554432; modification_time=1743803040000; access_time=0; owner=developer; group=developer; permission=rw-rw-rw-; isSymlink=false; hasAcl=false; isEncrypted=true; isErasureCoded=false} isEmptyDirectory=FALSE eTag=b2bfba830a2dc78d8d85c501b9d66547 versionId=null),org.apache.spark.sql.delta.EmptyCheckpointProvider$@4dbc4421,1743803040000), checksumOpt=Some(VersionChecksum(Some(06e43a65-4dd5-44a7-86d0-3d06662a6910),1424,2,None,None,1,1,None,Some(Stream()),Some(Stream()),Metadata(027ba8ff-d622-4f81-8ab9-6e03980ceab8,null,null,Format(parquet,Map()),{\"type\":\"struct\",\"fields\":[{\"name\":\"id\",\"type\":\"long\",\"nullable\":true,\"metadata\":{}},{\"name\":\"name\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}}]},List(),Map(delta.compatibility.symlinkFormatManifest.enabled -> false),Some(1743803035287)),Protocol(1,2),None,None,Some(Stream(AddFile(part-00007-7d2acd4f-83ba-40db-a8bd-8c46f564ae5b-c000.snappy.parquet,Map(),712,1743803037000,false,{\"numRecords\":1,\"minValues\":{\"id\":2,\"name\":\"Jane\"},\"maxValues\":{\"id\":2,\"name\":\"Jane\"},\"nullCount\":{\"id\":0,\"name\":0}},null,null,None,None,None), AddFile(part-00003-1373c84d-ff41-43d9-be23-5ca37994b068-c000.snappy.parquet,Map(),712,1743803037000,false,{\"numRecords\":1,\"minValues\":{\"id\":1,\"name\":\"John\"},\"maxValues\":{\"id\":1,\"name\":\"John\"},\"nullCount\":{\"id\":0,\"name\":0}},null,null,None,None,None))))))\n",
            "25/04/04 16:44:00 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 217.3 KiB, free 433.4 MiB)\n",
            "25/04/04 16:44:00 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 36.5 KiB, free 433.4 MiB)\n",
            "25/04/04 16:44:00 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 10.255.255.254:43641 (size: 36.5 KiB, free: 434.2 MiB)\n",
            "25/04/04 16:44:00 INFO SparkContext: Created broadcast 5 from $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128\n",
            "25/04/04 16:44:00 INFO DeltaLogFileIndex: Created DeltaLogFileIndex(JSON, numFilesInSegment: 1, totalFileSize: 1484)\n",
            "25/04/04 16:44:01 INFO DataSourceStrategy: Pruning directories with: \n",
            "25/04/04 16:44:01 INFO FileSourceStrategy: Pushed Filters: \n",
            "25/04/04 16:44:01 INFO FileSourceStrategy: Post-Scan Filters: \n",
            "25/04/04 16:44:01 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
            "25/04/04 16:44:01 INFO CodeGenerator: Code generated in 108.851493 ms\n",
            "25/04/04 16:44:01 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 217.6 KiB, free 433.2 MiB)\n",
            "25/04/04 16:44:01 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 36.6 KiB, free 433.1 MiB)\n",
            "25/04/04 16:44:01 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 10.255.255.254:43641 (size: 36.6 KiB, free: 434.1 MiB)\n",
            "25/04/04 16:44:01 INFO SparkContext: Created broadcast 6 from $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128\n",
            "25/04/04 16:44:01 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/04/04 16:44:01 INFO DAGScheduler: Registering RDD 21 ($anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128) as input to shuffle 0\n",
            "25/04/04 16:44:01 INFO DAGScheduler: Got map stage job 7 ($anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128) with 1 output partitions\n",
            "25/04/04 16:44:01 INFO DAGScheduler: Final stage: ShuffleMapStage 5 ($anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128)\n",
            "25/04/04 16:44:01 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/04/04 16:44:01 INFO DAGScheduler: Missing parents: List()\n",
            "25/04/04 16:44:01 INFO DAGScheduler: Submitting ShuffleMapStage 5 (MapPartitionsRDD[21] at $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128), which has no missing parents\n",
            "25/04/04 16:44:01 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 105.9 KiB, free 433.0 MiB)\n",
            "25/04/04 16:44:01 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 32.7 KiB, free 433.0 MiB)\n",
            "25/04/04 16:44:01 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 10.255.255.254:43641 (size: 32.7 KiB, free: 434.1 MiB)\n",
            "25/04/04 16:44:01 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1585\n",
            "25/04/04 16:44:01 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 5 (MapPartitionsRDD[21] at $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128) (first 15 tasks are for partitions Vector(0))\n",
            "25/04/04 16:44:01 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks resource profile 0\n",
            "25/04/04 16:44:01 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 24) (10.255.255.254, executor driver, partition 0, PROCESS_LOCAL, 9662 bytes) \n",
            "25/04/04 16:44:01 INFO Executor: Running task 0.0 in stage 5.0 (TID 24)\n",
            "25/04/04 16:44:01 INFO CodeGenerator: Code generated in 62.114456 ms\n",
            "25/04/04 16:44:01 INFO CodeGenerator: Code generated in 8.646856 ms\n",
            "25/04/04 16:44:01 INFO FileScanRDD: Reading File path: s3a://wba/example-table/_delta_log/00000000000000000000.json, range: 0-1484, partition values: [0]\n",
            "25/04/04 16:44:02 INFO CodeGenerator: Code generated in 56.096879 ms\n",
            "25/04/04 16:44:02 INFO Executor: Finished task 0.0 in stage 5.0 (TID 24). 1924 bytes result sent to driver\n",
            "25/04/04 16:44:02 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 24) in 373 ms on 10.255.255.254 (executor driver) (1/1)\n",
            "25/04/04 16:44:02 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool \n",
            "25/04/04 16:44:02 INFO DAGScheduler: ShuffleMapStage 5 ($anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128) finished in 0.426 s\n",
            "25/04/04 16:44:02 INFO DAGScheduler: looking for newly runnable stages\n",
            "25/04/04 16:44:02 INFO DAGScheduler: running: Set()\n",
            "25/04/04 16:44:02 INFO DAGScheduler: waiting: Set()\n",
            "25/04/04 16:44:02 INFO DAGScheduler: failed: Set()\n",
            "25/04/04 16:44:02 INFO BlockManagerInfo: Removed broadcast_7_piece0 on 10.255.255.254:43641 in memory (size: 32.7 KiB, free: 434.1 MiB)\n",
            "25/04/04 16:44:02 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 10.255.255.254:43641 in memory (size: 80.8 KiB, free: 434.2 MiB)\n",
            "25/04/04 16:44:02 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 10.255.255.254:43641 in memory (size: 126.0 KiB, free: 434.3 MiB)\n",
            "25/04/04 16:44:02 INFO CodeGenerator: Generated method too long to be JIT compiled: org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.serializefromobject_doConsume_0$ is 25072 bytes\n",
            "25/04/04 16:44:02 INFO CodeGenerator: Code generated in 462.117598 ms\n",
            "25/04/04 16:44:02 INFO CodeGenerator: Code generated in 68.473921 ms\n",
            "25/04/04 16:44:03 INFO SparkContext: Starting job: $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128\n",
            "25/04/04 16:44:03 INFO DAGScheduler: Got job 8 ($anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128) with 50 output partitions\n",
            "25/04/04 16:44:03 INFO DAGScheduler: Final stage: ResultStage 7 ($anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128)\n",
            "25/04/04 16:44:03 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 6)\n",
            "25/04/04 16:44:03 INFO DAGScheduler: Missing parents: List()\n",
            "25/04/04 16:44:03 INFO DAGScheduler: Submitting ResultStage 7 (MapPartitionsRDD[30] at $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128), which has no missing parents\n",
            "25/04/04 16:44:03 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 688.3 KiB, free 433.2 MiB)\n",
            "25/04/04 16:44:03 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 155.3 KiB, free 433.1 MiB)\n",
            "25/04/04 16:44:03 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 10.255.255.254:43641 (size: 155.3 KiB, free: 434.2 MiB)\n",
            "25/04/04 16:44:03 INFO SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1585\n",
            "25/04/04 16:44:03 INFO DAGScheduler: Submitting 50 missing tasks from ResultStage 7 (MapPartitionsRDD[30] at $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))\n",
            "25/04/04 16:44:03 INFO TaskSchedulerImpl: Adding task set 7.0 with 50 tasks resource profile 0\n",
            "25/04/04 16:44:03 INFO TaskSetManager: Starting task 14.0 in stage 7.0 (TID 25) (10.255.255.254, executor driver, partition 14, NODE_LOCAL, 8999 bytes) \n",
            "25/04/04 16:44:03 INFO TaskSetManager: Starting task 25.0 in stage 7.0 (TID 26) (10.255.255.254, executor driver, partition 25, NODE_LOCAL, 8999 bytes) \n",
            "25/04/04 16:44:03 INFO TaskSetManager: Starting task 42.0 in stage 7.0 (TID 27) (10.255.255.254, executor driver, partition 42, NODE_LOCAL, 8999 bytes) \n",
            "25/04/04 16:44:03 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 28) (10.255.255.254, executor driver, partition 0, PROCESS_LOCAL, 8999 bytes) \n",
            "25/04/04 16:44:03 INFO TaskSetManager: Starting task 1.0 in stage 7.0 (TID 29) (10.255.255.254, executor driver, partition 1, PROCESS_LOCAL, 8999 bytes) \n",
            "25/04/04 16:44:03 INFO TaskSetManager: Starting task 2.0 in stage 7.0 (TID 30) (10.255.255.254, executor driver, partition 2, PROCESS_LOCAL, 8999 bytes) \n",
            "25/04/04 16:44:03 INFO TaskSetManager: Starting task 3.0 in stage 7.0 (TID 31) (10.255.255.254, executor driver, partition 3, PROCESS_LOCAL, 8999 bytes) \n",
            "25/04/04 16:44:03 INFO TaskSetManager: Starting task 4.0 in stage 7.0 (TID 32) (10.255.255.254, executor driver, partition 4, PROCESS_LOCAL, 8999 bytes) \n",
            "25/04/04 16:44:03 INFO Executor: Running task 42.0 in stage 7.0 (TID 27)\n",
            "25/04/04 16:44:03 INFO Executor: Running task 14.0 in stage 7.0 (TID 25)\n",
            "25/04/04 16:44:03 INFO Executor: Running task 25.0 in stage 7.0 (TID 26)\n",
            "25/04/04 16:44:03 INFO Executor: Running task 0.0 in stage 7.0 (TID 28)\n",
            "25/04/04 16:44:03 INFO Executor: Running task 2.0 in stage 7.0 (TID 30)\n",
            "25/04/04 16:44:03 INFO Executor: Running task 4.0 in stage 7.0 (TID 32)\n",
            "25/04/04 16:44:03 INFO Executor: Running task 1.0 in stage 7.0 (TID 29)\n",
            "25/04/04 16:44:03 INFO Executor: Running task 3.0 in stage 7.0 (TID 31)\n",
            "25/04/04 16:44:03 INFO ShuffleBlockFetcherIterator: Getting 1 (276.0 B) non-empty blocks including 1 (276.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "25/04/04 16:44:03 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "25/04/04 16:44:03 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "25/04/04 16:44:03 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "25/04/04 16:44:03 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "25/04/04 16:44:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 21 ms\n",
            "25/04/04 16:44:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 21 ms\n",
            "25/04/04 16:44:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 21 ms\n",
            "25/04/04 16:44:03 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "25/04/04 16:44:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 22 ms\n",
            "25/04/04 16:44:03 INFO ShuffleBlockFetcherIterator: Getting 1 (789.0 B) non-empty blocks including 1 (789.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "25/04/04 16:44:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 23 ms\n",
            "25/04/04 16:44:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 23 ms\n",
            "25/04/04 16:44:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 23 ms\n",
            "25/04/04 16:44:03 INFO ShuffleBlockFetcherIterator: Getting 1 (276.0 B) non-empty blocks including 1 (276.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "25/04/04 16:44:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 34 ms\n",
            "25/04/04 16:44:03 INFO CodeGenerator: Code generated in 105.50737 ms\n",
            "25/04/04 16:44:03 INFO CodeGenerator: Code generated in 14.569405 ms\n",
            "25/04/04 16:44:03 INFO CodeGenerator: Code generated in 6.121695 ms\n",
            "25/04/04 16:44:04 INFO CodeGenerator: Code generated in 155.222065 ms + 8) / 50]\n",
            "25/04/04 16:44:04 INFO CodeGenerator: Generated method too long to be JIT compiled: org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.serializefromobject_doConsume_0$ is 25072 bytes\n",
            "25/04/04 16:44:04 INFO CodeGenerator: Code generated in 445.574087 ms\n",
            "25/04/04 16:44:04 INFO MemoryStore: Block rdd_28_1 stored as values in memory (estimated size 46.0 B, free 433.1 MiB)\n",
            "25/04/04 16:44:04 INFO MemoryStore: Block rdd_28_0 stored as values in memory (estimated size 46.0 B, free 433.1 MiB)\n",
            "25/04/04 16:44:04 INFO MemoryStore: Block rdd_28_3 stored as values in memory (estimated size 46.0 B, free 433.1 MiB)\n",
            "25/04/04 16:44:04 INFO MemoryStore: Block rdd_28_4 stored as values in memory (estimated size 46.0 B, free 433.1 MiB)\n",
            "25/04/04 16:44:04 INFO BlockManagerInfo: Added rdd_28_1 in memory on 10.255.255.254:43641 (size: 46.0 B, free: 434.2 MiB)\n",
            "25/04/04 16:44:04 INFO MemoryStore: Block rdd_28_42 stored as values in memory (estimated size 535.0 B, free 433.1 MiB)\n",
            "25/04/04 16:44:04 INFO MemoryStore: Block rdd_28_14 stored as values in memory (estimated size 392.0 B, free 433.1 MiB)\n",
            "25/04/04 16:44:04 INFO MemoryStore: Block rdd_28_25 stored as values in memory (estimated size 393.0 B, free 433.1 MiB)\n",
            "25/04/04 16:44:04 INFO BlockManagerInfo: Added rdd_28_0 in memory on 10.255.255.254:43641 (size: 46.0 B, free: 434.2 MiB)\n",
            "25/04/04 16:44:04 INFO MemoryStore: Block rdd_28_2 stored as values in memory (estimated size 46.0 B, free 433.1 MiB)\n",
            "25/04/04 16:44:04 INFO BlockManagerInfo: Added rdd_28_42 in memory on 10.255.255.254:43641 (size: 535.0 B, free: 434.2 MiB)\n",
            "25/04/04 16:44:04 INFO BlockManagerInfo: Added rdd_28_14 in memory on 10.255.255.254:43641 (size: 392.0 B, free: 434.2 MiB)\n",
            "25/04/04 16:44:04 INFO BlockManagerInfo: Added rdd_28_25 in memory on 10.255.255.254:43641 (size: 393.0 B, free: 434.2 MiB)\n",
            "25/04/04 16:44:04 INFO BlockManagerInfo: Added rdd_28_2 in memory on 10.255.255.254:43641 (size: 46.0 B, free: 434.2 MiB)\n",
            "25/04/04 16:44:04 INFO BlockManagerInfo: Added rdd_28_3 in memory on 10.255.255.254:43641 (size: 46.0 B, free: 434.2 MiB)\n",
            "25/04/04 16:44:04 INFO BlockManagerInfo: Added rdd_28_4 in memory on 10.255.255.254:43641 (size: 46.0 B, free: 434.2 MiB)\n",
            "25/04/04 16:44:04 INFO CodeGenerator: Code generated in 115.196504 ms\n",
            "25/04/04 16:44:04 INFO Executor: Finished task 3.0 in stage 7.0 (TID 31). 4955 bytes result sent to driver\n",
            "25/04/04 16:44:04 INFO Executor: Finished task 4.0 in stage 7.0 (TID 32). 4955 bytes result sent to driver\n",
            "25/04/04 16:44:04 INFO TaskSetManager: Starting task 5.0 in stage 7.0 (TID 33) (10.255.255.254, executor driver, partition 5, PROCESS_LOCAL, 8999 bytes) \n",
            "25/04/04 16:44:04 INFO Executor: Finished task 25.0 in stage 7.0 (TID 26). 5179 bytes result sent to driver\n",
            "25/04/04 16:44:04 INFO Executor: Finished task 2.0 in stage 7.0 (TID 30). 4955 bytes result sent to driver\n",
            "25/04/04 16:44:04 INFO TaskSetManager: Starting task 6.0 in stage 7.0 (TID 34) (10.255.255.254, executor driver, partition 6, PROCESS_LOCAL, 8999 bytes) \n",
            "25/04/04 16:44:04 INFO Executor: Finished task 42.0 in stage 7.0 (TID 27). 4998 bytes result sent to driver\n",
            "25/04/04 16:44:04 INFO Executor: Finished task 0.0 in stage 7.0 (TID 28). 4955 bytes result sent to driver\n",
            "25/04/04 16:44:04 INFO Executor: Running task 6.0 in stage 7.0 (TID 34)\n",
            "25/04/04 16:44:04 INFO TaskSetManager: Starting task 7.0 in stage 7.0 (TID 35) (10.255.255.254, executor driver, partition 7, PROCESS_LOCAL, 8999 bytes) \n",
            "25/04/04 16:44:04 INFO Executor: Running task 7.0 in stage 7.0 (TID 35)\n",
            "25/04/04 16:44:04 INFO Executor: Finished task 14.0 in stage 7.0 (TID 25). 5178 bytes result sent to driver\n",
            "25/04/04 16:44:04 INFO TaskSetManager: Starting task 8.0 in stage 7.0 (TID 36) (10.255.255.254, executor driver, partition 8, PROCESS_LOCAL, 8999 bytes) \n",
            "25/04/04 16:44:04 INFO Executor: Running task 8.0 in stage 7.0 (TID 36)\n",
            "25/04/04 16:44:04 INFO Executor: Finished task 1.0 in stage 7.0 (TID 29). 4998 bytes result sent to driver\n",
            "25/04/04 16:44:04 INFO Executor: Running task 5.0 in stage 7.0 (TID 33)\n",
            "25/04/04 16:44:04 INFO TaskSetManager: Starting task 9.0 in stage 7.0 (TID 37) (10.255.255.254, executor driver, partition 9, PROCESS_LOCAL, 8999 bytes) \n",
            "25/04/04 16:44:04 INFO TaskSetManager: Starting task 10.0 in stage 7.0 (TID 38) (10.255.255.254, executor driver, partition 10, PROCESS_LOCAL, 8999 bytes) \n",
            "25/04/04 16:44:04 INFO TaskSetManager: Finished task 4.0 in stage 7.0 (TID 32) in 1275 ms on 10.255.255.254 (executor driver) (1/50)\n",
            "25/04/04 16:44:04 INFO Executor: Running task 10.0 in stage 7.0 (TID 38)\n",
            "25/04/04 16:44:04 INFO TaskSetManager: Finished task 42.0 in stage 7.0 (TID 27) in 1281 ms on 10.255.255.254 (executor driver) (2/50)\n",
            "25/04/04 16:44:04 INFO Executor: Running task 9.0 in stage 7.0 (TID 37)\n",
            "25/04/04 16:44:04 INFO TaskSetManager: Finished task 2.0 in stage 7.0 (TID 30) in 1280 ms on 10.255.255.254 (executor driver) (3/50)\n",
            "25/04/04 16:44:04 INFO TaskSetManager: Finished task 25.0 in stage 7.0 (TID 26) in 1283 ms on 10.255.255.254 (executor driver) (4/50)\n",
            "25/04/04 16:44:04 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 28) in 1284 ms on 10.255.255.254 (executor driver) (5/50)\n",
            "25/04/04 16:44:04 INFO TaskSetManager: Finished task 3.0 in stage 7.0 (TID 31) in 1283 ms on 10.255.255.254 (executor driver) (6/50)\n",
            "25/04/04 16:44:04 INFO TaskSetManager: Starting task 11.0 in stage 7.0 (TID 39) (10.255.255.254, executor driver, partition 11, PROCESS_LOCAL, 8999 bytes) \n",
            "25/04/04 16:44:04 INFO Executor: Running task 11.0 in stage 7.0 (TID 39)\n",
            "25/04/04 16:44:04 INFO TaskSetManager: Starting task 12.0 in stage 7.0 (TID 40) (10.255.255.254, executor driver, partition 12, PROCESS_LOCAL, 8999 bytes) \n",
            "25/04/04 16:44:04 INFO TaskSetManager: Finished task 14.0 in stage 7.0 (TID 25) in 1302 ms on 10.255.255.254 (executor driver) (7/50)\n",
            "25/04/04 16:44:04 INFO TaskSetManager: Finished task 1.0 in stage 7.0 (TID 29) in 1297 ms on 10.255.255.254 (executor driver) (8/50)\n",
            "25/04/04 16:44:04 INFO Executor: Running task 12.0 in stage 7.0 (TID 40)\n",
            "25/04/04 16:44:04 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "25/04/04 16:44:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\n",
            "25/04/04 16:44:04 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "25/04/04 16:44:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
            "25/04/04 16:44:04 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "25/04/04 16:44:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
            "25/04/04 16:44:04 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "25/04/04 16:44:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
            "25/04/04 16:44:04 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "25/04/04 16:44:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\n",
            "25/04/04 16:44:04 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "25/04/04 16:44:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\n",
            "25/04/04 16:44:04 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "25/04/04 16:44:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
            "25/04/04 16:44:04 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "25/04/04 16:44:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
            "25/04/04 16:44:04 INFO MemoryStore: Block rdd_28_6 stored as values in memory (estimated size 46.0 B, free 432.6 MiB)\n",
            "25/04/04 16:44:04 INFO BlockManagerInfo: Added rdd_28_6 in memory on 10.255.255.254:43641 (size: 46.0 B, free: 434.2 MiB)\n",
            "25/04/04 16:44:04 INFO Executor: Finished task 6.0 in stage 7.0 (TID 34). 4955 bytes result sent to driver\n",
            "25/04/04 16:44:04 INFO TaskSetManager: Starting task 13.0 in stage 7.0 (TID 41) (10.255.255.254, executor driver, partition 13, PROCESS_LOCAL, 8999 bytes) \n",
            "25/04/04 16:44:04 INFO TaskSetManager: Finished task 6.0 in stage 7.0 (TID 34) in 169 ms on 10.255.255.254 (executor driver) (9/50)\n",
            "25/04/04 16:44:04 INFO MemoryStore: Block rdd_28_8 stored as values in memory (estimated size 46.0 B, free 432.7 MiB)\n",
            "25/04/04 16:44:04 INFO BlockManagerInfo: Added rdd_28_8 in memory on 10.255.255.254:43641 (size: 46.0 B, free: 434.2 MiB)\n",
            "25/04/04 16:44:04 INFO Executor: Finished task 8.0 in stage 7.0 (TID 36). 4955 bytes result sent to driver\n",
            "25/04/04 16:44:04 INFO Executor: Running task 13.0 in stage 7.0 (TID 41)\n",
            "25/04/04 16:44:04 INFO TaskSetManager: Starting task 15.0 in stage 7.0 (TID 42) (10.255.255.254, executor driver, partition 15, PROCESS_LOCAL, 8999 bytes) \n",
            "25/04/04 16:44:04 INFO Executor: Running task 15.0 in stage 7.0 (TID 42)\n",
            "25/04/04 16:44:04 INFO TaskSetManager: Finished task 8.0 in stage 7.0 (TID 36) in 192 ms on 10.255.255.254 (executor driver) (10/50)\n",
            "25/04/04 16:44:04 INFO MemoryStore: Block rdd_28_7 stored as values in memory (estimated size 46.0 B, free 432.8 MiB)\n",
            "25/04/04 16:44:04 INFO BlockManagerInfo: Added rdd_28_7 in memory on 10.255.255.254:43641 (size: 46.0 B, free: 434.2 MiB)\n",
            "25/04/04 16:44:04 INFO MemoryStore: Block rdd_28_5 stored as values in memory (estimated size 46.0 B, free 432.9 MiB)\n",
            "25/04/04 16:44:04 INFO BlockManagerInfo: Added rdd_28_5 in memory on 10.255.255.254:43641 (size: 46.0 B, free: 434.2 MiB)\n",
            "25/04/04 16:44:04 INFO MemoryStore: Block rdd_28_12 stored as values in memory (estimated size 46.0 B, free 432.8 MiB)\n",
            "25/04/04 16:44:04 INFO Executor: Finished task 7.0 in stage 7.0 (TID 35). 4955 bytes result sent to driver\n",
            "25/04/04 16:44:04 INFO BlockManagerInfo: Added rdd_28_12 in memory on 10.255.255.254:43641 (size: 46.0 B, free: 434.2 MiB)\n",
            "25/04/04 16:44:04 INFO Executor: Finished task 5.0 in stage 7.0 (TID 33). 4955 bytes result sent to driver\n",
            "25/04/04 16:44:04 INFO TaskSetManager: Starting task 16.0 in stage 7.0 (TID 43) (10.255.255.254, executor driver, partition 16, PROCESS_LOCAL, 8999 bytes) \n",
            "25/04/04 16:44:04 INFO Executor: Finished task 12.0 in stage 7.0 (TID 40). 4955 bytes result sent to driver\n",
            "25/04/04 16:44:04 INFO TaskSetManager: Starting task 17.0 in stage 7.0 (TID 44) (10.255.255.254, executor driver, partition 17, PROCESS_LOCAL, 8999 bytes) \n",
            "25/04/04 16:44:04 INFO Executor: Running task 17.0 in stage 7.0 (TID 44)\n",
            "25/04/04 16:44:04 INFO Executor: Running task 16.0 in stage 7.0 (TID 43)\n",
            "25/04/04 16:44:04 INFO MemoryStore: Block rdd_28_11 stored as values in memory (estimated size 46.0 B, free 433.0 MiB)\n",
            "25/04/04 16:44:04 INFO BlockManagerInfo: Added rdd_28_11 in memory on 10.255.255.254:43641 (size: 46.0 B, free: 434.2 MiB)\n",
            "25/04/04 16:44:04 INFO TaskSetManager: Starting task 18.0 in stage 7.0 (TID 45) (10.255.255.254, executor driver, partition 18, PROCESS_LOCAL, 8999 bytes) \n",
            "25/04/04 16:44:04 INFO Executor: Finished task 11.0 in stage 7.0 (TID 39). 4955 bytes result sent to driver\n",
            "25/04/04 16:44:04 INFO TaskSetManager: Finished task 5.0 in stage 7.0 (TID 33) in 249 ms on 10.255.255.254 (executor driver) (11/50)\n",
            "25/04/04 16:44:04 INFO TaskSetManager: Starting task 19.0 in stage 7.0 (TID 46) (10.255.255.254, executor driver, partition 19, PROCESS_LOCAL, 8999 bytes) \n",
            "25/04/04 16:44:04 INFO Executor: Running task 19.0 in stage 7.0 (TID 46)\n",
            "25/04/04 16:44:04 INFO Executor: Running task 18.0 in stage 7.0 (TID 45)\n",
            "25/04/04 16:44:04 INFO TaskSetManager: Finished task 12.0 in stage 7.0 (TID 40) in 246 ms on 10.255.255.254 (executor driver) (12/50)\n",
            "25/04/04 16:44:04 INFO TaskSetManager: Finished task 7.0 in stage 7.0 (TID 35) in 275 ms on 10.255.255.254 (executor driver) (13/50)\n",
            "25/04/04 16:44:04 INFO TaskSetManager: Finished task 11.0 in stage 7.0 (TID 39) in 259 ms on 10.255.255.254 (executor driver) (14/50)\n",
            "25/04/04 16:44:04 INFO MemoryStore: Block rdd_28_10 stored as values in memory (estimated size 46.0 B, free 433.0 MiB)\n",
            "25/04/04 16:44:04 INFO BlockManagerInfo: Added rdd_28_10 in memory on 10.255.255.254:43641 (size: 46.0 B, free: 434.2 MiB)\n",
            "25/04/04 16:44:04 INFO Executor: Finished task 10.0 in stage 7.0 (TID 38). 4955 bytes result sent to driver\n",
            "25/04/04 16:44:04 INFO TaskSetManager: Starting task 20.0 in stage 7.0 (TID 47) (10.255.255.254, executor driver, partition 20, PROCESS_LOCAL, 8999 bytes) \n",
            "25/04/04 16:44:04 INFO TaskSetManager: Finished task 10.0 in stage 7.0 (TID 38) in 318 ms on 10.255.255.254 (executor driver) (15/50)\n",
            "25/04/04 16:44:04 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "25/04/04 16:44:04 INFO Executor: Running task 20.0 in stage 7.0 (TID 47)\n",
            "25/04/04 16:44:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 24 ms\n",
            "25/04/04 16:44:04 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "25/04/04 16:44:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 54 ms\n",
            "25/04/04 16:44:04 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "25/04/04 16:44:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 18 ms\n",
            "25/04/04 16:44:04 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "25/04/04 16:44:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
            "25/04/04 16:44:04 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "25/04/04 16:44:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
            "25/04/04 16:44:05 INFO MemoryStore: Block rdd_28_9 stored as values in memory (estimated size 46.0 B, free 432.8 MiB)\n",
            "25/04/04 16:44:05 INFO BlockManagerInfo: Added rdd_28_9 in memory on 10.255.255.254:43641 (size: 46.0 B, free: 434.2 MiB)\n",
            "25/04/04 16:44:05 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "25/04/04 16:44:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
            "25/04/04 16:44:05 INFO Executor: Finished task 9.0 in stage 7.0 (TID 37). 4955 bytes result sent to driver\n",
            "25/04/04 16:44:05 INFO TaskSetManager: Starting task 21.0 in stage 7.0 (TID 48) (10.255.255.254, executor driver, partition 21, PROCESS_LOCAL, 8999 bytes) \n",
            "25/04/04 16:44:05 INFO TaskSetManager: Finished task 9.0 in stage 7.0 (TID 37) in 377 ms on 10.255.255.254 (executor driver) (16/50)\n",
            "25/04/04 16:44:05 INFO Executor: Running task 21.0 in stage 7.0 (TID 48)\n",
            "25/04/04 16:44:05 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "25/04/04 16:44:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\n",
            "25/04/04 16:44:05 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "25/04/04 16:44:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
            "25/04/04 16:44:05 INFO MemoryStore: Block rdd_28_17 stored as values in memory (estimated size 46.0 B, free 432.6 MiB)\n",
            "25/04/04 16:44:05 INFO BlockManagerInfo: Added rdd_28_17 in memory on 10.255.255.254:43641 (size: 46.0 B, free: 434.2 MiB)\n",
            "25/04/04 16:44:05 INFO Executor: Finished task 17.0 in stage 7.0 (TID 44). 4912 bytes result sent to driver\n",
            "25/04/04 16:44:05 INFO TaskSetManager: Starting task 22.0 in stage 7.0 (TID 49) (10.255.255.254, executor driver, partition 22, PROCESS_LOCAL, 8999 bytes) \n",
            "25/04/04 16:44:05 INFO TaskSetManager: Finished task 17.0 in stage 7.0 (TID 44) in 239 ms on 10.255.255.254 (executor driver) (17/50)\n",
            "25/04/04 16:44:05 INFO Executor: Running task 22.0 in stage 7.0 (TID 49)\n",
            "25/04/04 16:44:05 INFO MemoryStore: Block rdd_28_13 stored as values in memory (estimated size 46.0 B, free 432.7 MiB)\n",
            "25/04/04 16:44:05 INFO BlockManagerInfo: Added rdd_28_13 in memory on 10.255.255.254:43641 (size: 46.0 B, free: 434.2 MiB)\n",
            "25/04/04 16:44:05 INFO Executor: Finished task 13.0 in stage 7.0 (TID 41). 4912 bytes result sent to driver\n",
            "25/04/04 16:44:05 INFO TaskSetManager: Starting task 23.0 in stage 7.0 (TID 50) (10.255.255.254, executor driver, partition 23, PROCESS_LOCAL, 8999 bytes) \n",
            "25/04/04 16:44:05 INFO TaskSetManager: Finished task 13.0 in stage 7.0 (TID 41) in 324 ms on 10.255.255.254 (executor driver) (18/50)\n",
            "25/04/04 16:44:05 INFO MemoryStore: Block rdd_28_19 stored as values in memory (estimated size 46.0 B, free 432.8 MiB)\n",
            "25/04/04 16:44:05 INFO BlockManagerInfo: Added rdd_28_19 in memory on 10.255.255.254:43641 (size: 46.0 B, free: 434.2 MiB)\n",
            "25/04/04 16:44:05 INFO MemoryStore: Block rdd_28_16 stored as values in memory (estimated size 46.0 B, free 432.8 MiB)\n",
            "25/04/04 16:44:05 INFO BlockManagerInfo: Added rdd_28_16 in memory on 10.255.255.254:43641 (size: 46.0 B, free: 434.2 MiB)\n",
            "25/04/04 16:44:05 INFO Executor: Running task 23.0 in stage 7.0 (TID 50)\n",
            "25/04/04 16:44:05 INFO Executor: Finished task 19.0 in stage 7.0 (TID 46). 4912 bytes result sent to driver\n",
            "25/04/04 16:44:05 INFO Executor: Finished task 16.0 in stage 7.0 (TID 43). 4912 bytes result sent to driver\n",
            "25/04/04 16:44:05 INFO MemoryStore: Block rdd_28_20 stored as values in memory (estimated size 46.0 B, free 432.9 MiB)\n",
            "25/04/04 16:44:05 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "25/04/04 16:44:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\n",
            "25/04/04 16:44:05 INFO BlockManagerInfo: Added rdd_28_20 in memory on 10.255.255.254:43641 (size: 46.0 B, free: 434.2 MiB)\n",
            "25/04/04 16:44:05 INFO TaskSetManager: Starting task 24.0 in stage 7.0 (TID 51) (10.255.255.254, executor driver, partition 24, PROCESS_LOCAL, 8999 bytes) \n",
            "25/04/04 16:44:05 INFO Executor: Finished task 20.0 in stage 7.0 (TID 47). 4912 bytes result sent to driver\n",
            "25/04/04 16:44:05 INFO TaskSetManager: Starting task 26.0 in stage 7.0 (TID 52) (10.255.255.254, executor driver, partition 26, PROCESS_LOCAL, 8999 bytes) \n",
            "25/04/04 16:44:05 INFO Executor: Running task 26.0 in stage 7.0 (TID 52)\n",
            "25/04/04 16:44:05 INFO TaskSetManager: Finished task 20.0 in stage 7.0 (TID 47) in 198 ms on 10.255.255.254 (executor driver) (19/50)\n",
            "25/04/04 16:44:05 INFO TaskSetManager: Finished task 19.0 in stage 7.0 (TID 46) in 276 ms on 10.255.255.254 (executor driver) (20/50)\n",
            "25/04/04 16:44:05 INFO TaskSetManager: Finished task 16.0 in stage 7.0 (TID 43) in 295 ms on 10.255.255.254 (executor driver) (21/50)\n",
            "25/04/04 16:44:05 INFO TaskSetManager: Starting task 27.0 in stage 7.0 (TID 53) (10.255.255.254, executor driver, partition 27, PROCESS_LOCAL, 8999 bytes) \n",
            "25/04/04 16:44:05 INFO Executor: Running task 24.0 in stage 7.0 (TID 51)\n",
            "25/04/04 16:44:05 INFO Executor: Running task 27.0 in stage 7.0 (TID 53)\n",
            "25/04/04 16:44:05 INFO MemoryStore: Block rdd_28_15 stored as values in memory (estimated size 46.0 B, free 432.9 MiB)\n",
            "25/04/04 16:44:05 INFO BlockManagerInfo: Added rdd_28_15 in memory on 10.255.255.254:43641 (size: 46.0 B, free: 434.2 MiB)\n",
            "25/04/04 16:44:05 INFO Executor: Finished task 15.0 in stage 7.0 (TID 42). 4912 bytes result sent to driver\n",
            "25/04/04 16:44:05 INFO TaskSetManager: Starting task 28.0 in stage 7.0 (TID 54) (10.255.255.254, executor driver, partition 28, PROCESS_LOCAL, 8999 bytes) \n",
            "25/04/04 16:44:05 INFO Executor: Running task 28.0 in stage 7.0 (TID 54)\n",
            "25/04/04 16:44:05 INFO TaskSetManager: Finished task 15.0 in stage 7.0 (TID 42) in 366 ms on 10.255.255.254 (executor driver) (22/50)\n",
            "25/04/04 16:44:05 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "25/04/04 16:44:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\n",
            "25/04/04 16:44:05 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "25/04/04 16:44:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
            "25/04/04 16:44:05 INFO MemoryStore: Block rdd_28_18 stored as values in memory (estimated size 46.0 B, free 433.0 MiB)\n",
            "25/04/04 16:44:05 INFO BlockManagerInfo: Added rdd_28_18 in memory on 10.255.255.254:43641 (size: 46.0 B, free: 434.2 MiB)\n",
            "25/04/04 16:44:05 INFO MemoryStore: Block rdd_28_21 stored as values in memory (estimated size 46.0 B, free 432.9 MiB)\n",
            "25/04/04 16:44:05 INFO BlockManagerInfo: Added rdd_28_21 in memory on 10.255.255.254:43641 (size: 46.0 B, free: 434.2 MiB)\n",
            "25/04/04 16:44:05 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "25/04/04 16:44:05 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "25/04/04 16:44:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 6 ms\n",
            "25/04/04 16:44:05 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "25/04/04 16:44:05 INFO Executor: Finished task 21.0 in stage 7.0 (TID 48). 4955 bytes result sent to driver\n",
            "25/04/04 16:44:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
            "25/04/04 16:44:05 INFO TaskSetManager: Starting task 29.0 in stage 7.0 (TID 55) (10.255.255.254, executor driver, partition 29, PROCESS_LOCAL, 8999 bytes) \n",
            "25/04/04 16:44:05 INFO Executor: Running task 29.0 in stage 7.0 (TID 55)\n",
            "25/04/04 16:44:05 INFO Executor: Finished task 18.0 in stage 7.0 (TID 45). 4955 bytes result sent to driver\n",
            "25/04/04 16:44:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 23 ms\n",
            "25/04/04 16:44:05 INFO TaskSetManager: Finished task 21.0 in stage 7.0 (TID 48) in 286 ms on 10.255.255.254 (executor driver) (23/50)\n",
            "25/04/04 16:44:05 INFO TaskSetManager: Starting task 30.0 in stage 7.0 (TID 56) (10.255.255.254, executor driver, partition 30, PROCESS_LOCAL, 8999 bytes) \n",
            "25/04/04 16:44:05 INFO Executor: Running task 30.0 in stage 7.0 (TID 56)\n",
            "25/04/04 16:44:05 INFO TaskSetManager: Finished task 18.0 in stage 7.0 (TID 45) in 429 ms on 10.255.255.254 (executor driver) (24/50)\n",
            "25/04/04 16:44:05 INFO MemoryStore: Block rdd_28_22 stored as values in memory (estimated size 46.0 B, free 432.8 MiB)\n",
            "25/04/04 16:44:05 INFO BlockManagerInfo: Added rdd_28_22 in memory on 10.255.255.254:43641 (size: 46.0 B, free: 434.2 MiB)\n",
            "25/04/04 16:44:05 INFO Executor: Finished task 22.0 in stage 7.0 (TID 49). 4955 bytes result sent to driver\n",
            "25/04/04 16:44:05 INFO TaskSetManager: Starting task 31.0 in stage 7.0 (TID 57) (10.255.255.254, executor driver, partition 31, PROCESS_LOCAL, 8999 bytes) \n",
            "25/04/04 16:44:05 INFO Executor: Running task 31.0 in stage 7.0 (TID 57)\n",
            "25/04/04 16:44:05 INFO TaskSetManager: Finished task 22.0 in stage 7.0 (TID 49) in 238 ms on 10.255.255.254 (executor driver) (25/50)\n",
            "25/04/04 16:44:05 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "25/04/04 16:44:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\n",
            "25/04/04 16:44:05 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "25/04/04 16:44:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms\n",
            "25/04/04 16:44:05 INFO MemoryStore: Block rdd_28_23 stored as values in memory (estimated size 46.0 B, free 432.8 MiB)\n",
            "25/04/04 16:44:05 INFO BlockManagerInfo: Added rdd_28_23 in memory on 10.255.255.254:43641 (size: 46.0 B, free: 434.2 MiB)\n",
            "25/04/04 16:44:05 INFO Executor: Finished task 23.0 in stage 7.0 (TID 50). 4955 bytes result sent to driver\n",
            "25/04/04 16:44:05 INFO TaskSetManager: Starting task 32.0 in stage 7.0 (TID 58) (10.255.255.254, executor driver, partition 32, PROCESS_LOCAL, 8999 bytes) \n",
            "25/04/04 16:44:05 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "25/04/04 16:44:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\n",
            "25/04/04 16:44:05 INFO Executor: Running task 32.0 in stage 7.0 (TID 58)\n",
            "25/04/04 16:44:05 INFO TaskSetManager: Finished task 23.0 in stage 7.0 (TID 50) in 256 ms on 10.255.255.254 (executor driver) (26/50)\n",
            "25/04/04 16:44:05 INFO MemoryStore: Block rdd_28_26 stored as values in memory (estimated size 46.0 B, free 432.7 MiB)\n",
            "25/04/04 16:44:05 INFO BlockManagerInfo: Added rdd_28_26 in memory on 10.255.255.254:43641 (size: 46.0 B, free: 434.2 MiB)\n",
            "25/04/04 16:44:05 INFO Executor: Finished task 26.0 in stage 7.0 (TID 52). 4955 bytes result sent to driver\n",
            "25/04/04 16:44:05 INFO TaskSetManager: Starting task 33.0 in stage 7.0 (TID 59) (10.255.255.254, executor driver, partition 33, PROCESS_LOCAL, 8999 bytes) \n",
            "25/04/04 16:44:05 INFO Executor: Running task 33.0 in stage 7.0 (TID 59)\n",
            "25/04/04 16:44:05 INFO TaskSetManager: Finished task 26.0 in stage 7.0 (TID 52) in 248 ms on 10.255.255.254 (executor driver) (27/50)\n",
            "25/04/04 16:44:05 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "25/04/04 16:44:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\n",
            "25/04/04 16:44:05 INFO MemoryStore: Block rdd_28_24 stored as values in memory (estimated size 46.0 B, free 432.8 MiB)\n",
            "25/04/04 16:44:05 INFO BlockManagerInfo: Added rdd_28_24 in memory on 10.255.255.254:43641 (size: 46.0 B, free: 434.2 MiB)\n",
            "25/04/04 16:44:05 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "25/04/04 16:44:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
            "25/04/04 16:44:05 INFO Executor: Finished task 24.0 in stage 7.0 (TID 51). 4955 bytes result sent to driver\n",
            "25/04/04 16:44:05 INFO TaskSetManager: Starting task 34.0 in stage 7.0 (TID 60) (10.255.255.254, executor driver, partition 34, PROCESS_LOCAL, 8999 bytes) \n",
            "25/04/04 16:44:05 INFO MemoryStore: Block rdd_28_27 stored as values in memory (estimated size 46.0 B, free 432.7 MiB)\n",
            "25/04/04 16:44:05 INFO Executor: Running task 34.0 in stage 7.0 (TID 60)\n",
            "25/04/04 16:44:05 INFO TaskSetManager: Finished task 24.0 in stage 7.0 (TID 51) in 292 ms on 10.255.255.254 (executor driver) (28/50)\n",
            "25/04/04 16:44:05 INFO BlockManagerInfo: Added rdd_28_27 in memory on 10.255.255.254:43641 (size: 46.0 B, free: 434.2 MiB)\n",
            "25/04/04 16:44:05 INFO Executor: Finished task 27.0 in stage 7.0 (TID 53). 4955 bytes result sent to driver\n",
            "25/04/04 16:44:05 INFO TaskSetManager: Starting task 35.0 in stage 7.0 (TID 61) (10.255.255.254, executor driver, partition 35, PROCESS_LOCAL, 8999 bytes) \n",
            "25/04/04 16:44:05 INFO Executor: Running task 35.0 in stage 7.0 (TID 61)\n",
            "25/04/04 16:44:05 INFO MemoryStore: Block rdd_28_28 stored as values in memory (estimated size 46.0 B, free 432.8 MiB)\n",
            "25/04/04 16:44:05 INFO BlockManagerInfo: Added rdd_28_28 in memory on 10.255.255.254:43641 (size: 46.0 B, free: 434.2 MiB)\n",
            "25/04/04 16:44:05 INFO TaskSetManager: Finished task 27.0 in stage 7.0 (TID 53) in 305 ms on 10.255.255.254 (executor driver) (29/50)\n",
            "25/04/04 16:44:05 INFO MemoryStore: Block rdd_28_31 stored as values in memory (estimated size 46.0 B, free 432.8 MiB)\n",
            "25/04/04 16:44:05 INFO BlockManagerInfo: Added rdd_28_31 in memory on 10.255.255.254:43641 (size: 46.0 B, free: 434.2 MiB)\n",
            "25/04/04 16:44:05 INFO Executor: Finished task 28.0 in stage 7.0 (TID 54). 4955 bytes result sent to driver\n",
            "25/04/04 16:44:05 INFO Executor: Finished task 31.0 in stage 7.0 (TID 57). 4912 bytes result sent to driver\n",
            "25/04/04 16:44:05 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "25/04/04 16:44:05 INFO TaskSetManager: Starting task 36.0 in stage 7.0 (TID 62) (10.255.255.254, executor driver, partition 36, PROCESS_LOCAL, 8999 bytes) \n",
            "25/04/04 16:44:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
            "25/04/04 16:44:05 INFO Executor: Running task 36.0 in stage 7.0 (TID 62)\n",
            "25/04/04 16:44:05 INFO TaskSetManager: Finished task 28.0 in stage 7.0 (TID 54) in 288 ms on 10.255.255.254 (executor driver) (30/50)\n",
            "25/04/04 16:44:05 INFO TaskSetManager: Starting task 37.0 in stage 7.0 (TID 63) (10.255.255.254, executor driver, partition 37, PROCESS_LOCAL, 8999 bytes) \n",
            "25/04/04 16:44:05 INFO Executor: Running task 37.0 in stage 7.0 (TID 63)\n",
            "25/04/04 16:44:05 INFO TaskSetManager: Finished task 31.0 in stage 7.0 (TID 57) in 141 ms on 10.255.255.254 (executor driver) (31/50)\n",
            "25/04/04 16:44:05 INFO MemoryStore: Block rdd_28_30 stored as values in memory (estimated size 46.0 B, free 432.8 MiB)\n",
            "25/04/04 16:44:05 INFO BlockManagerInfo: Added rdd_28_30 in memory on 10.255.255.254:43641 (size: 46.0 B, free: 434.2 MiB)\n",
            "25/04/04 16:44:05 INFO Executor: Finished task 30.0 in stage 7.0 (TID 56). 4912 bytes result sent to driver\n",
            "25/04/04 16:44:05 INFO TaskSetManager: Starting task 38.0 in stage 7.0 (TID 64) (10.255.255.254, executor driver, partition 38, PROCESS_LOCAL, 8999 bytes) \n",
            "25/04/04 16:44:05 INFO TaskSetManager: Finished task 30.0 in stage 7.0 (TID 56) in 189 ms on 10.255.255.254 (executor driver) (32/50)\n",
            "25/04/04 16:44:05 INFO Executor: Running task 38.0 in stage 7.0 (TID 64)\n",
            "25/04/04 16:44:05 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "25/04/04 16:44:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\n",
            "25/04/04 16:44:05 INFO MemoryStore: Block rdd_28_32 stored as values in memory (estimated size 46.0 B, free 433.0 MiB)\n",
            "25/04/04 16:44:05 INFO BlockManagerInfo: Added rdd_28_32 in memory on 10.255.255.254:43641 (size: 46.0 B, free: 434.2 MiB)\n",
            "25/04/04 16:44:05 INFO MemoryStore: Block rdd_28_29 stored as values in memory (estimated size 46.0 B, free 433.0 MiB)\n",
            "25/04/04 16:44:05 INFO BlockManagerInfo: Added rdd_28_29 in memory on 10.255.255.254:43641 (size: 46.0 B, free: 434.2 MiB)\n",
            "25/04/04 16:44:05 INFO Executor: Finished task 32.0 in stage 7.0 (TID 58). 4912 bytes result sent to driver\n",
            "25/04/04 16:44:05 INFO Executor: Finished task 29.0 in stage 7.0 (TID 55). 4912 bytes result sent to driver\n",
            "25/04/04 16:44:05 INFO TaskSetManager: Starting task 39.0 in stage 7.0 (TID 65) (10.255.255.254, executor driver, partition 39, PROCESS_LOCAL, 8999 bytes) \n",
            "25/04/04 16:44:05 INFO Executor: Running task 39.0 in stage 7.0 (TID 65)\n",
            "25/04/04 16:44:05 INFO TaskSetManager: Finished task 32.0 in stage 7.0 (TID 58) in 137 ms on 10.255.255.254 (executor driver) (33/50)\n",
            "25/04/04 16:44:05 INFO TaskSetManager: Starting task 40.0 in stage 7.0 (TID 66) (10.255.255.254, executor driver, partition 40, PROCESS_LOCAL, 8999 bytes) \n",
            "25/04/04 16:44:05 INFO Executor: Running task 40.0 in stage 7.0 (TID 66)\n",
            "25/04/04 16:44:05 INFO TaskSetManager: Finished task 29.0 in stage 7.0 (TID 55) in 228 ms on 10.255.255.254 (executor driver) (34/50)\n",
            "25/04/04 16:44:05 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "25/04/04 16:44:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\n",
            "25/04/04 16:44:05 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "25/04/04 16:44:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
            "25/04/04 16:44:05 INFO MemoryStore: Block rdd_28_33 stored as values in memory (estimated size 46.0 B, free 432.9 MiB)\n",
            "25/04/04 16:44:05 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "25/04/04 16:44:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
            "25/04/04 16:44:05 INFO BlockManagerInfo: Added rdd_28_33 in memory on 10.255.255.254:43641 (size: 46.0 B, free: 434.2 MiB)\n",
            "25/04/04 16:44:05 INFO Executor: Finished task 33.0 in stage 7.0 (TID 59). 4912 bytes result sent to driver\n",
            "25/04/04 16:44:05 INFO TaskSetManager: Starting task 41.0 in stage 7.0 (TID 67) (10.255.255.254, executor driver, partition 41, PROCESS_LOCAL, 8999 bytes) \n",
            "25/04/04 16:44:05 INFO Executor: Running task 41.0 in stage 7.0 (TID 67)\n",
            "25/04/04 16:44:05 INFO TaskSetManager: Finished task 33.0 in stage 7.0 (TID 59) in 164 ms on 10.255.255.254 (executor driver) (35/50)\n",
            "25/04/04 16:44:05 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "25/04/04 16:44:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\n",
            "25/04/04 16:44:05 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "25/04/04 16:44:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
            "25/04/04 16:44:05 INFO MemoryStore: Block rdd_28_35 stored as values in memory (estimated size 46.0 B, free 432.8 MiB)\n",
            "25/04/04 16:44:05 INFO BlockManagerInfo: Added rdd_28_35 in memory on 10.255.255.254:43641 (size: 46.0 B, free: 434.2 MiB)\n",
            "25/04/04 16:44:05 INFO Executor: Finished task 35.0 in stage 7.0 (TID 61). 4912 bytes result sent to driver\n",
            "25/04/04 16:44:05 INFO TaskSetManager: Starting task 43.0 in stage 7.0 (TID 68) (10.255.255.254, executor driver, partition 43, PROCESS_LOCAL, 8999 bytes) \n",
            "25/04/04 16:44:05 INFO TaskSetManager: Finished task 35.0 in stage 7.0 (TID 61) in 148 ms on 10.255.255.254 (executor driver) (36/50)\n",
            "25/04/04 16:44:05 INFO Executor: Running task 43.0 in stage 7.0 (TID 68)\n",
            "25/04/04 16:44:05 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "25/04/04 16:44:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\n",
            "25/04/04 16:44:05 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "25/04/04 16:44:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\n",
            "25/04/04 16:44:05 INFO MemoryStore: Block rdd_28_38 stored as values in memory (estimated size 46.0 B, free 432.6 MiB)\n",
            "25/04/04 16:44:05 INFO BlockManagerInfo: Added rdd_28_38 in memory on 10.255.255.254:43641 (size: 46.0 B, free: 434.2 MiB)\n",
            "25/04/04 16:44:05 INFO MemoryStore: Block rdd_28_34 stored as values in memory (estimated size 46.0 B, free 432.7 MiB)\n",
            "25/04/04 16:44:05 INFO BlockManagerInfo: Added rdd_28_34 in memory on 10.255.255.254:43641 (size: 46.0 B, free: 434.2 MiB)\n",
            "25/04/04 16:44:05 INFO Executor: Finished task 38.0 in stage 7.0 (TID 64). 4955 bytes result sent to driver\n",
            "25/04/04 16:44:05 INFO TaskSetManager: Starting task 44.0 in stage 7.0 (TID 69) (10.255.255.254, executor driver, partition 44, PROCESS_LOCAL, 8999 bytes) \n",
            "25/04/04 16:44:05 INFO TaskSetManager: Finished task 38.0 in stage 7.0 (TID 64) in 310 ms on 10.255.255.254 (executor driver) (37/50)\n",
            "25/04/04 16:44:05 INFO Executor: Finished task 34.0 in stage 7.0 (TID 60). 4955 bytes result sent to driver\n",
            "25/04/04 16:44:05 INFO MemoryStore: Block rdd_28_37 stored as values in memory (estimated size 46.0 B, free 432.8 MiB)\n",
            "25/04/04 16:44:05 INFO Executor: Running task 44.0 in stage 7.0 (TID 69)\n",
            "25/04/04 16:44:05 INFO BlockManagerInfo: Added rdd_28_37 in memory on 10.255.255.254:43641 (size: 46.0 B, free: 434.2 MiB)\n",
            "25/04/04 16:44:05 INFO TaskSetManager: Starting task 45.0 in stage 7.0 (TID 70) (10.255.255.254, executor driver, partition 45, PROCESS_LOCAL, 8999 bytes) \n",
            "25/04/04 16:44:05 INFO Executor: Finished task 37.0 in stage 7.0 (TID 63). 4955 bytes result sent to driver\n",
            "25/04/04 16:44:05 INFO TaskSetManager: Starting task 46.0 in stage 7.0 (TID 71) (10.255.255.254, executor driver, partition 46, PROCESS_LOCAL, 8999 bytes) \n",
            "25/04/04 16:44:05 INFO Executor: Running task 46.0 in stage 7.0 (TID 71)\n",
            "25/04/04 16:44:05 INFO TaskSetManager: Finished task 37.0 in stage 7.0 (TID 63) in 343 ms on 10.255.255.254 (executor driver) (38/50)\n",
            "25/04/04 16:44:05 INFO TaskSetManager: Finished task 34.0 in stage 7.0 (TID 60) in 380 ms on 10.255.255.254 (executor driver) (39/50)\n",
            "25/04/04 16:44:05 INFO Executor: Running task 45.0 in stage 7.0 (TID 70)\n",
            "25/04/04 16:44:05 INFO MemoryStore: Block rdd_28_40 stored as values in memory (estimated size 46.0 B, free 432.9 MiB)\n",
            "25/04/04 16:44:05 INFO MemoryStore: Block rdd_28_39 stored as values in memory (estimated size 46.0 B, free 433.0 MiB)\n",
            "25/04/04 16:44:05 INFO MemoryStore: Block rdd_28_41 stored as values in memory (estimated size 46.0 B, free 432.9 MiB)\n",
            "25/04/04 16:44:05 INFO BlockManagerInfo: Added rdd_28_40 in memory on 10.255.255.254:43641 (size: 46.0 B, free: 434.2 MiB)\n",
            "25/04/04 16:44:05 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "25/04/04 16:44:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
            "25/04/04 16:44:05 INFO MemoryStore: Block rdd_28_36 stored as values in memory (estimated size 46.0 B, free 433.0 MiB)\n",
            "25/04/04 16:44:05 INFO BlockManagerInfo: Added rdd_28_39 in memory on 10.255.255.254:43641 (size: 46.0 B, free: 434.2 MiB)\n",
            "25/04/04 16:44:05 INFO BlockManagerInfo: Added rdd_28_41 in memory on 10.255.255.254:43641 (size: 46.0 B, free: 434.2 MiB)\n",
            "25/04/04 16:44:05 INFO BlockManagerInfo: Added rdd_28_36 in memory on 10.255.255.254:43641 (size: 46.0 B, free: 434.2 MiB)\n",
            "25/04/04 16:44:05 INFO Executor: Finished task 40.0 in stage 7.0 (TID 66). 4955 bytes result sent to driver\n",
            "25/04/04 16:44:05 INFO MemoryStore: Block rdd_28_43 stored as values in memory (estimated size 46.0 B, free 433.1 MiB)\n",
            "25/04/04 16:44:05 INFO TaskSetManager: Starting task 47.0 in stage 7.0 (TID 72) (10.255.255.254, executor driver, partition 47, PROCESS_LOCAL, 8999 bytes) \n",
            "25/04/04 16:44:05 INFO BlockManagerInfo: Added rdd_28_43 in memory on 10.255.255.254:43641 (size: 46.0 B, free: 434.2 MiB)\n",
            "25/04/04 16:44:05 INFO Executor: Running task 47.0 in stage 7.0 (TID 72)\n",
            "25/04/04 16:44:05 INFO Executor: Finished task 41.0 in stage 7.0 (TID 67). 4955 bytes result sent to driver\n",
            "25/04/04 16:44:05 INFO TaskSetManager: Finished task 40.0 in stage 7.0 (TID 66) in 330 ms on 10.255.255.254 (executor driver) (40/50)\n",
            "25/04/04 16:44:05 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "25/04/04 16:44:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
            "25/04/04 16:44:05 INFO Executor: Finished task 36.0 in stage 7.0 (TID 62). 4955 bytes result sent to driver\n",
            "25/04/04 16:44:05 INFO Executor: Finished task 43.0 in stage 7.0 (TID 68). 4955 bytes result sent to driver\n",
            "25/04/04 16:44:05 INFO TaskSetManager: Starting task 48.0 in stage 7.0 (TID 73) (10.255.255.254, executor driver, partition 48, PROCESS_LOCAL, 8999 bytes) \n",
            "25/04/04 16:44:05 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "25/04/04 16:44:05 INFO TaskSetManager: Starting task 49.0 in stage 7.0 (TID 74) (10.255.255.254, executor driver, partition 49, PROCESS_LOCAL, 8999 bytes) \n",
            "25/04/04 16:44:05 INFO Executor: Running task 48.0 in stage 7.0 (TID 73)\n",
            "25/04/04 16:44:05 INFO TaskSetManager: Finished task 36.0 in stage 7.0 (TID 62) in 381 ms on 10.255.255.254 (executor driver) (41/50)\n",
            "25/04/04 16:44:05 INFO TaskSetManager: Finished task 43.0 in stage 7.0 (TID 68) in 256 ms on 10.255.255.254 (executor driver) (42/50)\n",
            "25/04/04 16:44:05 INFO Executor: Running task 49.0 in stage 7.0 (TID 74)\n",
            "25/04/04 16:44:05 INFO TaskSetManager: Finished task 41.0 in stage 7.0 (TID 67) in 294 ms on 10.255.255.254 (executor driver) (43/50)\n",
            "25/04/04 16:44:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 11 ms\n",
            "25/04/04 16:44:05 INFO Executor: Finished task 39.0 in stage 7.0 (TID 65). 4955 bytes result sent to driver\n",
            "25/04/04 16:44:05 INFO TaskSetManager: Finished task 39.0 in stage 7.0 (TID 65) in 364 ms on 10.255.255.254 (executor driver) (44/50)\n",
            "25/04/04 16:44:05 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "25/04/04 16:44:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\n",
            "25/04/04 16:44:05 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "25/04/04 16:44:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\n",
            "25/04/04 16:44:05 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "25/04/04 16:44:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\n",
            "25/04/04 16:44:05 INFO MemoryStore: Block rdd_28_45 stored as values in memory (estimated size 46.0 B, free 432.8 MiB)\n",
            "25/04/04 16:44:05 INFO BlockManagerInfo: Added rdd_28_45 in memory on 10.255.255.254:43641 (size: 46.0 B, free: 434.2 MiB)\n",
            "25/04/04 16:44:05 INFO Executor: Finished task 45.0 in stage 7.0 (TID 70). 4912 bytes result sent to driver\n",
            "25/04/04 16:44:05 INFO TaskSetManager: Finished task 45.0 in stage 7.0 (TID 70) in 111 ms on 10.255.255.254 (executor driver) (45/50)\n",
            "25/04/04 16:44:05 INFO MemoryStore: Block rdd_28_44 stored as values in memory (estimated size 46.0 B, free 432.8 MiB)\n",
            "25/04/04 16:44:05 INFO BlockManagerInfo: Added rdd_28_44 in memory on 10.255.255.254:43641 (size: 46.0 B, free: 434.2 MiB)\n",
            "25/04/04 16:44:05 INFO Executor: Finished task 44.0 in stage 7.0 (TID 69). 4912 bytes result sent to driver\n",
            "25/04/04 16:44:05 INFO TaskSetManager: Finished task 44.0 in stage 7.0 (TID 69) in 154 ms on 10.255.255.254 (executor driver) (46/50)\n",
            "25/04/04 16:44:05 INFO MemoryStore: Block rdd_28_49 stored as values in memory (estimated size 46.0 B, free 432.9 MiB)\n",
            "25/04/04 16:44:05 INFO BlockManagerInfo: Added rdd_28_49 in memory on 10.255.255.254:43641 (size: 46.0 B, free: 434.2 MiB)\n",
            "25/04/04 16:44:05 INFO Executor: Finished task 49.0 in stage 7.0 (TID 74). 4912 bytes result sent to driver\n",
            "25/04/04 16:44:05 INFO TaskSetManager: Finished task 49.0 in stage 7.0 (TID 74) in 128 ms on 10.255.255.254 (executor driver) (47/50)\n",
            "25/04/04 16:44:06 INFO MemoryStore: Block rdd_28_46 stored as values in memory (estimated size 46.0 B, free 433.0 MiB)\n",
            "25/04/04 16:44:06 INFO BlockManagerInfo: Added rdd_28_46 in memory on 10.255.255.254:43641 (size: 46.0 B, free: 434.2 MiB)\n",
            "25/04/04 16:44:06 INFO Executor: Finished task 46.0 in stage 7.0 (TID 71). 4955 bytes result sent to driver\n",
            "25/04/04 16:44:06 INFO TaskSetManager: Finished task 46.0 in stage 7.0 (TID 71) in 212 ms on 10.255.255.254 (executor driver) (48/50)\n",
            "25/04/04 16:44:06 INFO MemoryStore: Block rdd_28_47 stored as values in memory (estimated size 46.0 B, free 433.0 MiB)\n",
            "25/04/04 16:44:06 INFO BlockManagerInfo: Added rdd_28_47 in memory on 10.255.255.254:43641 (size: 46.0 B, free: 434.2 MiB)\n",
            "25/04/04 16:44:06 INFO MemoryStore: Block rdd_28_48 stored as values in memory (estimated size 46.0 B, free 433.1 MiB)\n",
            "25/04/04 16:44:06 INFO BlockManagerInfo: Added rdd_28_48 in memory on 10.255.255.254:43641 (size: 46.0 B, free: 434.2 MiB)\n",
            "25/04/04 16:44:06 INFO Executor: Finished task 48.0 in stage 7.0 (TID 73). 4912 bytes result sent to driver\n",
            "25/04/04 16:44:06 INFO Executor: Finished task 47.0 in stage 7.0 (TID 72). 4912 bytes result sent to driver\n",
            "25/04/04 16:44:06 INFO TaskSetManager: Finished task 48.0 in stage 7.0 (TID 73) in 205 ms on 10.255.255.254 (executor driver) (49/50)\n",
            "25/04/04 16:44:06 INFO TaskSetManager: Finished task 47.0 in stage 7.0 (TID 72) in 210 ms on 10.255.255.254 (executor driver) (50/50)\n",
            "25/04/04 16:44:06 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool \n",
            "25/04/04 16:44:06 INFO DAGScheduler: ResultStage 7 ($anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128) finished in 2.743 s\n",
            "25/04/04 16:44:06 INFO DAGScheduler: Job 8 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/04/04 16:44:06 INFO TaskSchedulerImpl: Killing all running tasks in stage 7: Stage finished\n",
            "25/04/04 16:44:06 INFO DAGScheduler: Job 8 finished: $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128, took 2.791086 s\n",
            "25/04/04 16:44:06 INFO CodeGenerator: Code generated in 42.760679 ms            \n",
            "25/04/04 16:44:06 INFO DeltaLog: Updated snapshot to Snapshot(path=s3a://wba/example-table/_delta_log, version=0, metadata=Metadata(027ba8ff-d622-4f81-8ab9-6e03980ceab8,null,null,Format(parquet,Map()),{\"type\":\"struct\",\"fields\":[{\"name\":\"id\",\"type\":\"long\",\"nullable\":true,\"metadata\":{}},{\"name\":\"name\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}}]},List(),Map(delta.compatibility.symlinkFormatManifest.enabled -> false),Some(1743803035287)), logSegment=LogSegment(s3a://wba/example-table/_delta_log,0,List(S3AFileStatus{path=s3a://wba/example-table/_delta_log/00000000000000000000.json; isDirectory=false; length=1484; replication=1; blocksize=33554432; modification_time=1743803040000; access_time=0; owner=developer; group=developer; permission=rw-rw-rw-; isSymlink=false; hasAcl=false; isEncrypted=true; isErasureCoded=false} isEmptyDirectory=FALSE eTag=b2bfba830a2dc78d8d85c501b9d66547 versionId=null),org.apache.spark.sql.delta.EmptyCheckpointProvider$@4dbc4421,1743803040000), checksumOpt=Some(VersionChecksum(Some(06e43a65-4dd5-44a7-86d0-3d06662a6910),1424,2,None,None,1,1,None,Some(Stream()),Some(Stream()),Metadata(027ba8ff-d622-4f81-8ab9-6e03980ceab8,null,null,Format(parquet,Map()),{\"type\":\"struct\",\"fields\":[{\"name\":\"id\",\"type\":\"long\",\"nullable\":true,\"metadata\":{}},{\"name\":\"name\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}}]},List(),Map(delta.compatibility.symlinkFormatManifest.enabled -> false),Some(1743803035287)),Protocol(1,2),None,None,Some(Stream(AddFile(part-00007-7d2acd4f-83ba-40db-a8bd-8c46f564ae5b-c000.snappy.parquet,Map(),712,1743803037000,false,{\"numRecords\":1,\"minValues\":{\"id\":2,\"name\":\"Jane\"},\"maxValues\":{\"id\":2,\"name\":\"Jane\"},\"nullCount\":{\"id\":0,\"name\":0}},null,null,None,None,None), AddFile(part-00003-1373c84d-ff41-43d9-be23-5ca37994b068-c000.snappy.parquet,Map(),712,1743803037000,false,{\"numRecords\":1,\"minValues\":{\"id\":1,\"name\":\"John\"},\"maxValues\":{\"id\":1,\"name\":\"John\"},\"nullCount\":{\"id\":0,\"name\":0}},null,null,None,None,None))))))\n",
            "25/04/04 16:44:06 INFO OptimisticTransaction: [tableId=e5c5e22a,txnId=06e43a65] Committed delta #0 to s3a://wba/example-table/_delta_log\n",
            "25/04/04 16:44:06 INFO ChecksumHook: Writing checksum file for table path s3a://wba/example-table/_delta_log version 0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Successfully wrote Delta table\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "25/04/04 16:44:06 INFO deprecation: org.apache.hadoop.shaded.io.bytes.per.checksum is deprecated. Instead, use dfs.bytes-per-checksum\n",
            "25/04/04 16:44:06 INFO CheckpointFileManager: Writing atomically to s3a://wba/example-table/_delta_log/00000000000000000000.crc using temp file s3a://wba/example-table/_delta_log/.00000000000000000000.crc.bea21759-a810-48c6-8e68-5b0368175a58.tmp\n",
            "25/04/04 16:44:06 INFO CheckpointFileManager: Renamed temp file s3a://wba/example-table/_delta_log/.00000000000000000000.crc.bea21759-a810-48c6-8e68-5b0368175a58.tmp to s3a://wba/example-table/_delta_log/00000000000000000000.crc\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import subprocess\n",
        "import sys\n",
        "from typing import List\n",
        "from pyspark.sql import SparkSession\n",
        "import logging\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Define the base directory\n",
        "jars_home = '/workspace/delta-jars'\n",
        "\n",
        "# Required core JARs\n",
        "jars_list = [\n",
        "    # Delta Lake\n",
        "    f\"{jars_home}/delta-spark_2.12-3.3.0.jar\",\n",
        "    f\"{jars_home}/delta-storage-3.3.0.jar\",\n",
        "    # AWS\n",
        "    f\"{jars_home}/hadoop-aws-3.3.2.jar\",\n",
        "    f\"{jars_home}/aws-java-sdk-bundle-1.12.782.jar\",\n",
        "    # Kyuubi\n",
        "    f\"{jars_home}/kyuubi/externals/engines/spark/kyuubi-spark-sql-engine_2.12-1.10.0.jar\",\n",
        "    f\"{jars_home}/kyuubi/externals/engines/spark/kyuubi-common_2.12-1.10.0.jar\"\n",
        "]\n",
        "\n",
        "# Convert to comma-separated string\n",
        "jars = \",\".join(jars_list)\n",
        "\n",
        "# Create SparkSession using the builder pattern\n",
        "builder = (SparkSession.builder\n",
        "           .appName(\"DeltaExample\")\n",
        "           .master(\"local[*]\")\n",
        "           # Add debug configurations\n",
        "           .config(\"spark.hadoop.fs.s3a.connection.maximum\", \"1\")\n",
        "           .config(\"spark.hadoop.fs.s3a.attempts.maximum\", \"1\")\n",
        "           .config(\"spark.hadoop.fs.s3a.connection.timeout\", \"5000\")\n",
        "           .config(\"spark.hadoop.fs.s3a.impl.disable.cache\", \"true\")\n",
        "           .config(\"spark.hadoop.fs.s3a.debug.detailed.exceptions\", \"true\")\n",
        "           # Add jars directly\n",
        "           .config(\"spark.jars\", jars)\n",
        "           .config(\"spark.driver.extraClassPath\", jars)\n",
        "           .config(\"spark.executor.extraClassPath\", jars)\n",
        "           # Delta Lake configurations\n",
        "           .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
        "           .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
        "           # S3/MinIO configurations\n",
        "           .config(\"spark.hadoop.fs.s3a.access.key\", \"minioadmin\")\n",
        "           .config(\"spark.hadoop.fs.s3a.secret.key\", \"minioadmin\")\n",
        "           .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://localhost:9000\")\n",
        "           .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\")\n",
        "           .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
        "           .config(\"spark.hadoop.fs.s3a.aws.credentials.provider\", \"org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider\")\n",
        "           .config(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"false\")\n",
        "           # Additional Delta Lake configurations\n",
        "           .config(\"spark.delta.logStore.class\", \"io.delta.storage.S3SingleDriverLogStore\")\n",
        "           .config(\"spark.hadoop.fs.s3a.fast.upload\", \"true\")\n",
        "           .config(\"spark.hadoop.fs.s3a.multipart.size\", \"104857600\")\n",
        "           .config(\"spark.sql.warehouse.dir\", \"s3a://wba/warehouse\"))\n",
        "\n",
        "# Stop any existing session\n",
        "if 'spark' in locals():\n",
        "    spark.stop()\n",
        "\n",
        "\n",
        "# Create the session\n",
        "spark = builder.enableHiveSupport().getOrCreate()\n",
        "\n",
        "# Initialize Delta Lake settings\n",
        "# spark.sql(\"SET spark.databricks.delta.formatCheck.enabled=false\")\n",
        "\n",
        "# Access the SparkContext\n",
        "# sc = spark.sparkContext\n",
        "\n",
        "# Set the log level to INFO\n",
        "# sc.setLogLevel(\"DEBUG\")\n",
        "\n",
        "# Test DataFrame\n",
        "data = [(1, \"John\"), (2, \"Jane\")]\n",
        "df = spark.createDataFrame(data, [\"id\", \"name\"])\n",
        "df.show()\n",
        "\n",
        "# First verify the S3 connection by listing the bucket\n",
        "try:\n",
        "    # Try to write to a simple parquet file first to test S3 connection\n",
        "    print(\"Testing S3 connection with parquet write...\")\n",
        "    df.write.format(\"parquet\").mode(\"overwrite\").save(\"s3a://wba/test.parquet\")\n",
        "    print(\"S3 connection successful\")\n",
        "\n",
        "    print(\"Attempting to write Delta table...\")\n",
        "    df.write \\\n",
        "        .format(\"delta\") \\\n",
        "        .mode(\"overwrite\") \\\n",
        "        .option(\"overwriteSchema\", \"true\") \\\n",
        "        .option(\"delta.compatibility.symlinkFormatManifest.enabled\", \"false\") \\\n",
        "        .save(\"s3a://wba/example-table\")\n",
        "    print(\"Successfully wrote Delta table\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error: {str(e)}\")\n",
        "    print(\"\\nTrying local filesystem instead...\")\n",
        "    try:\n",
        "        local_path = \"/tmp/test-delta-table\"\n",
        "        df.write \\\n",
        "            .format(\"delta\") \\\n",
        "            .mode(\"overwrite\") \\\n",
        "            .option(\"overwriteSchema\", \"true\") \\\n",
        "            .save(local_path)\n",
        "        print(f\"Successfully wrote to {local_path}\")\n",
        "    except Exception as local_e:\n",
        "        print(f\"Error writing to local filesystem: {str(local_e)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
