FROM mcr.microsoft.com/vscode/devcontainers/python:3.10

# Install system dependencies
RUN apt-get update && export DEBIAN_FRONTEND=noninteractive \
    && apt-get -y install --no-install-recommends \
    openjdk-17-jdk \
    curl \
    wget \
    sudo \
    # Docker and Docker Compose dependencies
    ca-certificates \
    gnupg \
    lsb-release \
    && rm -rf /var/lib/apt/lists/*

# Add Docker's official GPG key and repository
RUN curl -fsSL https://download.docker.com/linux/debian/gpg | gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpg \
    && echo "deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://download.docker.com/linux/debian \
    $(lsb_release -cs) stable" | tee /etc/apt/sources.list.d/docker.list > /dev/null

# Install Docker and Docker Compose
RUN apt-get update && export DEBIAN_FRONTEND=noninteractive \
    && apt-get -y install --no-install-recommends \
    docker-ce \
    docker-ce-cli \
    containerd.io \
    docker-buildx-plugin \
    docker-compose-plugin \
    && rm -rf /var/lib/apt/lists/* \
    && echo "vscode ALL=(ALL) NOPASSWD:ALL" >> /etc/sudoers \
    && groupadd -f docker \
    && usermod -aG docker vscode

# Install Spark and set up environment
ENV SPARK_VERSION=3.4.4
ENV SPARK_HOME=/opt/spark
ENV PATH=$PATH:$SPARK_HOME/bin
ENV PYTHONPATH=$SPARK_HOME/python:$SPARK_HOME/python/lib/py4j-0.10.9.5-src.zip:$PYTHONPATH

RUN curl -O https://downloads.apache.org/spark/spark-${SPARK_VERSION}/spark-${SPARK_VERSION}-bin-hadoop3.tgz \
    && tar xvf spark-${SPARK_VERSION}-bin-hadoop3.tgz \
    && mv spark-${SPARK_VERSION}-bin-hadoop3 /opt/spark \
    && rm spark-${SPARK_VERSION}-bin-hadoop3.tgz

# Install Delta Lake dependencies
RUN wget https://repo1.maven.org/maven2/io/delta/delta-spark_2.12/3.2.1/delta-spark_2.12-3.2.1.jar -P /opt/spark/jars/ \
    && wget https://repo1.maven.org/maven2/io/delta/delta-storage/3.2.1/delta-storage-3.2.1.jar -P /opt/spark/jars/ \
    && wget https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-aws/3.3.2/hadoop-aws-3.3.2.jar -P /opt/spark/jars/ \
    && wget https://repo1.maven.org/maven2/com/amazonaws/aws-java-sdk-bundle/1.12.261/aws-java-sdk-bundle-1.12.261.jar -P /opt/spark/jars/
 
# Download and install Kyuubi SQL engine JAR
RUN mkdir -p /opt/spark/jars && \
    wget https://repository.apache.org/content/groups/public/org/apache/kyuubi/kyuubi-spark-sql-engine_2.12/1.9.3/kyuubi-spark-sql-engine_2.12-1.9.3.jar \
    -O /opt/spark/jars/kyuubi-spark-sql-engine_2.12-1.9.3.jar

 # Install make and wget
RUN apt-get update && \
apt-get install -y make wget && \
rm -rf /var/lib/apt/lists/*

# Create working directory
WORKDIR /app

# Copy Makefile
COPY mk/hive.mk /app/mk/
COPY Makefile /app/

# Run make to download and setup JDBC drivers
RUN make hive-all

# Set environment variables for JDBC drivers
ENV HIVE_JDBC_DIR=/opt/kyuubi-sqltools/jdbc
ENV AWS_ACCESS_KEY_ID=minioadmin
ENV AWS_SECRET_ACCESS_KEY=minioadmin
ENV AWS_REGION=us-east-1
ENV AWS_S3_ENDPOINT=http://minio:9000

# Optional: if you want to keep the container running for debugging
CMD ["tail", "-f", "/dev/null"]